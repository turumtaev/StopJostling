{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a1373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/app/repos/selfishTokens_2/nanoGPT\n",
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "%cd nanoGPT\n",
    "!python data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_training(args):\n",
    "    \"\"\"\n",
    "    Runs the training script with the given arguments and streams the output in real time in Jupyter Notebook.\n",
    "    \n",
    "    Args:\n",
    "        args (dict): Dictionary of command-line arguments where keys are the argument names \n",
    "                     and values are the argument values.\n",
    "    \"\"\"\n",
    "    # Base command with the script and config file\n",
    "    command = [\n",
    "        \"python\", \"train.py\", \"config/train_shakespeare_char.py\"\n",
    "    ] + [f'--{k}={v}' for k, v in args.items()]\n",
    "\n",
    "    # Use Popen to continuously capture and stream the output\n",
    "    with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1) as process:\n",
    "        for line in process.stdout:\n",
    "            print(line, end=\"\")  # Print the output as it arrives\n",
    "            sys.stdout.flush()  # Ensure immediate flush of the output in Jupyter\n",
    "        \n",
    "        # Wait for the process to complete and handle errors if they occur\n",
    "        process.wait()  # Ensure the process completes\n",
    "        if process.returncode != 0:\n",
    "            # In case of errors, print the stderr\n",
    "            error_output = process.stderr.read()\n",
    "            print(\"Error:\", error_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d05c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/baseline\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 817.67ms, mfu -100.00%\n",
      "iter 100: loss 2.7908, time 30.50ms, mfu 0.04%\n",
      "iter 200: loss 2.5009, time 209.08ms, mfu 0.04%\n",
      "step 250: train loss 2.4772, val loss 2.4916\n",
      "saving checkpoint to out/baseline\n",
      "iter 300: loss 2.4487, time 33.48ms, mfu 0.04%\n",
      "iter 400: loss 2.4665, time 30.50ms, mfu 0.04%\n",
      "step 500: train loss 2.2669, val loss 2.3109\n",
      "saving checkpoint to out/baseline\n",
      "iter 500: loss 2.2378, time 3302.05ms, mfu 0.04%\n",
      "iter 600: loss 2.1920, time 81.04ms, mfu 0.03%\n",
      "iter 700: loss 2.1545, time 60.87ms, mfu 0.03%\n",
      "step 750: train loss 2.1503, val loss 2.1625\n",
      "saving checkpoint to out/baseline\n",
      "iter 800: loss 2.2467, time 114.13ms, mfu 0.03%\n",
      "iter 900: loss 2.0326, time 36.78ms, mfu 0.03%\n",
      "step 1000: train loss 2.0337, val loss 2.0900\n",
      "saving checkpoint to out/baseline\n",
      "iter 1000: loss 2.1145, time 927.41ms, mfu 0.03%\n",
      "iter 1100: loss 2.0279, time 34.66ms, mfu 0.03%\n",
      "iter 1200: loss 1.9514, time 36.79ms, mfu 0.03%\n",
      "step 1250: train loss 1.9259, val loss 2.0529\n",
      "saving checkpoint to out/baseline\n",
      "iter 1300: loss 1.9198, time 147.78ms, mfu 0.03%\n",
      "iter 1400: loss 1.8443, time 44.55ms, mfu 0.03%\n",
      "step 1500: train loss 1.9026, val loss 1.9910\n",
      "saving checkpoint to out/baseline\n",
      "iter 1500: loss 1.9099, time 705.51ms, mfu 0.02%\n",
      "iter 1600: loss 1.9199, time 31.29ms, mfu 0.03%\n",
      "iter 1700: loss 1.8352, time 688.19ms, mfu 0.02%\n",
      "step 1750: train loss 1.8322, val loss 1.9485\n",
      "saving checkpoint to out/baseline\n",
      "iter 1800: loss 1.8018, time 174.78ms, mfu 0.02%\n",
      "iter 1900: loss 1.8322, time 77.63ms, mfu 0.02%\n",
      "step 2000: train loss 1.7869, val loss 1.9819\n",
      "iter 2000: loss 1.6818, time 1811.48ms, mfu 0.02%\n",
      "iter 2100: loss 1.7227, time 45.07ms, mfu 0.02%\n",
      "iter 2200: loss 1.8114, time 143.53ms, mfu 0.02%\n",
      "step 2250: train loss 1.7891, val loss 1.9237\n",
      "saving checkpoint to out/baseline\n",
      "iter 2300: loss 1.8021, time 41.25ms, mfu 0.02%\n",
      "iter 2400: loss 1.7369, time 103.23ms, mfu 0.02%\n",
      "step 2500: train loss 1.7487, val loss 1.8560\n",
      "saving checkpoint to out/baseline\n",
      "iter 2500: loss 1.6898, time 1756.65ms, mfu 0.02%\n",
      "iter 2600: loss 1.6720, time 54.26ms, mfu 0.02%\n",
      "iter 2700: loss 1.6428, time 50.30ms, mfu 0.02%\n",
      "step 2750: train loss 1.7344, val loss 1.8880\n",
      "iter 2800: loss 1.7888, time 39.80ms, mfu 0.02%\n",
      "iter 2900: loss 1.6998, time 39.59ms, mfu 0.02%\n",
      "step 3000: train loss 1.6968, val loss 1.8220\n",
      "saving checkpoint to out/baseline\n",
      "iter 3000: loss 1.8184, time 774.50ms, mfu 0.02%\n",
      "iter 3100: loss 1.8093, time 56.83ms, mfu 0.02%\n",
      "iter 3200: loss 1.7376, time 37.45ms, mfu 0.02%\n",
      "step 3250: train loss 1.6237, val loss 1.8156\n",
      "saving checkpoint to out/baseline\n",
      "iter 3300: loss 1.7233, time 88.62ms, mfu 0.02%\n",
      "iter 3400: loss 1.7498, time 43.62ms, mfu 0.02%\n",
      "step 3500: train loss 1.6554, val loss 1.8254\n",
      "iter 3500: loss 1.8636, time 709.05ms, mfu 0.02%\n",
      "iter 3600: loss 1.6789, time 77.02ms, mfu 0.02%\n",
      "iter 3700: loss 1.6544, time 58.35ms, mfu 0.02%\n",
      "step 3750: train loss 1.6191, val loss 1.7781\n",
      "saving checkpoint to out/baseline\n",
      "iter 3800: loss 1.6352, time 163.84ms, mfu 0.02%\n",
      "iter 3900: loss 1.4672, time 86.01ms, mfu 0.02%\n",
      "step 4000: train loss 1.5904, val loss 1.7715\n",
      "saving checkpoint to out/baseline\n",
      "iter 4000: loss 1.6164, time 1555.34ms, mfu 0.02%\n",
      "iter 4100: loss 1.7416, time 59.34ms, mfu 0.02%\n",
      "iter 4200: loss 1.5046, time 39.72ms, mfu 0.02%\n",
      "step 4250: train loss 1.5390, val loss 1.7327\n",
      "saving checkpoint to out/baseline\n",
      "iter 4300: loss 1.6213, time 69.40ms, mfu 0.02%\n",
      "iter 4400: loss 1.6296, time 146.89ms, mfu 0.02%\n",
      "step 4500: train loss 1.5597, val loss 1.7433\n",
      "iter 4500: loss 1.5264, time 1506.22ms, mfu 0.02%\n",
      "iter 4600: loss 1.6227, time 879.49ms, mfu 0.01%\n",
      "iter 4700: loss 1.5195, time 283.42ms, mfu 0.01%\n",
      "step 4750: train loss 1.5504, val loss 1.7198\n",
      "saving checkpoint to out/baseline\n",
      "iter 4800: loss 1.5362, time 69.23ms, mfu 0.01%\n",
      "iter 4900: loss 1.5825, time 45.03ms, mfu 0.02%\n",
      "step 5000: train loss 1.5166, val loss 1.7088\n",
      "saving checkpoint to out/baseline\n",
      "iter 5000: loss 1.4665, time 1406.61ms, mfu 0.01%\n",
      "iter 5100: loss 1.4571, time 80.74ms, mfu 0.01%\n",
      "iter 5200: loss 1.4854, time 108.20ms, mfu 0.01%\n",
      "step 5250: train loss 1.4963, val loss 1.7164\n",
      "iter 5300: loss 1.5664, time 93.52ms, mfu 0.01%\n",
      "iter 5400: loss 1.4264, time 96.10ms, mfu 0.01%\n",
      "step 5500: train loss 1.5105, val loss 1.6693\n",
      "saving checkpoint to out/baseline\n",
      "iter 5500: loss 1.4574, time 2806.00ms, mfu 0.01%\n",
      "iter 5600: loss 1.5100, time 75.04ms, mfu 0.01%\n",
      "iter 5700: loss 1.4645, time 41.04ms, mfu 0.01%\n",
      "step 5750: train loss 1.4783, val loss 1.6757\n",
      "iter 5800: loss 1.5725, time 190.99ms, mfu 0.01%\n",
      "iter 5900: loss 1.5255, time 49.15ms, mfu 0.02%\n",
      "step 6000: train loss 1.5161, val loss 1.6500\n",
      "saving checkpoint to out/baseline\n",
      "iter 6000: loss 1.4916, time 2721.33ms, mfu 0.01%\n",
      "iter 6100: loss 1.5831, time 141.02ms, mfu 0.01%\n",
      "iter 6200: loss 1.3376, time 85.20ms, mfu 0.01%\n",
      "step 6250: train loss 1.4649, val loss 1.6433\n",
      "saving checkpoint to out/baseline\n",
      "iter 6300: loss 1.3878, time 53.07ms, mfu 0.01%\n",
      "iter 6400: loss 1.5065, time 101.49ms, mfu 0.01%\n",
      "step 6500: train loss 1.4446, val loss 1.6321\n",
      "saving checkpoint to out/baseline\n",
      "iter 6500: loss 1.4048, time 2138.98ms, mfu 0.01%\n",
      "iter 6600: loss 1.3880, time 88.98ms, mfu 0.01%\n",
      "iter 6700: loss 1.3989, time 132.37ms, mfu 0.01%\n",
      "step 6750: train loss 1.4583, val loss 1.6456\n",
      "iter 6800: loss 1.4383, time 437.83ms, mfu 0.01%\n",
      "iter 6900: loss 1.5448, time 57.35ms, mfu 0.01%\n",
      "step 7000: train loss 1.4253, val loss 1.6415\n",
      "iter 7000: loss 1.4400, time 823.36ms, mfu 0.01%\n",
      "iter 7100: loss 1.5453, time 48.50ms, mfu 0.01%\n",
      "iter 7200: loss 1.4864, time 80.90ms, mfu 0.01%\n",
      "step 7250: train loss 1.4480, val loss 1.6445\n",
      "iter 7300: loss 1.4000, time 195.18ms, mfu 0.01%\n",
      "iter 7400: loss 1.3473, time 55.94ms, mfu 0.01%\n",
      "step 7500: train loss 1.4378, val loss 1.6154\n",
      "saving checkpoint to out/baseline\n",
      "iter 7500: loss 1.4497, time 2649.17ms, mfu 0.01%\n",
      "iter 7600: loss 1.4446, time 1014.56ms, mfu 0.01%\n",
      "iter 7700: loss 1.5329, time 60.54ms, mfu 0.01%\n",
      "step 7750: train loss 1.4369, val loss 1.6754\n",
      "iter 7800: loss 1.4041, time 80.15ms, mfu 0.01%\n",
      "iter 7900: loss 1.3969, time 66.09ms, mfu 0.01%\n",
      "step 8000: train loss 1.4624, val loss 1.6347\n",
      "iter 8000: loss 1.3548, time 1076.45ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "args = {\n",
    "    \"out_dir\": \"out/baseline\"\n",
    "}\n",
    "run_training(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc09316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/slo\n",
      "Overriding: log_interval = 2\n",
      "Overriding: max_iters = 160\n",
      "Overriding: lr_decay_iters = 160\n",
      "Overriding: eval_interval = 5\n",
      "Overriding: big_language_prob = 0.0\n",
      "Overriding: warmup_iters = 2\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8793, val loss 4.8752\n",
      "iter 0: loss 4.8859, time 1045.24ms, mfu -100.00%\n",
      "iter 2: loss 4.4649, time 69.74ms, mfu -100.00%\n",
      "iter 4: loss 4.1202, time 35.04ms, mfu -100.00%\n",
      "step 5: train loss 4.0533, val loss 4.0824\n",
      "saving checkpoint to out/slo\n",
      "iter 6: loss 3.9110, time 94.18ms, mfu 0.01%\n",
      "iter 8: loss 3.7573, time 52.58ms, mfu 0.01%\n",
      "step 10: train loss 3.5610, val loss 3.5863\n",
      "saving checkpoint to out/slo\n",
      "iter 10: loss 3.6122, time 769.67ms, mfu 0.01%\n",
      "iter 12: loss 3.4747, time 46.96ms, mfu 0.01%\n",
      "iter 14: loss 3.3802, time 29.60ms, mfu 0.02%\n",
      "step 15: train loss 3.2972, val loss 3.2986\n",
      "saving checkpoint to out/slo\n",
      "iter 16: loss 3.2561, time 40.60ms, mfu 0.02%\n",
      "iter 18: loss 3.1334, time 39.68ms, mfu 0.02%\n",
      "step 20: train loss 3.0842, val loss 3.1040\n",
      "saving checkpoint to out/slo\n",
      "iter 20: loss 3.0421, time 556.38ms, mfu 0.02%\n",
      "iter 22: loss 2.9824, time 50.43ms, mfu 0.02%\n",
      "iter 24: loss 2.9415, time 76.28ms, mfu 0.02%\n",
      "step 25: train loss 2.9504, val loss 2.9662\n",
      "saving checkpoint to out/slo\n",
      "iter 26: loss 2.9319, time 28.22ms, mfu 0.02%\n",
      "iter 28: loss 2.8497, time 40.84ms, mfu 0.02%\n",
      "step 30: train loss 2.8512, val loss 2.8643\n",
      "saving checkpoint to out/slo\n",
      "iter 30: loss 2.8199, time 620.25ms, mfu 0.02%\n",
      "iter 32: loss 2.8299, time 37.78ms, mfu 0.02%\n",
      "iter 34: loss 2.8064, time 30.07ms, mfu 0.02%\n",
      "step 35: train loss 2.7988, val loss 2.8154\n",
      "saving checkpoint to out/slo\n",
      "iter 36: loss 2.7254, time 31.83ms, mfu 0.03%\n",
      "iter 38: loss 2.7308, time 49.49ms, mfu 0.03%\n",
      "step 40: train loss 2.7292, val loss 2.7592\n",
      "saving checkpoint to out/slo\n",
      "iter 40: loss 2.6273, time 563.85ms, mfu 0.02%\n",
      "iter 42: loss 2.7014, time 73.96ms, mfu 0.02%\n",
      "iter 44: loss 2.6847, time 56.27ms, mfu 0.02%\n",
      "step 45: train loss 2.7387, val loss 2.7229\n",
      "saving checkpoint to out/slo\n",
      "iter 46: loss 2.6343, time 29.87ms, mfu 0.02%\n",
      "iter 48: loss 2.7177, time 31.15ms, mfu 0.03%\n",
      "step 50: train loss 2.6990, val loss 2.6942\n",
      "saving checkpoint to out/slo\n",
      "iter 50: loss 2.6971, time 653.35ms, mfu 0.02%\n",
      "iter 52: loss 2.6644, time 32.78ms, mfu 0.03%\n",
      "iter 54: loss 2.6897, time 49.27ms, mfu 0.03%\n",
      "step 55: train loss 2.6639, val loss 2.6510\n",
      "saving checkpoint to out/slo\n",
      "iter 56: loss 2.6350, time 35.45ms, mfu 0.03%\n",
      "iter 58: loss 2.6675, time 53.48ms, mfu 0.03%\n",
      "step 60: train loss 2.6538, val loss 2.6291\n",
      "saving checkpoint to out/slo\n",
      "iter 60: loss 2.6291, time 673.64ms, mfu 0.02%\n",
      "iter 62: loss 2.6614, time 56.70ms, mfu 0.02%\n",
      "iter 64: loss 2.5740, time 48.34ms, mfu 0.02%\n",
      "step 65: train loss 2.6332, val loss 2.6201\n",
      "saving checkpoint to out/slo\n",
      "iter 66: loss 2.6286, time 47.74ms, mfu 0.02%\n",
      "iter 68: loss 2.7832, time 46.42ms, mfu 0.02%\n",
      "step 70: train loss 2.5979, val loss 2.6269\n",
      "iter 70: loss 2.6096, time 562.67ms, mfu 0.02%\n",
      "iter 72: loss 2.5897, time 65.72ms, mfu 0.02%\n",
      "iter 74: loss 2.5943, time 34.93ms, mfu 0.02%\n",
      "step 75: train loss 2.6035, val loss 2.5926\n",
      "saving checkpoint to out/slo\n",
      "iter 76: loss 2.5455, time 33.43ms, mfu 0.03%\n",
      "iter 78: loss 2.5364, time 121.79ms, mfu 0.02%\n",
      "step 80: train loss 2.5842, val loss 2.5918\n",
      "saving checkpoint to out/slo\n",
      "iter 80: loss 2.6684, time 725.18ms, mfu 0.02%\n",
      "iter 82: loss 2.6002, time 48.74ms, mfu 0.02%\n",
      "iter 84: loss 2.6012, time 28.71ms, mfu 0.02%\n",
      "step 85: train loss 2.5838, val loss 2.5859\n",
      "saving checkpoint to out/slo\n",
      "iter 86: loss 2.5646, time 81.66ms, mfu 0.02%\n",
      "iter 88: loss 2.6302, time 32.30ms, mfu 0.03%\n",
      "step 90: train loss 2.5786, val loss 2.5932\n",
      "iter 90: loss 2.5440, time 594.53ms, mfu 0.02%\n",
      "iter 92: loss 2.5968, time 44.64ms, mfu 0.02%\n",
      "iter 94: loss 2.6042, time 59.71ms, mfu 0.02%\n",
      "step 95: train loss 2.5728, val loss 2.5589\n",
      "saving checkpoint to out/slo\n",
      "iter 96: loss 2.5550, time 31.14ms, mfu 0.02%\n",
      "iter 98: loss 2.5762, time 28.76ms, mfu 0.03%\n",
      "step 100: train loss 2.5608, val loss 2.5428\n",
      "saving checkpoint to out/slo\n",
      "iter 100: loss 2.5705, time 522.29ms, mfu 0.02%\n",
      "iter 102: loss 2.5330, time 38.61ms, mfu 0.03%\n",
      "iter 104: loss 2.5889, time 44.26ms, mfu 0.03%\n",
      "step 105: train loss 2.5655, val loss 2.5545\n",
      "iter 106: loss 2.6324, time 42.47ms, mfu 0.03%\n",
      "iter 108: loss 2.5428, time 30.40ms, mfu 0.03%\n",
      "step 110: train loss 2.5329, val loss 2.5244\n",
      "saving checkpoint to out/slo\n",
      "iter 110: loss 2.5183, time 515.96ms, mfu 0.03%\n",
      "iter 112: loss 2.5438, time 80.04ms, mfu 0.02%\n",
      "iter 114: loss 2.5268, time 87.01ms, mfu 0.02%\n",
      "step 115: train loss 2.5311, val loss 2.5366\n",
      "iter 116: loss 2.4692, time 57.07ms, mfu 0.02%\n",
      "iter 118: loss 2.4853, time 40.62ms, mfu 0.02%\n",
      "step 120: train loss 2.5404, val loss 2.5340\n",
      "iter 120: loss 2.5019, time 529.98ms, mfu 0.02%\n",
      "iter 122: loss 2.5626, time 41.30ms, mfu 0.02%\n",
      "iter 124: loss 2.4936, time 71.55ms, mfu 0.02%\n",
      "step 125: train loss 2.5108, val loss 2.5176\n",
      "saving checkpoint to out/slo\n",
      "iter 126: loss 2.4966, time 71.66ms, mfu 0.02%\n",
      "iter 128: loss 2.5182, time 58.02ms, mfu 0.02%\n",
      "step 130: train loss 2.5406, val loss 2.5021\n",
      "saving checkpoint to out/slo\n",
      "iter 130: loss 2.4568, time 639.90ms, mfu 0.02%\n",
      "iter 132: loss 2.5760, time 92.96ms, mfu 0.02%\n",
      "iter 134: loss 2.5180, time 47.37ms, mfu 0.02%\n",
      "step 135: train loss 2.5205, val loss 2.5459\n",
      "iter 136: loss 2.5736, time 29.98ms, mfu 0.02%\n",
      "iter 138: loss 2.5565, time 28.41ms, mfu 0.02%\n",
      "step 140: train loss 2.5222, val loss 2.5097\n",
      "iter 140: loss 2.5306, time 547.81ms, mfu 0.02%\n",
      "iter 142: loss 2.4034, time 33.96ms, mfu 0.02%\n",
      "iter 144: loss 2.4819, time 128.27ms, mfu 0.02%\n",
      "step 145: train loss 2.5049, val loss 2.5249\n",
      "iter 146: loss 2.5734, time 27.93ms, mfu 0.02%\n",
      "iter 148: loss 2.5767, time 29.14ms, mfu 0.03%\n",
      "step 150: train loss 2.5051, val loss 2.5158\n",
      "iter 150: loss 2.4628, time 625.87ms, mfu 0.02%\n",
      "iter 152: loss 2.6268, time 89.98ms, mfu 0.02%\n",
      "iter 154: loss 2.5318, time 74.66ms, mfu 0.02%\n",
      "step 155: train loss 2.5272, val loss 2.4984\n",
      "saving checkpoint to out/slo\n",
      "iter 156: loss 2.4830, time 30.73ms, mfu 0.02%\n",
      "iter 158: loss 2.5529, time 46.19ms, mfu 0.02%\n",
      "step 160: train loss 2.5196, val loss 2.5234\n",
      "iter 160: loss 2.4858, time 683.23ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "#slo\n",
    "args = {\n",
    "    \"out_dir\": \"out/slo\",\n",
    "    \"log_interval\": 2,\n",
    "    \"max_iters\": 160,\n",
    "    \"lr_decay_iters\": 160,\n",
    "    \"eval_interval\": 5,\n",
    "    \"big_language_prob\": 0.0,\n",
    "    \"warmup_iters\": 2,\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41bc9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_8\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 8\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1611.06ms, mfu -100.00%\n",
      "iter 100: loss 2.7907, time 66.63ms, mfu 0.02%\n",
      "iter 200: loss 2.4998, time 63.25ms, mfu 0.02%\n",
      "step 250: train loss 2.4780, val loss 2.4926\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 300: loss 2.4432, time 66.47ms, mfu 0.02%\n",
      "iter 400: loss 2.4793, time 28.70ms, mfu 0.02%\n",
      "step 500: train loss 2.2618, val loss 2.3074\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 500: loss 2.2312, time 708.00ms, mfu 0.02%\n",
      "iter 600: loss 2.1992, time 32.32ms, mfu 0.02%\n",
      "iter 700: loss 2.1465, time 37.92ms, mfu 0.02%\n",
      "step 750: train loss 2.1423, val loss 2.1591\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 800: loss 2.2327, time 87.40ms, mfu 0.02%\n",
      "iter 900: loss 2.0178, time 42.95ms, mfu 0.02%\n",
      "step 1000: train loss 2.0386, val loss 2.0952\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 1000: loss 2.1072, time 2692.01ms, mfu 0.02%\n",
      "iter 1100: loss 2.0390, time 63.84ms, mfu 0.02%\n",
      "iter 1200: loss 1.9434, time 38.49ms, mfu 0.02%\n",
      "step 1250: train loss 1.9402, val loss 2.0580\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 1300: loss 1.9228, time 785.06ms, mfu 0.02%\n",
      "iter 1400: loss 1.8497, time 40.47ms, mfu 0.02%\n",
      "step 1500: train loss 1.9007, val loss 1.9802\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 1500: loss 1.8870, time 1351.34ms, mfu 0.02%\n",
      "iter 1600: loss 1.9101, time 31.54ms, mfu 0.02%\n",
      "iter 1700: loss 1.8374, time 85.54ms, mfu 0.02%\n",
      "step 1750: train loss 1.8408, val loss 1.9641\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 1800: loss 1.8018, time 39.70ms, mfu 0.02%\n",
      "iter 1900: loss 1.8353, time 41.21ms, mfu 0.02%\n",
      "step 2000: train loss 1.7911, val loss 1.9931\n",
      "iter 2000: loss 1.6680, time 1644.20ms, mfu 0.02%\n",
      "iter 2100: loss 1.7514, time 37.65ms, mfu 0.02%\n",
      "iter 2200: loss 1.8252, time 237.25ms, mfu 0.02%\n",
      "step 2250: train loss 1.7877, val loss 1.9277\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 2300: loss 1.8035, time 44.28ms, mfu 0.02%\n",
      "iter 2400: loss 1.7305, time 65.68ms, mfu 0.02%\n",
      "step 2500: train loss 1.7427, val loss 1.8558\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 2500: loss 1.6909, time 1431.70ms, mfu 0.02%\n",
      "iter 2600: loss 1.6620, time 35.75ms, mfu 0.02%\n",
      "iter 2700: loss 1.6203, time 34.60ms, mfu 0.02%\n",
      "step 2750: train loss 1.7297, val loss 1.8826\n",
      "iter 2800: loss 1.7704, time 62.68ms, mfu 0.02%\n",
      "iter 2900: loss 1.6910, time 40.28ms, mfu 0.02%\n",
      "step 3000: train loss 1.6984, val loss 1.8257\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 3000: loss 1.7949, time 816.49ms, mfu 0.02%\n",
      "iter 3100: loss 1.8063, time 60.66ms, mfu 0.02%\n",
      "iter 3200: loss 1.7234, time 34.68ms, mfu 0.02%\n",
      "step 3250: train loss 1.6177, val loss 1.8288\n",
      "iter 3300: loss 1.7191, time 115.06ms, mfu 0.02%\n",
      "iter 3400: loss 1.7573, time 35.57ms, mfu 0.02%\n",
      "step 3500: train loss 1.6533, val loss 1.8113\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 3500: loss 1.8514, time 664.25ms, mfu 0.02%\n",
      "iter 3600: loss 1.6461, time 53.57ms, mfu 0.02%\n",
      "iter 3700: loss 1.6344, time 45.91ms, mfu 0.02%\n",
      "step 3750: train loss 1.6085, val loss 1.7849\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 3800: loss 1.6164, time 63.53ms, mfu 0.02%\n",
      "iter 3900: loss 1.4706, time 53.81ms, mfu 0.02%\n",
      "step 4000: train loss 1.5894, val loss 1.7827\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 4000: loss 1.5945, time 584.89ms, mfu 0.02%\n",
      "iter 4100: loss 1.7321, time 64.41ms, mfu 0.02%\n",
      "iter 4200: loss 1.4920, time 44.61ms, mfu 0.02%\n",
      "step 4250: train loss 1.5391, val loss 1.7353\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 4300: loss 1.6016, time 72.86ms, mfu 0.02%\n",
      "iter 4400: loss 1.5946, time 46.39ms, mfu 0.02%\n",
      "step 4500: train loss 1.5535, val loss 1.7540\n",
      "iter 4500: loss 1.5248, time 721.13ms, mfu 0.02%\n",
      "iter 4600: loss 1.6279, time 36.19ms, mfu 0.02%\n",
      "iter 4700: loss 1.5045, time 39.18ms, mfu 0.02%\n",
      "step 4750: train loss 1.5447, val loss 1.7166\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 4800: loss 1.5521, time 42.15ms, mfu 0.02%\n",
      "iter 4900: loss 1.6140, time 75.65ms, mfu 0.02%\n",
      "step 5000: train loss 1.5233, val loss 1.7021\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 5000: loss 1.4704, time 1071.09ms, mfu 0.02%\n",
      "iter 5100: loss 1.4640, time 60.85ms, mfu 0.02%\n",
      "iter 5200: loss 1.4836, time 44.14ms, mfu 0.02%\n",
      "step 5250: train loss 1.4994, val loss 1.7124\n",
      "iter 5300: loss 1.5392, time 61.32ms, mfu 0.02%\n",
      "iter 5400: loss 1.4026, time 67.98ms, mfu 0.02%\n",
      "step 5500: train loss 1.5085, val loss 1.6576\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 5500: loss 1.4333, time 932.45ms, mfu 0.02%\n",
      "iter 5600: loss 1.4901, time 57.94ms, mfu 0.02%\n",
      "iter 5700: loss 1.4495, time 52.59ms, mfu 0.02%\n",
      "step 5750: train loss 1.4799, val loss 1.6727\n",
      "iter 5800: loss 1.5877, time 41.01ms, mfu 0.02%\n",
      "iter 5900: loss 1.5242, time 55.72ms, mfu 0.02%\n",
      "step 6000: train loss 1.5125, val loss 1.6407\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 6000: loss 1.4911, time 834.20ms, mfu 0.02%\n",
      "iter 6100: loss 1.6015, time 65.25ms, mfu 0.02%\n",
      "iter 6200: loss 1.3298, time 53.36ms, mfu 0.02%\n",
      "step 6250: train loss 1.4690, val loss 1.6451\n",
      "iter 6300: loss 1.4035, time 43.87ms, mfu 0.02%\n",
      "iter 6400: loss 1.5147, time 42.19ms, mfu 0.02%\n",
      "step 6500: train loss 1.4516, val loss 1.6240\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 6500: loss 1.3994, time 874.59ms, mfu 0.02%\n",
      "iter 6600: loss 1.3907, time 62.82ms, mfu 0.02%\n",
      "iter 6700: loss 1.4128, time 89.28ms, mfu 0.02%\n",
      "step 6750: train loss 1.4544, val loss 1.6425\n",
      "iter 6800: loss 1.4474, time 71.05ms, mfu 0.02%\n",
      "iter 6900: loss 1.5546, time 78.84ms, mfu 0.02%\n",
      "step 7000: train loss 1.4265, val loss 1.6301\n",
      "iter 7000: loss 1.4514, time 929.93ms, mfu 0.02%\n",
      "iter 7100: loss 1.5361, time 68.58ms, mfu 0.02%\n",
      "iter 7200: loss 1.4855, time 60.53ms, mfu 0.02%\n",
      "step 7250: train loss 1.4434, val loss 1.6352\n",
      "iter 7300: loss 1.3835, time 42.85ms, mfu 0.02%\n",
      "iter 7400: loss 1.3369, time 82.72ms, mfu 0.02%\n",
      "step 7500: train loss 1.4368, val loss 1.6092\n",
      "saving checkpoint to out/proposed_margin_8\n",
      "iter 7500: loss 1.4581, time 917.37ms, mfu 0.02%\n",
      "iter 7600: loss 1.4469, time 79.20ms, mfu 0.02%\n",
      "iter 7700: loss 1.5145, time 95.65ms, mfu 0.02%\n",
      "step 7750: train loss 1.4356, val loss 1.6688\n",
      "iter 7800: loss 1.3841, time 110.72ms, mfu 0.02%\n",
      "iter 7900: loss 1.4449, time 57.87ms, mfu 0.02%\n",
      "step 8000: train loss 1.4558, val loss 1.6344\n",
      "iter 8000: loss 1.3492, time 910.67ms, mfu 0.02%\n",
      "done for margin=8\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_4\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 4\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1304.81ms, mfu -100.00%\n",
      "iter 100: loss 2.7869, time 78.52ms, mfu 0.02%\n",
      "iter 200: loss 2.4670, time 172.54ms, mfu 0.02%\n",
      "step 250: train loss 2.5053, val loss 2.5191\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 300: loss 2.4386, time 78.80ms, mfu 0.02%\n",
      "iter 400: loss 2.4596, time 115.91ms, mfu 0.02%\n",
      "step 500: train loss 2.2949, val loss 2.3377\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 500: loss 2.1978, time 957.62ms, mfu 0.01%\n",
      "iter 600: loss 2.1583, time 50.98ms, mfu 0.01%\n",
      "iter 700: loss 2.1091, time 62.00ms, mfu 0.02%\n",
      "step 750: train loss 2.1590, val loss 2.1769\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 800: loss 2.2020, time 54.00ms, mfu 0.02%\n",
      "iter 900: loss 1.9956, time 69.52ms, mfu 0.02%\n",
      "step 1000: train loss 2.0413, val loss 2.0940\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 1000: loss 2.0536, time 982.22ms, mfu 0.01%\n",
      "iter 1100: loss 2.0058, time 52.58ms, mfu 0.02%\n",
      "iter 1200: loss 1.9143, time 98.02ms, mfu 0.02%\n",
      "step 1250: train loss 1.9389, val loss 2.0623\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 1300: loss 1.9027, time 52.13ms, mfu 0.02%\n",
      "iter 1400: loss 1.8318, time 81.63ms, mfu 0.02%\n",
      "step 1500: train loss 1.9243, val loss 2.0136\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 1500: loss 1.8297, time 1115.46ms, mfu 0.01%\n",
      "iter 1600: loss 1.8800, time 100.69ms, mfu 0.01%\n",
      "iter 1700: loss 1.8044, time 68.89ms, mfu 0.02%\n",
      "step 1750: train loss 1.8534, val loss 1.9741\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 1800: loss 1.7813, time 75.55ms, mfu 0.02%\n",
      "iter 1900: loss 1.7993, time 45.94ms, mfu 0.02%\n",
      "step 2000: train loss 1.8030, val loss 1.9857\n",
      "iter 2000: loss 1.6231, time 1076.51ms, mfu 0.02%\n",
      "iter 2100: loss 1.7090, time 125.69ms, mfu 0.01%\n",
      "iter 2200: loss 1.7892, time 114.35ms, mfu 0.01%\n",
      "step 2250: train loss 1.8183, val loss 1.9283\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 2300: loss 1.7650, time 58.88ms, mfu 0.01%\n",
      "iter 2400: loss 1.6814, time 84.84ms, mfu 0.01%\n",
      "step 2500: train loss 1.7684, val loss 1.8611\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 2500: loss 1.6642, time 993.44ms, mfu 0.01%\n",
      "iter 2600: loss 1.6589, time 73.53ms, mfu 0.01%\n",
      "iter 2700: loss 1.6042, time 64.18ms, mfu 0.01%\n",
      "step 2750: train loss 1.7614, val loss 1.8880\n",
      "iter 2800: loss 1.7440, time 94.63ms, mfu 0.01%\n",
      "iter 2900: loss 1.6605, time 76.28ms, mfu 0.01%\n",
      "step 3000: train loss 1.7164, val loss 1.8272\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 3000: loss 1.7624, time 981.19ms, mfu 0.01%\n",
      "iter 3100: loss 1.7601, time 80.58ms, mfu 0.01%\n",
      "iter 3200: loss 1.7186, time 76.03ms, mfu 0.01%\n",
      "step 3250: train loss 1.6397, val loss 1.8167\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 3300: loss 1.6870, time 89.10ms, mfu 0.01%\n",
      "iter 3400: loss 1.7085, time 86.89ms, mfu 0.01%\n",
      "step 3500: train loss 1.6655, val loss 1.8096\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 3500: loss 1.8118, time 1095.87ms, mfu 0.01%\n",
      "iter 3600: loss 1.6397, time 126.32ms, mfu 0.01%\n",
      "iter 3700: loss 1.6220, time 89.00ms, mfu 0.01%\n",
      "step 3750: train loss 1.6285, val loss 1.7851\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 3800: loss 1.5723, time 76.09ms, mfu 0.01%\n",
      "iter 3900: loss 1.4507, time 54.37ms, mfu 0.01%\n",
      "step 4000: train loss 1.6075, val loss 1.7758\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 4000: loss 1.5467, time 1287.78ms, mfu 0.01%\n",
      "iter 4100: loss 1.6822, time 214.34ms, mfu 0.01%\n",
      "iter 4200: loss 1.4781, time 108.72ms, mfu 0.01%\n",
      "step 4250: train loss 1.5593, val loss 1.7407\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 4300: loss 1.5781, time 109.66ms, mfu 0.01%\n",
      "iter 4400: loss 1.6085, time 1103.79ms, mfu 0.01%\n",
      "step 4500: train loss 1.5775, val loss 1.7463\n",
      "iter 4500: loss 1.5113, time 1670.81ms, mfu 0.01%\n",
      "iter 4600: loss 1.6301, time 69.13ms, mfu 0.01%\n",
      "iter 4700: loss 1.4945, time 467.07ms, mfu 0.01%\n",
      "step 4750: train loss 1.5686, val loss 1.7154\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 4800: loss 1.5092, time 62.18ms, mfu 0.01%\n",
      "iter 4900: loss 1.5916, time 174.80ms, mfu 0.01%\n",
      "step 5000: train loss 1.5415, val loss 1.7062\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 5000: loss 1.4368, time 1450.73ms, mfu 0.01%\n",
      "iter 5100: loss 1.4448, time 62.53ms, mfu 0.01%\n",
      "iter 5200: loss 1.4582, time 94.62ms, mfu 0.01%\n",
      "step 5250: train loss 1.5159, val loss 1.7163\n",
      "iter 5300: loss 1.5326, time 291.70ms, mfu 0.01%\n",
      "iter 5400: loss 1.3888, time 65.52ms, mfu 0.01%\n",
      "step 5500: train loss 1.5218, val loss 1.6638\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 5500: loss 1.4109, time 4998.61ms, mfu 0.01%\n",
      "iter 5600: loss 1.4397, time 62.89ms, mfu 0.01%\n",
      "iter 5700: loss 1.4473, time 170.35ms, mfu 0.01%\n",
      "step 5750: train loss 1.5020, val loss 1.6726\n",
      "iter 5800: loss 1.5441, time 80.74ms, mfu 0.01%\n",
      "iter 5900: loss 1.5139, time 65.47ms, mfu 0.01%\n",
      "step 6000: train loss 1.5295, val loss 1.6427\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 6000: loss 1.4662, time 1007.81ms, mfu 0.01%\n",
      "iter 6100: loss 1.5880, time 133.52ms, mfu 0.01%\n",
      "iter 6200: loss 1.3181, time 86.13ms, mfu 0.01%\n",
      "step 6250: train loss 1.4907, val loss 1.6426\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 6300: loss 1.3599, time 255.17ms, mfu 0.01%\n",
      "iter 6400: loss 1.4799, time 295.48ms, mfu 0.01%\n",
      "step 6500: train loss 1.4616, val loss 1.6294\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 6500: loss 1.3714, time 1283.81ms, mfu 0.01%\n",
      "iter 6600: loss 1.3598, time 63.69ms, mfu 0.01%\n",
      "iter 6700: loss 1.3674, time 93.45ms, mfu 0.01%\n",
      "step 6750: train loss 1.4783, val loss 1.6407\n",
      "iter 6800: loss 1.4349, time 59.23ms, mfu 0.01%\n",
      "iter 6900: loss 1.5425, time 215.63ms, mfu 0.01%\n",
      "step 7000: train loss 1.4451, val loss 1.6355\n",
      "iter 7000: loss 1.4287, time 1324.91ms, mfu 0.01%\n",
      "iter 7100: loss 1.5229, time 111.05ms, mfu 0.01%\n",
      "iter 7200: loss 1.4369, time 51.64ms, mfu 0.01%\n",
      "step 7250: train loss 1.4667, val loss 1.6357\n",
      "iter 7300: loss 1.3583, time 87.42ms, mfu 0.01%\n",
      "iter 7400: loss 1.3408, time 61.49ms, mfu 0.01%\n",
      "step 7500: train loss 1.4576, val loss 1.6135\n",
      "saving checkpoint to out/proposed_margin_4\n",
      "iter 7500: loss 1.4249, time 1401.18ms, mfu 0.01%\n",
      "iter 7600: loss 1.4241, time 49.45ms, mfu 0.01%\n",
      "iter 7700: loss 1.5022, time 56.21ms, mfu 0.01%\n",
      "step 7750: train loss 1.4525, val loss 1.6634\n",
      "iter 7800: loss 1.3549, time 95.55ms, mfu 0.01%\n",
      "iter 7900: loss 1.3707, time 102.23ms, mfu 0.01%\n",
      "step 8000: train loss 1.4750, val loss 1.6325\n",
      "iter 8000: loss 1.3301, time 1096.25ms, mfu 0.01%\n",
      "done for margin=4\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_2\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1395.26ms, mfu -100.00%\n",
      "iter 100: loss 2.6473, time 96.84ms, mfu 0.01%\n",
      "iter 200: loss 2.3632, time 912.75ms, mfu 0.01%\n",
      "step 250: train loss 2.6497, val loss 2.6646\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 300: loss 2.3224, time 90.07ms, mfu 0.01%\n",
      "iter 400: loss 2.3056, time 77.21ms, mfu 0.01%\n",
      "step 500: train loss 2.4515, val loss 2.4873\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 500: loss 2.1096, time 1085.88ms, mfu 0.01%\n",
      "iter 600: loss 2.0999, time 139.89ms, mfu 0.01%\n",
      "iter 700: loss 2.0081, time 80.09ms, mfu 0.01%\n",
      "step 750: train loss 2.3084, val loss 2.3264\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 800: loss 2.1017, time 105.38ms, mfu 0.01%\n",
      "iter 900: loss 1.8868, time 114.49ms, mfu 0.01%\n",
      "step 1000: train loss 2.2041, val loss 2.2383\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 1000: loss 1.9766, time 1028.16ms, mfu 0.01%\n",
      "iter 1100: loss 1.9157, time 64.59ms, mfu 0.01%\n",
      "iter 1200: loss 1.8050, time 52.26ms, mfu 0.01%\n",
      "step 1250: train loss 2.0928, val loss 2.2086\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 1300: loss 1.8040, time 54.90ms, mfu 0.01%\n",
      "iter 1400: loss 1.7132, time 61.37ms, mfu 0.01%\n",
      "step 1500: train loss 2.0696, val loss 2.1475\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 1500: loss 1.7205, time 3346.23ms, mfu 0.01%\n",
      "iter 1600: loss 1.7596, time 279.82ms, mfu 0.01%\n",
      "iter 1700: loss 1.6591, time 69.44ms, mfu 0.01%\n",
      "step 1750: train loss 1.9951, val loss 2.0955\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 1800: loss 1.6500, time 108.46ms, mfu 0.01%\n",
      "iter 1900: loss 1.6877, time 45.17ms, mfu 0.01%\n",
      "step 2000: train loss 1.9459, val loss 2.0933\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 2000: loss 1.5333, time 1434.28ms, mfu 0.01%\n",
      "iter 2100: loss 1.6398, time 61.32ms, mfu 0.01%\n",
      "iter 2200: loss 1.6905, time 223.74ms, mfu 0.01%\n",
      "step 2250: train loss 1.9609, val loss 2.0554\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 2300: loss 1.6466, time 176.87ms, mfu 0.01%\n",
      "iter 2400: loss 1.5591, time 99.99ms, mfu 0.01%\n",
      "step 2500: train loss 1.9223, val loss 1.9975\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 2500: loss 1.5963, time 1062.05ms, mfu 0.01%\n",
      "iter 2600: loss 1.5862, time 49.05ms, mfu 0.01%\n",
      "iter 2700: loss 1.4874, time 60.68ms, mfu 0.01%\n",
      "step 2750: train loss 1.8942, val loss 2.0161\n",
      "iter 2800: loss 1.6295, time 48.12ms, mfu 0.01%\n",
      "iter 2900: loss 1.5485, time 71.02ms, mfu 0.02%\n",
      "step 3000: train loss 1.8570, val loss 1.9469\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 3000: loss 1.6576, time 1821.32ms, mfu 0.01%\n",
      "iter 3100: loss 1.6633, time 632.93ms, mfu 0.01%\n",
      "iter 3200: loss 1.5843, time 104.15ms, mfu 0.01%\n",
      "step 3250: train loss 1.7995, val loss 1.9467\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 3300: loss 1.5667, time 150.31ms, mfu 0.01%\n",
      "iter 3400: loss 1.6090, time 51.42ms, mfu 0.01%\n",
      "step 3500: train loss 1.8217, val loss 1.9326\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 3500: loss 1.6622, time 1874.51ms, mfu 0.01%\n",
      "iter 3600: loss 1.5557, time 580.95ms, mfu 0.01%\n",
      "iter 3700: loss 1.5132, time 189.29ms, mfu 0.01%\n",
      "step 3750: train loss 1.7872, val loss 1.9126\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 3800: loss 1.4968, time 121.28ms, mfu 0.01%\n",
      "iter 3900: loss 1.3684, time 141.39ms, mfu 0.01%\n",
      "step 4000: train loss 1.7650, val loss 1.9077\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 4000: loss 1.4655, time 2879.81ms, mfu 0.01%\n",
      "iter 4100: loss 1.5641, time 70.11ms, mfu 0.01%\n",
      "iter 4200: loss 1.4076, time 389.52ms, mfu 0.01%\n",
      "step 4250: train loss 1.7259, val loss 1.8698\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 4300: loss 1.4553, time 146.93ms, mfu 0.01%\n",
      "iter 4400: loss 1.4988, time 95.03ms, mfu 0.01%\n",
      "step 4500: train loss 1.7367, val loss 1.8688\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 4500: loss 1.4047, time 3568.12ms, mfu 0.01%\n",
      "iter 4600: loss 1.5262, time 54.00ms, mfu 0.01%\n",
      "iter 4700: loss 1.3950, time 53.68ms, mfu 0.01%\n",
      "step 4750: train loss 1.7307, val loss 1.8484\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 4800: loss 1.4389, time 53.72ms, mfu 0.01%\n",
      "iter 4900: loss 1.4840, time 56.63ms, mfu 0.01%\n",
      "step 5000: train loss 1.7037, val loss 1.8429\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 5000: loss 1.3458, time 824.11ms, mfu 0.01%\n",
      "iter 5100: loss 1.3545, time 45.60ms, mfu 0.01%\n",
      "iter 5200: loss 1.3560, time 50.00ms, mfu 0.02%\n",
      "step 5250: train loss 1.6838, val loss 1.8464\n",
      "iter 5300: loss 1.4394, time 53.77ms, mfu 0.02%\n",
      "iter 5400: loss 1.3355, time 88.39ms, mfu 0.02%\n",
      "step 5500: train loss 1.6880, val loss 1.8105\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 5500: loss 1.3312, time 1349.28ms, mfu 0.01%\n",
      "iter 5600: loss 1.3687, time 88.62ms, mfu 0.01%\n",
      "iter 5700: loss 1.3521, time 133.41ms, mfu 0.01%\n",
      "step 5750: train loss 1.6694, val loss 1.8161\n",
      "iter 5800: loss 1.4767, time 144.03ms, mfu 0.01%\n",
      "iter 5900: loss 1.4248, time 85.82ms, mfu 0.01%\n",
      "step 6000: train loss 1.6963, val loss 1.7868\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 6000: loss 1.3731, time 1165.17ms, mfu 0.01%\n",
      "iter 6100: loss 1.4439, time 107.58ms, mfu 0.01%\n",
      "iter 6200: loss 1.2357, time 52.21ms, mfu 0.01%\n",
      "step 6250: train loss 1.6671, val loss 1.7951\n",
      "iter 6300: loss 1.3030, time 225.77ms, mfu 0.01%\n",
      "iter 6400: loss 1.4016, time 73.64ms, mfu 0.01%\n",
      "step 6500: train loss 1.6376, val loss 1.7831\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 6500: loss 1.3091, time 2881.91ms, mfu 0.01%\n",
      "iter 6600: loss 1.2766, time 58.44ms, mfu 0.01%\n",
      "iter 6700: loss 1.2966, time 411.78ms, mfu 0.01%\n",
      "step 6750: train loss 1.6434, val loss 1.7911\n",
      "iter 6800: loss 1.3556, time 236.66ms, mfu 0.01%\n",
      "iter 6900: loss 1.4507, time 54.85ms, mfu 0.01%\n",
      "step 7000: train loss 1.6228, val loss 1.7831\n",
      "iter 7000: loss 1.3343, time 1075.25ms, mfu 0.01%\n",
      "iter 7100: loss 1.4263, time 46.14ms, mfu 0.01%\n",
      "iter 7200: loss 1.3340, time 63.85ms, mfu 0.01%\n",
      "step 7250: train loss 1.6408, val loss 1.7791\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 7300: loss 1.2904, time 75.66ms, mfu 0.01%\n",
      "iter 7400: loss 1.2695, time 99.60ms, mfu 0.01%\n",
      "step 7500: train loss 1.6332, val loss 1.7695\n",
      "saving checkpoint to out/proposed_margin_2\n",
      "iter 7500: loss 1.3598, time 1140.73ms, mfu 0.01%\n",
      "iter 7600: loss 1.3379, time 74.22ms, mfu 0.01%\n",
      "iter 7700: loss 1.4183, time 102.19ms, mfu 0.01%\n",
      "step 7750: train loss 1.6335, val loss 1.8019\n",
      "iter 7800: loss 1.2886, time 70.08ms, mfu 0.01%\n",
      "iter 7900: loss 1.2917, time 71.65ms, mfu 0.01%\n",
      "step 8000: train loss 1.6433, val loss 1.7773\n",
      "iter 8000: loss 1.2578, time 1194.62ms, mfu 0.01%\n",
      "done for margin=2\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8454, time 1184.50ms, mfu -100.00%\n",
      "iter 100: loss 2.4845, time 76.76ms, mfu 0.02%\n",
      "iter 200: loss 2.1829, time 64.55ms, mfu 0.02%\n",
      "step 250: train loss 3.0077, val loss 3.0220\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 300: loss 2.1750, time 48.54ms, mfu 0.02%\n",
      "iter 400: loss 2.1198, time 45.34ms, mfu 0.02%\n",
      "step 500: train loss 2.8322, val loss 2.8665\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 500: loss 1.9587, time 3037.14ms, mfu 0.02%\n",
      "iter 600: loss 1.8913, time 45.94ms, mfu 0.02%\n",
      "iter 700: loss 1.8347, time 74.15ms, mfu 0.02%\n",
      "step 750: train loss 2.6726, val loss 2.6838\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 800: loss 1.8837, time 85.80ms, mfu 0.02%\n",
      "iter 900: loss 1.6924, time 44.98ms, mfu 0.02%\n",
      "step 1000: train loss 2.5738, val loss 2.6115\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 1000: loss 1.7592, time 835.07ms, mfu 0.02%\n",
      "iter 1100: loss 1.6736, time 51.73ms, mfu 0.02%\n",
      "iter 1200: loss 1.6490, time 74.64ms, mfu 0.02%\n",
      "step 1250: train loss 2.4773, val loss 2.5696\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 1300: loss 1.6084, time 62.63ms, mfu 0.02%\n",
      "iter 1400: loss 1.5530, time 45.75ms, mfu 0.02%\n",
      "step 1500: train loss 2.4384, val loss 2.4994\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 1500: loss 1.5267, time 998.02ms, mfu 0.02%\n",
      "iter 1600: loss 1.5952, time 49.22ms, mfu 0.02%\n",
      "iter 1700: loss 1.5132, time 49.25ms, mfu 0.02%\n",
      "step 1750: train loss 2.3966, val loss 2.4756\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 1800: loss 1.5173, time 87.64ms, mfu 0.02%\n",
      "iter 1900: loss 1.5218, time 45.31ms, mfu 0.02%\n",
      "step 2000: train loss 2.3697, val loss 2.4783\n",
      "iter 2000: loss 1.3979, time 1002.04ms, mfu 0.02%\n",
      "iter 2100: loss 1.4932, time 59.20ms, mfu 0.02%\n",
      "iter 2200: loss 1.5274, time 78.23ms, mfu 0.02%\n",
      "step 2250: train loss 2.3733, val loss 2.4435\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 2300: loss 1.5261, time 44.29ms, mfu 0.02%\n",
      "iter 2400: loss 1.4538, time 65.69ms, mfu 0.02%\n",
      "step 2500: train loss 2.3293, val loss 2.3875\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 2500: loss 1.4526, time 972.41ms, mfu 0.02%\n",
      "iter 2600: loss 1.4242, time 48.25ms, mfu 0.02%\n",
      "iter 2700: loss 1.3655, time 98.70ms, mfu 0.02%\n",
      "step 2750: train loss 2.3288, val loss 2.4133\n",
      "iter 2800: loss 1.4929, time 72.27ms, mfu 0.02%\n",
      "iter 2900: loss 1.4130, time 89.54ms, mfu 0.02%\n",
      "step 3000: train loss 2.2926, val loss 2.3602\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 3000: loss 1.5041, time 862.90ms, mfu 0.02%\n",
      "iter 3100: loss 1.5176, time 42.55ms, mfu 0.02%\n",
      "iter 3200: loss 1.4421, time 64.75ms, mfu 0.02%\n",
      "step 3250: train loss 2.2451, val loss 2.3549\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 3300: loss 1.4472, time 69.79ms, mfu 0.02%\n",
      "iter 3400: loss 1.4588, time 49.97ms, mfu 0.02%\n",
      "step 3500: train loss 2.2613, val loss 2.3419\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 3500: loss 1.5462, time 931.49ms, mfu 0.02%\n",
      "iter 3600: loss 1.3931, time 95.59ms, mfu 0.02%\n",
      "iter 3700: loss 1.4154, time 47.91ms, mfu 0.02%\n",
      "step 3750: train loss 2.2413, val loss 2.3297\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 3800: loss 1.3675, time 51.00ms, mfu 0.02%\n",
      "iter 3900: loss 1.2554, time 106.78ms, mfu 0.02%\n",
      "step 4000: train loss 2.2350, val loss 2.3372\n",
      "iter 4000: loss 1.3651, time 885.95ms, mfu 0.02%\n",
      "iter 4100: loss 1.4120, time 54.75ms, mfu 0.02%\n",
      "iter 4200: loss 1.3054, time 73.38ms, mfu 0.02%\n",
      "step 4250: train loss 2.1944, val loss 2.2977\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 4300: loss 1.3465, time 43.57ms, mfu 0.02%\n",
      "iter 4400: loss 1.3701, time 48.01ms, mfu 0.02%\n",
      "step 4500: train loss 2.2045, val loss 2.3037\n",
      "iter 4500: loss 1.3106, time 1106.29ms, mfu 0.02%\n",
      "iter 4600: loss 1.3847, time 113.18ms, mfu 0.02%\n",
      "iter 4700: loss 1.2822, time 89.61ms, mfu 0.02%\n",
      "step 4750: train loss 2.2082, val loss 2.2907\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 4800: loss 1.3093, time 46.22ms, mfu 0.02%\n",
      "iter 4900: loss 1.3627, time 61.89ms, mfu 0.02%\n",
      "step 5000: train loss 2.1851, val loss 2.2804\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 5000: loss 1.2716, time 1147.87ms, mfu 0.02%\n",
      "iter 5100: loss 1.2661, time 47.67ms, mfu 0.02%\n",
      "iter 5200: loss 1.2697, time 66.01ms, mfu 0.02%\n",
      "step 5250: train loss 2.1726, val loss 2.2893\n",
      "iter 5300: loss 1.3096, time 108.60ms, mfu 0.02%\n",
      "iter 5400: loss 1.2371, time 49.10ms, mfu 0.02%\n",
      "step 5500: train loss 2.1808, val loss 2.2664\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 5500: loss 1.2505, time 916.56ms, mfu 0.02%\n",
      "iter 5600: loss 1.2457, time 84.12ms, mfu 0.02%\n",
      "iter 5700: loss 1.2386, time 82.89ms, mfu 0.02%\n",
      "step 5750: train loss 2.1642, val loss 2.2634\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 5800: loss 1.3614, time 104.73ms, mfu 0.02%\n",
      "iter 5900: loss 1.3056, time 69.08ms, mfu 0.02%\n",
      "step 6000: train loss 2.1946, val loss 2.2563\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 6000: loss 1.2768, time 812.73ms, mfu 0.01%\n",
      "iter 6100: loss 1.3343, time 67.06ms, mfu 0.01%\n",
      "iter 6200: loss 1.1683, time 62.25ms, mfu 0.02%\n",
      "step 6250: train loss 2.1642, val loss 2.2598\n",
      "iter 6300: loss 1.2285, time 51.42ms, mfu 0.02%\n",
      "iter 6400: loss 1.2781, time 92.92ms, mfu 0.02%\n",
      "step 6500: train loss 2.1414, val loss 2.2427\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 6500: loss 1.2067, time 1173.44ms, mfu 0.01%\n",
      "iter 6600: loss 1.1640, time 47.38ms, mfu 0.02%\n",
      "iter 6700: loss 1.1863, time 110.33ms, mfu 0.02%\n",
      "step 6750: train loss 2.1539, val loss 2.2594\n",
      "iter 6800: loss 1.2539, time 88.76ms, mfu 0.02%\n",
      "iter 6900: loss 1.3380, time 100.60ms, mfu 0.02%\n",
      "step 7000: train loss 2.1450, val loss 2.2576\n",
      "iter 7000: loss 1.2492, time 1050.97ms, mfu 0.01%\n",
      "iter 7100: loss 1.3082, time 99.11ms, mfu 0.01%\n",
      "iter 7200: loss 1.2362, time 128.23ms, mfu 0.01%\n",
      "step 7250: train loss 2.1509, val loss 2.2506\n",
      "iter 7300: loss 1.1986, time 503.58ms, mfu 0.01%\n",
      "iter 7400: loss 1.1562, time 84.57ms, mfu 0.01%\n",
      "step 7500: train loss 2.1411, val loss 2.2385\n",
      "saving checkpoint to out/proposed_margin_1\n",
      "iter 7500: loss 1.2406, time 2135.08ms, mfu 0.01%\n",
      "iter 7600: loss 1.2234, time 73.58ms, mfu 0.01%\n",
      "iter 7700: loss 1.3215, time 93.41ms, mfu 0.01%\n",
      "step 7750: train loss 2.1438, val loss 2.2641\n",
      "iter 7800: loss 1.1811, time 101.75ms, mfu 0.01%\n",
      "iter 7900: loss 1.2248, time 52.04ms, mfu 0.01%\n",
      "step 8000: train loss 2.1452, val loss 2.2431\n",
      "iter 8000: loss 1.1551, time 1457.63ms, mfu 0.01%\n",
      "done for margin=1\n"
     ]
    }
   ],
   "source": [
    "for margin in [8, 4, 2, 1]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": float(margin)\n",
    "\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c852ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/agg_0.02_1600\n",
      "Overriding: model_type = agg\n",
      "Overriding: agg_alpha = 0.02\n",
      "Overriding: agg_k = 1600\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 9.7377, time 867.35ms, mfu -100.00%\n",
      "iter 100: loss 5.6059, time 77.70ms, mfu 0.02%\n",
      "iter 200: loss 5.0277, time 37.48ms, mfu 0.02%\n",
      "step 250: train loss 2.4790, val loss 2.4850\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 300: loss 4.8779, time 103.80ms, mfu 0.02%\n",
      "iter 400: loss 4.9453, time 73.71ms, mfu 0.02%\n",
      "step 500: train loss 2.2694, val loss 2.3037\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 500: loss 4.4969, time 542.64ms, mfu 0.02%\n",
      "iter 600: loss 4.4259, time 78.81ms, mfu 0.02%\n",
      "iter 700: loss 4.2995, time 236.93ms, mfu 0.02%\n",
      "step 750: train loss 2.1445, val loss 2.1610\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 800: loss 4.4912, time 62.13ms, mfu 0.02%\n",
      "iter 900: loss 4.0671, time 52.03ms, mfu 0.02%\n",
      "step 1000: train loss 2.0221, val loss 2.0767\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 1000: loss 4.1763, time 653.36ms, mfu 0.02%\n",
      "iter 1100: loss 4.0386, time 35.87ms, mfu 0.02%\n",
      "iter 1200: loss 3.8853, time 60.48ms, mfu 0.02%\n",
      "step 1250: train loss 1.9387, val loss 2.0589\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 1300: loss 3.8958, time 75.05ms, mfu 0.02%\n",
      "iter 1400: loss 3.6631, time 35.62ms, mfu 0.02%\n",
      "step 1500: train loss 1.9062, val loss 1.9926\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 1500: loss 3.7352, time 928.61ms, mfu 0.02%\n",
      "iter 1600: loss 3.8831, time 1457.10ms, mfu 0.02%\n",
      "iter 1700: loss 3.6399, time 90.17ms, mfu 0.02%\n",
      "step 1750: train loss 1.8362, val loss 1.9516\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 1800: loss 3.6349, time 40.79ms, mfu 0.02%\n",
      "iter 1900: loss 3.6722, time 73.07ms, mfu 0.02%\n",
      "step 2000: train loss 1.7837, val loss 1.9706\n",
      "iter 2000: loss 3.3426, time 622.02ms, mfu 0.02%\n",
      "iter 2100: loss 3.4822, time 64.41ms, mfu 0.02%\n",
      "iter 2200: loss 3.6512, time 103.97ms, mfu 0.02%\n",
      "step 2250: train loss 1.7920, val loss 1.9081\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 2300: loss 3.6083, time 87.06ms, mfu 0.02%\n",
      "iter 2400: loss 3.4459, time 39.98ms, mfu 0.02%\n",
      "step 2500: train loss 1.7558, val loss 1.8664\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 2500: loss 3.4193, time 770.72ms, mfu 0.02%\n",
      "iter 2600: loss 3.3799, time 35.31ms, mfu 0.02%\n",
      "iter 2700: loss 3.3102, time 40.43ms, mfu 0.02%\n",
      "step 2750: train loss 1.7404, val loss 1.8929\n",
      "iter 2800: loss 3.5514, time 36.11ms, mfu 0.02%\n",
      "iter 2900: loss 3.3948, time 38.18ms, mfu 0.02%\n",
      "step 3000: train loss 1.6980, val loss 1.8129\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 3000: loss 3.6166, time 920.03ms, mfu 0.02%\n",
      "iter 3100: loss 3.6040, time 59.57ms, mfu 0.02%\n",
      "iter 3200: loss 3.4911, time 60.02ms, mfu 0.02%\n",
      "step 3250: train loss 1.6146, val loss 1.8078\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 3300: loss 3.5128, time 46.42ms, mfu 0.02%\n",
      "iter 3400: loss 3.4714, time 38.02ms, mfu 0.02%\n",
      "step 3500: train loss 1.6562, val loss 1.8179\n",
      "iter 3500: loss 3.6951, time 681.02ms, mfu 0.02%\n",
      "iter 3600: loss 3.3237, time 65.06ms, mfu 0.02%\n",
      "iter 3700: loss 3.3275, time 82.39ms, mfu 0.02%\n",
      "step 3750: train loss 1.6068, val loss 1.7749\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 3800: loss 3.2509, time 47.70ms, mfu 0.02%\n",
      "iter 3900: loss 2.9427, time 72.63ms, mfu 0.02%\n",
      "step 4000: train loss 1.5870, val loss 1.7872\n",
      "iter 4000: loss 3.2242, time 1063.74ms, mfu 0.02%\n",
      "iter 4100: loss 3.4015, time 67.30ms, mfu 0.02%\n",
      "iter 4200: loss 3.0265, time 107.55ms, mfu 0.02%\n",
      "step 4250: train loss 1.5429, val loss 1.7368\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 4300: loss 3.2290, time 137.07ms, mfu 0.02%\n",
      "iter 4400: loss 3.2680, time 74.48ms, mfu 0.02%\n",
      "step 4500: train loss 1.5526, val loss 1.7454\n",
      "iter 4500: loss 3.0501, time 680.47ms, mfu 0.02%\n",
      "iter 4600: loss 3.2724, time 47.08ms, mfu 0.02%\n",
      "iter 4700: loss 3.0548, time 107.38ms, mfu 0.02%\n",
      "step 4750: train loss 1.5503, val loss 1.7123\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 4800: loss 3.1336, time 106.63ms, mfu 0.02%\n",
      "iter 4900: loss 3.2014, time 88.54ms, mfu 0.02%\n",
      "step 5000: train loss 1.5220, val loss 1.7054\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 5000: loss 2.9260, time 763.95ms, mfu 0.01%\n",
      "iter 5100: loss 2.9772, time 69.91ms, mfu 0.01%\n",
      "iter 5200: loss 2.9902, time 50.42ms, mfu 0.02%\n",
      "step 5250: train loss 1.4987, val loss 1.7162\n",
      "iter 5300: loss 3.0663, time 91.66ms, mfu 0.02%\n",
      "iter 5400: loss 2.8487, time 353.99ms, mfu 0.01%\n",
      "step 5500: train loss 1.5136, val loss 1.6604\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 5500: loss 2.8857, time 858.16ms, mfu 0.01%\n",
      "iter 5600: loss 2.9960, time 140.64ms, mfu 0.01%\n",
      "iter 5700: loss 2.9093, time 104.73ms, mfu 0.01%\n",
      "step 5750: train loss 1.4837, val loss 1.6732\n",
      "iter 5800: loss 3.1263, time 41.09ms, mfu 0.01%\n",
      "iter 5900: loss 2.9933, time 289.95ms, mfu 0.01%\n",
      "step 6000: train loss 1.5180, val loss 1.6296\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 6000: loss 2.9676, time 4910.22ms, mfu 0.01%\n",
      "iter 6100: loss 3.1240, time 58.01ms, mfu 0.01%\n",
      "iter 6200: loss 2.6805, time 199.42ms, mfu 0.01%\n",
      "step 6250: train loss 1.4672, val loss 1.6526\n",
      "iter 6300: loss 2.7712, time 54.11ms, mfu 0.01%\n",
      "iter 6400: loss 2.9955, time 79.53ms, mfu 0.01%\n",
      "step 6500: train loss 1.4456, val loss 1.6376\n",
      "iter 6500: loss 2.8111, time 2267.35ms, mfu 0.01%\n",
      "iter 6600: loss 2.7244, time 100.71ms, mfu 0.01%\n",
      "iter 6700: loss 2.7815, time 60.28ms, mfu 0.01%\n",
      "step 6750: train loss 1.4586, val loss 1.6527\n",
      "iter 6800: loss 2.9402, time 63.94ms, mfu 0.01%\n",
      "iter 6900: loss 3.1275, time 116.31ms, mfu 0.01%\n",
      "step 7000: train loss 1.4260, val loss 1.6433\n",
      "iter 7000: loss 2.8748, time 946.04ms, mfu 0.01%\n",
      "iter 7100: loss 3.0652, time 94.69ms, mfu 0.01%\n",
      "iter 7200: loss 2.9248, time 156.13ms, mfu 0.01%\n",
      "step 7250: train loss 1.4440, val loss 1.6310\n",
      "iter 7300: loss 2.7507, time 134.86ms, mfu 0.01%\n",
      "iter 7400: loss 2.7288, time 96.44ms, mfu 0.01%\n",
      "step 7500: train loss 1.4424, val loss 1.6166\n",
      "saving checkpoint to out/agg_0.02_1600\n",
      "iter 7500: loss 2.9157, time 1268.74ms, mfu 0.01%\n",
      "iter 7600: loss 2.8898, time 171.71ms, mfu 0.01%\n",
      "iter 7700: loss 3.0540, time 124.36ms, mfu 0.01%\n",
      "step 7750: train loss 1.4351, val loss 1.6671\n",
      "iter 7800: loss 2.7719, time 61.41ms, mfu 0.01%\n",
      "iter 7900: loss 2.8283, time 61.70ms, mfu 0.01%\n",
      "step 8000: train loss 1.4563, val loss 1.6270\n",
      "iter 8000: loss 2.6954, time 729.58ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "# agg\n",
    "args = {\n",
    "    \"out_dir\": f\"out/agg_0.02_1600\",\n",
    "    \"model_type\": \"agg\",\n",
    "    \"agg_alpha\": 0.02,\n",
    "    \"agg_k\": 1600\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8bd4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/agg_0.2_1600\n",
      "Overriding: model_type = agg\n",
      "Overriding: agg_alpha = 0.2\n",
      "Overriding: agg_k = 1600\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 9.7377, time 1279.04ms, mfu -100.00%\n",
      "iter 100: loss 5.6337, time 82.93ms, mfu 0.02%\n",
      "iter 200: loss 5.0233, time 32.64ms, mfu 0.02%\n",
      "step 250: train loss 2.4681, val loss 2.4743\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 300: loss 4.8893, time 54.98ms, mfu 0.02%\n",
      "iter 400: loss 4.9253, time 40.54ms, mfu 0.02%\n",
      "step 500: train loss 2.2642, val loss 2.3010\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 500: loss 4.4691, time 731.19ms, mfu 0.02%\n",
      "iter 600: loss 4.3960, time 86.29ms, mfu 0.02%\n",
      "iter 700: loss 4.2723, time 34.73ms, mfu 0.02%\n",
      "step 750: train loss 2.1581, val loss 2.1735\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 800: loss 4.4952, time 58.22ms, mfu 0.02%\n",
      "iter 900: loss 4.0324, time 45.53ms, mfu 0.02%\n",
      "step 1000: train loss 2.0177, val loss 2.0826\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 1000: loss 4.2212, time 1583.68ms, mfu 0.02%\n",
      "iter 1100: loss 4.0296, time 91.03ms, mfu 0.02%\n",
      "iter 1200: loss 3.9349, time 56.42ms, mfu 0.02%\n",
      "step 1250: train loss 1.9307, val loss 2.0503\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 1300: loss 3.8771, time 93.13ms, mfu 0.02%\n",
      "iter 1400: loss 3.7265, time 53.11ms, mfu 0.02%\n",
      "step 1500: train loss 1.9248, val loss 2.0080\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 1500: loss 3.7603, time 867.62ms, mfu 0.02%\n",
      "iter 1600: loss 3.8719, time 86.12ms, mfu 0.02%\n",
      "iter 1700: loss 3.6148, time 94.97ms, mfu 0.02%\n",
      "step 1750: train loss 1.8445, val loss 1.9674\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 1800: loss 3.6336, time 151.31ms, mfu 0.02%\n",
      "iter 1900: loss 3.6445, time 87.97ms, mfu 0.02%\n",
      "step 2000: train loss 1.7837, val loss 1.9963\n",
      "iter 2000: loss 3.3136, time 879.41ms, mfu 0.01%\n",
      "iter 2100: loss 3.4988, time 54.50ms, mfu 0.02%\n",
      "iter 2200: loss 3.6080, time 84.05ms, mfu 0.02%\n",
      "step 2250: train loss 1.8129, val loss 1.9354\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 2300: loss 3.6063, time 106.88ms, mfu 0.01%\n",
      "iter 2400: loss 3.3929, time 65.36ms, mfu 0.02%\n",
      "step 2500: train loss 1.7557, val loss 1.8665\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 2500: loss 3.3957, time 1334.88ms, mfu 0.01%\n",
      "iter 2600: loss 3.3891, time 100.38ms, mfu 0.01%\n",
      "iter 2700: loss 3.2542, time 65.49ms, mfu 0.01%\n",
      "step 2750: train loss 1.7400, val loss 1.9053\n",
      "iter 2800: loss 3.5330, time 111.13ms, mfu 0.01%\n",
      "iter 2900: loss 3.4295, time 43.26ms, mfu 0.02%\n",
      "step 3000: train loss 1.7015, val loss 1.8372\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 3000: loss 3.6651, time 1369.15ms, mfu 0.01%\n",
      "iter 3100: loss 3.5752, time 43.55ms, mfu 0.02%\n",
      "iter 3200: loss 3.4867, time 41.97ms, mfu 0.02%\n",
      "step 3250: train loss 1.6137, val loss 1.8234\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 3300: loss 3.4673, time 770.66ms, mfu 0.02%\n",
      "iter 3400: loss 3.5166, time 75.15ms, mfu 0.02%\n",
      "step 3500: train loss 1.6614, val loss 1.8284\n",
      "iter 3500: loss 3.7196, time 712.59ms, mfu 0.01%\n",
      "iter 3600: loss 3.3194, time 77.31ms, mfu 0.01%\n",
      "iter 3700: loss 3.2701, time 153.77ms, mfu 0.01%\n",
      "step 3750: train loss 1.6154, val loss 1.7841\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 3800: loss 3.3301, time 44.70ms, mfu 0.02%\n",
      "iter 3900: loss 2.9728, time 102.73ms, mfu 0.02%\n",
      "step 4000: train loss 1.5917, val loss 1.7870\n",
      "iter 4000: loss 3.2320, time 688.58ms, mfu 0.01%\n",
      "iter 4100: loss 3.3536, time 67.28ms, mfu 0.01%\n",
      "iter 4200: loss 2.9385, time 86.02ms, mfu 0.01%\n",
      "step 4250: train loss 1.5399, val loss 1.7309\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 4300: loss 3.2590, time 41.53ms, mfu 0.02%\n",
      "iter 4400: loss 3.2388, time 45.26ms, mfu 0.02%\n",
      "step 4500: train loss 1.5514, val loss 1.7492\n",
      "iter 4500: loss 3.0495, time 852.94ms, mfu 0.02%\n",
      "iter 4600: loss 3.2838, time 47.43ms, mfu 0.02%\n",
      "iter 4700: loss 3.0857, time 74.91ms, mfu 0.02%\n",
      "step 4750: train loss 1.5444, val loss 1.7136\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 4800: loss 3.1123, time 136.59ms, mfu 0.02%\n",
      "iter 4900: loss 3.2247, time 44.67ms, mfu 0.02%\n",
      "step 5000: train loss 1.5235, val loss 1.7084\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 5000: loss 2.9033, time 890.06ms, mfu 0.02%\n",
      "iter 5100: loss 2.9397, time 109.13ms, mfu 0.02%\n",
      "iter 5200: loss 2.9812, time 93.60ms, mfu 0.02%\n",
      "step 5250: train loss 1.5001, val loss 1.7188\n",
      "iter 5300: loss 3.1064, time 224.26ms, mfu 0.01%\n",
      "iter 5400: loss 2.8834, time 61.94ms, mfu 0.01%\n",
      "step 5500: train loss 1.5125, val loss 1.6772\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 5500: loss 2.9378, time 928.96ms, mfu 0.01%\n",
      "iter 5600: loss 2.9735, time 114.04ms, mfu 0.01%\n",
      "iter 5700: loss 2.9491, time 178.72ms, mfu 0.01%\n",
      "step 5750: train loss 1.4799, val loss 1.6753\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 5800: loss 3.1174, time 46.35ms, mfu 0.01%\n",
      "iter 5900: loss 3.0460, time 46.39ms, mfu 0.02%\n",
      "step 6000: train loss 1.5157, val loss 1.6477\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 6000: loss 2.9897, time 1010.00ms, mfu 0.01%\n",
      "iter 6100: loss 3.1766, time 44.12ms, mfu 0.02%\n",
      "iter 6200: loss 2.6895, time 100.99ms, mfu 0.02%\n",
      "step 6250: train loss 1.4696, val loss 1.6479\n",
      "iter 6300: loss 2.7730, time 75.90ms, mfu 0.02%\n",
      "iter 6400: loss 3.0469, time 90.97ms, mfu 0.02%\n",
      "step 6500: train loss 1.4417, val loss 1.6300\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 6500: loss 2.8064, time 1170.66ms, mfu 0.01%\n",
      "iter 6600: loss 2.7443, time 65.68ms, mfu 0.01%\n",
      "iter 6700: loss 2.7655, time 72.74ms, mfu 0.01%\n",
      "step 6750: train loss 1.4533, val loss 1.6508\n",
      "iter 6800: loss 2.8830, time 43.56ms, mfu 0.02%\n",
      "iter 6900: loss 3.1477, time 89.50ms, mfu 0.02%\n",
      "step 7000: train loss 1.4261, val loss 1.6350\n",
      "iter 7000: loss 2.9084, time 876.13ms, mfu 0.01%\n",
      "iter 7100: loss 3.1441, time 94.63ms, mfu 0.01%\n",
      "iter 7200: loss 2.9106, time 69.37ms, mfu 0.01%\n",
      "step 7250: train loss 1.4474, val loss 1.6349\n",
      "iter 7300: loss 2.7506, time 44.76ms, mfu 0.02%\n",
      "iter 7400: loss 2.7106, time 102.23ms, mfu 0.02%\n",
      "step 7500: train loss 1.4420, val loss 1.6119\n",
      "saving checkpoint to out/agg_0.2_1600\n",
      "iter 7500: loss 2.8840, time 1509.46ms, mfu 0.01%\n",
      "iter 7600: loss 2.9550, time 49.34ms, mfu 0.02%\n",
      "iter 7700: loss 3.0373, time 72.73ms, mfu 0.02%\n",
      "step 7750: train loss 1.4329, val loss 1.6693\n",
      "iter 7800: loss 2.7886, time 166.78ms, mfu 0.01%\n",
      "iter 7900: loss 2.8395, time 86.45ms, mfu 0.01%\n",
      "step 8000: train loss 1.4601, val loss 1.6240\n",
      "iter 8000: loss 2.6740, time 1368.46ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "# agg\n",
    "args = {\n",
    "    \"out_dir\": f\"out/agg_0.2_1600\",\n",
    "    \"model_type\": \"agg\",\n",
    "    \"agg_alpha\": 0.2,\n",
    "    \"agg_k\": 1600\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "285cffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/cosreg_1\n",
      "Overriding: model_type = cosreg\n",
      "Overriding: cosreg_gamma = 1\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8687, time 1924.51ms, mfu -100.00%\n",
      "iter 100: loss 2.8265, time 31.02ms, mfu 0.04%\n",
      "iter 200: loss 2.4964, time 63.72ms, mfu 0.04%\n",
      "step 250: train loss 2.4812, val loss 2.4866\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 300: loss 2.4093, time 131.47ms, mfu 0.04%\n",
      "iter 400: loss 2.4048, time 114.18ms, mfu 0.03%\n",
      "step 500: train loss 2.2543, val loss 2.2940\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 500: loss 2.2175, time 754.99ms, mfu 0.03%\n",
      "iter 600: loss 2.1794, time 51.11ms, mfu 0.03%\n",
      "iter 700: loss 2.1077, time 1109.76ms, mfu 0.03%\n",
      "step 750: train loss 2.1293, val loss 2.1564\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 800: loss 2.2082, time 52.09ms, mfu 0.03%\n",
      "iter 900: loss 2.0116, time 38.85ms, mfu 0.03%\n",
      "step 1000: train loss 2.0206, val loss 2.0763\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 1000: loss 2.0802, time 640.66ms, mfu 0.02%\n",
      "iter 1100: loss 1.9642, time 93.67ms, mfu 0.02%\n",
      "iter 1200: loss 1.9303, time 37.03ms, mfu 0.02%\n",
      "step 1250: train loss 1.9375, val loss 2.0596\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 1300: loss 1.9247, time 42.08ms, mfu 0.03%\n",
      "iter 1400: loss 1.8368, time 36.49ms, mfu 0.03%\n",
      "step 1500: train loss 1.8893, val loss 1.9769\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 1500: loss 1.8370, time 623.29ms, mfu 0.02%\n",
      "iter 1600: loss 1.9074, time 33.83ms, mfu 0.03%\n",
      "iter 1700: loss 1.7870, time 38.31ms, mfu 0.03%\n",
      "step 1750: train loss 1.8236, val loss 1.9460\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 1800: loss 1.7643, time 51.05ms, mfu 0.03%\n",
      "iter 1900: loss 1.8090, time 33.91ms, mfu 0.03%\n",
      "step 2000: train loss 1.7734, val loss 1.9512\n",
      "iter 2000: loss 1.6248, time 671.25ms, mfu 0.02%\n",
      "iter 2100: loss 1.7502, time 35.36ms, mfu 0.03%\n",
      "iter 2200: loss 1.8075, time 39.90ms, mfu 0.03%\n",
      "step 2250: train loss 1.8044, val loss 1.9260\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 2300: loss 1.7745, time 48.79ms, mfu 0.03%\n",
      "iter 2400: loss 1.7012, time 43.48ms, mfu 0.03%\n",
      "step 2500: train loss 1.7481, val loss 1.8535\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 2500: loss 1.7231, time 779.06ms, mfu 0.02%\n",
      "iter 2600: loss 1.6736, time 41.49ms, mfu 0.02%\n",
      "iter 2700: loss 1.6299, time 44.01ms, mfu 0.03%\n",
      "step 2750: train loss 1.7328, val loss 1.8837\n",
      "iter 2800: loss 1.7617, time 76.39ms, mfu 0.02%\n",
      "iter 2900: loss 1.6549, time 34.12ms, mfu 0.03%\n",
      "step 3000: train loss 1.6918, val loss 1.8155\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 3000: loss 1.8317, time 677.89ms, mfu 0.02%\n",
      "iter 3100: loss 1.7642, time 38.62ms, mfu 0.02%\n",
      "iter 3200: loss 1.7339, time 34.15ms, mfu 0.03%\n",
      "step 3250: train loss 1.6261, val loss 1.8125\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 3300: loss 1.6792, time 56.89ms, mfu 0.03%\n",
      "iter 3400: loss 1.7266, time 57.72ms, mfu 0.03%\n",
      "step 3500: train loss 1.6363, val loss 1.7986\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 3500: loss 1.8296, time 765.41ms, mfu 0.02%\n",
      "iter 3600: loss 1.6435, time 75.46ms, mfu 0.02%\n",
      "iter 3700: loss 1.6475, time 69.47ms, mfu 0.02%\n",
      "step 3750: train loss 1.6049, val loss 1.7711\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 3800: loss 1.6054, time 69.59ms, mfu 0.02%\n",
      "iter 3900: loss 1.4700, time 88.90ms, mfu 0.02%\n",
      "step 4000: train loss 1.5843, val loss 1.7746\n",
      "iter 4000: loss 1.5887, time 528.37ms, mfu 0.02%\n",
      "iter 4100: loss 1.6820, time 34.49ms, mfu 0.02%\n",
      "iter 4200: loss 1.4784, time 100.81ms, mfu 0.02%\n",
      "step 4250: train loss 1.5354, val loss 1.7248\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 4300: loss 1.5925, time 34.57ms, mfu 0.02%\n",
      "iter 4400: loss 1.6382, time 36.13ms, mfu 0.02%\n",
      "step 4500: train loss 1.5452, val loss 1.7363\n",
      "iter 4500: loss 1.5230, time 549.64ms, mfu 0.02%\n",
      "iter 4600: loss 1.6442, time 52.04ms, mfu 0.02%\n",
      "iter 4700: loss 1.4977, time 34.55ms, mfu 0.02%\n",
      "step 4750: train loss 1.5435, val loss 1.7132\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 4800: loss 1.5059, time 34.29ms, mfu 0.02%\n",
      "iter 4900: loss 1.5929, time 49.20ms, mfu 0.02%\n",
      "step 5000: train loss 1.5169, val loss 1.7062\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 5000: loss 1.4405, time 839.93ms, mfu 0.02%\n",
      "iter 5100: loss 1.4502, time 72.38ms, mfu 0.02%\n",
      "iter 5200: loss 1.4653, time 32.95ms, mfu 0.02%\n",
      "step 5250: train loss 1.4908, val loss 1.7084\n",
      "iter 5300: loss 1.5380, time 79.45ms, mfu 0.02%\n",
      "iter 5400: loss 1.4101, time 45.69ms, mfu 0.02%\n",
      "step 5500: train loss 1.5009, val loss 1.6592\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 5500: loss 1.4311, time 594.16ms, mfu 0.02%\n",
      "iter 5600: loss 1.4873, time 42.54ms, mfu 0.02%\n",
      "iter 5700: loss 1.4425, time 89.04ms, mfu 0.02%\n",
      "step 5750: train loss 1.4718, val loss 1.6715\n",
      "iter 5800: loss 1.5566, time 54.90ms, mfu 0.02%\n",
      "iter 5900: loss 1.5015, time 67.25ms, mfu 0.02%\n",
      "step 6000: train loss 1.5103, val loss 1.6389\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 6000: loss 1.4840, time 819.41ms, mfu 0.02%\n",
      "iter 6100: loss 1.5550, time 33.51ms, mfu 0.02%\n",
      "iter 6200: loss 1.3251, time 36.20ms, mfu 0.02%\n",
      "step 6250: train loss 1.4605, val loss 1.6466\n",
      "iter 6300: loss 1.3806, time 80.35ms, mfu 0.02%\n",
      "iter 6400: loss 1.4898, time 57.92ms, mfu 0.02%\n",
      "step 6500: train loss 1.4368, val loss 1.6213\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 6500: loss 1.4008, time 927.58ms, mfu 0.02%\n",
      "iter 6600: loss 1.3420, time 72.79ms, mfu 0.02%\n",
      "iter 6700: loss 1.3720, time 45.12ms, mfu 0.02%\n",
      "step 6750: train loss 1.4480, val loss 1.6418\n",
      "iter 6800: loss 1.4380, time 43.87ms, mfu 0.02%\n",
      "iter 6900: loss 1.5654, time 38.39ms, mfu 0.02%\n",
      "step 7000: train loss 1.4201, val loss 1.6334\n",
      "iter 7000: loss 1.4545, time 572.82ms, mfu 0.02%\n",
      "iter 7100: loss 1.5298, time 34.48ms, mfu 0.02%\n",
      "iter 7200: loss 1.4534, time 29.35ms, mfu 0.02%\n",
      "step 7250: train loss 1.4387, val loss 1.6337\n",
      "iter 7300: loss 1.3507, time 55.10ms, mfu 0.02%\n",
      "iter 7400: loss 1.3358, time 78.91ms, mfu 0.02%\n",
      "step 7500: train loss 1.4347, val loss 1.6121\n",
      "saving checkpoint to out/cosreg_1\n",
      "iter 7500: loss 1.4272, time 723.48ms, mfu 0.02%\n",
      "iter 7600: loss 1.4376, time 57.19ms, mfu 0.02%\n",
      "iter 7700: loss 1.5188, time 38.79ms, mfu 0.02%\n",
      "step 7750: train loss 1.4291, val loss 1.6652\n",
      "iter 7800: loss 1.3808, time 62.02ms, mfu 0.02%\n",
      "iter 7900: loss 1.4029, time 33.72ms, mfu 0.02%\n",
      "step 8000: train loss 1.4491, val loss 1.6191\n",
      "iter 8000: loss 1.3153, time 688.80ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "# cosreg\n",
    "args = {\n",
    "    \"out_dir\": f\"out/cosreg_1\",\n",
    "    \"model_type\": \"cosreg\",\n",
    "    \"cosreg_gamma\": 1\n",
    "\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cccdbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/adv_0.05\n",
      "Overriding: model_type = adv\n",
      "Overriding: adv_alpha = 0.005\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8813, time 1645.67ms, mfu -100.00%\n",
      "iter 100: loss 2.8129, time 110.67ms, mfu 0.01%\n",
      "iter 200: loss 2.5280, time 66.07ms, mfu 0.01%\n",
      "step 250: train loss 2.4780, val loss 2.4917\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 300: loss 2.4797, time 29.55ms, mfu 0.02%\n",
      "iter 400: loss 2.4896, time 29.79ms, mfu 0.02%\n",
      "step 500: train loss 2.2684, val loss 2.3073\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 500: loss 2.2608, time 552.54ms, mfu 0.02%\n",
      "iter 600: loss 2.2233, time 35.11ms, mfu 0.02%\n",
      "iter 700: loss 2.1874, time 84.12ms, mfu 0.02%\n",
      "step 750: train loss 2.1524, val loss 2.1693\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 800: loss 2.2839, time 78.55ms, mfu 0.02%\n",
      "iter 900: loss 2.0709, time 42.71ms, mfu 0.02%\n",
      "step 1000: train loss 2.0301, val loss 2.0880\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 1000: loss 2.1397, time 995.26ms, mfu 0.02%\n",
      "iter 1100: loss 2.0907, time 42.63ms, mfu 0.02%\n",
      "iter 1200: loss 1.9851, time 44.56ms, mfu 0.02%\n",
      "step 1250: train loss 1.9405, val loss 2.0646\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 1300: loss 1.9648, time 48.46ms, mfu 0.02%\n",
      "iter 1400: loss 1.8853, time 48.98ms, mfu 0.02%\n",
      "step 1500: train loss 1.9127, val loss 1.9952\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 1500: loss 1.9442, time 995.42ms, mfu 0.02%\n",
      "iter 1600: loss 1.9394, time 43.99ms, mfu 0.02%\n",
      "iter 1700: loss 1.8650, time 42.29ms, mfu 0.02%\n",
      "step 1750: train loss 1.8314, val loss 1.9453\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 1800: loss 1.8447, time 55.30ms, mfu 0.02%\n",
      "iter 1900: loss 1.8616, time 90.75ms, mfu 0.02%\n",
      "step 2000: train loss 1.7921, val loss 1.9882\n",
      "iter 2000: loss 1.7212, time 718.82ms, mfu 0.02%\n",
      "iter 2100: loss 1.7653, time 43.27ms, mfu 0.02%\n",
      "iter 2200: loss 1.8632, time 49.44ms, mfu 0.02%\n",
      "step 2250: train loss 1.7914, val loss 1.9224\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 2300: loss 1.8593, time 98.25ms, mfu 0.02%\n",
      "iter 2400: loss 1.7919, time 46.88ms, mfu 0.02%\n",
      "step 2500: train loss 1.7540, val loss 1.8806\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 2500: loss 1.7357, time 1015.78ms, mfu 0.02%\n",
      "iter 2600: loss 1.7114, time 224.25ms, mfu 0.02%\n",
      "iter 2700: loss 1.6788, time 125.80ms, mfu 0.02%\n",
      "step 2750: train loss 1.7349, val loss 1.8867\n",
      "iter 2800: loss 1.8231, time 215.33ms, mfu 0.02%\n",
      "iter 2900: loss 1.7532, time 48.10ms, mfu 0.02%\n",
      "step 3000: train loss 1.6969, val loss 1.8207\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 3000: loss 1.8706, time 1275.91ms, mfu 0.02%\n",
      "iter 3100: loss 1.8443, time 75.00ms, mfu 0.02%\n",
      "iter 3200: loss 1.7755, time 85.74ms, mfu 0.02%\n",
      "step 3250: train loss 1.6364, val loss 1.8267\n",
      "iter 3300: loss 1.7769, time 80.20ms, mfu 0.02%\n",
      "iter 3400: loss 1.7850, time 85.91ms, mfu 0.02%\n",
      "step 3500: train loss 1.6602, val loss 1.8325\n",
      "iter 3500: loss 1.8928, time 7473.17ms, mfu 0.01%\n",
      "iter 3600: loss 1.7083, time 288.83ms, mfu 0.01%\n",
      "iter 3700: loss 1.7090, time 89.05ms, mfu 0.01%\n",
      "step 3750: train loss 1.6236, val loss 1.7858\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 3800: loss 1.6559, time 106.38ms, mfu 0.01%\n",
      "iter 3900: loss 1.5223, time 85.80ms, mfu 0.01%\n",
      "step 4000: train loss 1.5955, val loss 1.7806\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 4000: loss 1.6336, time 965.24ms, mfu 0.01%\n",
      "iter 4100: loss 1.7474, time 59.85ms, mfu 0.01%\n",
      "iter 4200: loss 1.5259, time 46.41ms, mfu 0.01%\n",
      "step 4250: train loss 1.5475, val loss 1.7410\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 4300: loss 1.6428, time 92.31ms, mfu 0.01%\n",
      "iter 4400: loss 1.6493, time 52.23ms, mfu 0.02%\n",
      "step 4500: train loss 1.5541, val loss 1.7454\n",
      "iter 4500: loss 1.5587, time 1074.88ms, mfu 0.01%\n",
      "iter 4600: loss 1.6715, time 49.34ms, mfu 0.02%\n",
      "iter 4700: loss 1.5448, time 45.27ms, mfu 0.02%\n",
      "step 4750: train loss 1.5486, val loss 1.7152\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 4800: loss 1.5729, time 46.48ms, mfu 0.02%\n",
      "iter 4900: loss 1.6454, time 48.80ms, mfu 0.02%\n",
      "step 5000: train loss 1.5197, val loss 1.7008\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 5000: loss 1.4968, time 881.22ms, mfu 0.02%\n",
      "iter 5100: loss 1.5030, time 73.55ms, mfu 0.02%\n",
      "iter 5200: loss 1.5544, time 47.65ms, mfu 0.02%\n",
      "step 5250: train loss 1.4992, val loss 1.7195\n",
      "iter 5300: loss 1.6059, time 45.53ms, mfu 0.02%\n",
      "iter 5400: loss 1.4621, time 70.88ms, mfu 0.02%\n",
      "step 5500: train loss 1.5125, val loss 1.6673\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 5500: loss 1.4724, time 820.56ms, mfu 0.02%\n",
      "iter 5600: loss 1.5240, time 107.77ms, mfu 0.02%\n",
      "iter 5700: loss 1.5175, time 48.16ms, mfu 0.02%\n",
      "step 5750: train loss 1.4820, val loss 1.6742\n",
      "iter 5800: loss 1.6014, time 48.94ms, mfu 0.02%\n",
      "iter 5900: loss 1.5698, time 93.16ms, mfu 0.02%\n",
      "step 6000: train loss 1.5193, val loss 1.6467\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 6000: loss 1.5485, time 1044.32ms, mfu 0.02%\n",
      "iter 6100: loss 1.6378, time 77.57ms, mfu 0.02%\n",
      "iter 6200: loss 1.3537, time 73.51ms, mfu 0.02%\n",
      "step 6250: train loss 1.4712, val loss 1.6481\n",
      "iter 6300: loss 1.4214, time 72.65ms, mfu 0.02%\n",
      "iter 6400: loss 1.5424, time 104.24ms, mfu 0.02%\n",
      "step 6500: train loss 1.4430, val loss 1.6367\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 6500: loss 1.4273, time 1239.43ms, mfu 0.01%\n",
      "iter 6600: loss 1.4359, time 43.23ms, mfu 0.02%\n",
      "iter 6700: loss 1.4265, time 63.48ms, mfu 0.02%\n",
      "step 6750: train loss 1.4562, val loss 1.6427\n",
      "iter 6800: loss 1.4900, time 48.69ms, mfu 0.02%\n",
      "iter 6900: loss 1.5932, time 58.31ms, mfu 0.02%\n",
      "step 7000: train loss 1.4262, val loss 1.6332\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 7000: loss 1.4807, time 1127.37ms, mfu 0.02%\n",
      "iter 7100: loss 1.5741, time 69.82ms, mfu 0.02%\n",
      "iter 7200: loss 1.5015, time 59.93ms, mfu 0.02%\n",
      "step 7250: train loss 1.4447, val loss 1.6375\n",
      "iter 7300: loss 1.4235, time 45.19ms, mfu 0.02%\n",
      "iter 7400: loss 1.3804, time 126.93ms, mfu 0.02%\n",
      "step 7500: train loss 1.4431, val loss 1.6196\n",
      "saving checkpoint to out/adv_0.05\n",
      "iter 7500: loss 1.4937, time 990.22ms, mfu 0.02%\n",
      "iter 7600: loss 1.5022, time 78.00ms, mfu 0.02%\n",
      "iter 7700: loss 1.5430, time 51.98ms, mfu 0.02%\n",
      "step 7750: train loss 1.4430, val loss 1.6759\n",
      "iter 7800: loss 1.4281, time 81.58ms, mfu 0.02%\n",
      "iter 7900: loss 1.4532, time 71.95ms, mfu 0.02%\n",
      "step 8000: train loss 1.4554, val loss 1.6334\n",
      "iter 8000: loss 1.3885, time 908.70ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"out_dir\": f\"out/adv_0.05\",\n",
    "    \"model_type\": \"adv\",\n",
    "    \"adv_alpha\": 0.005\n",
    "\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57e711b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1_0.46\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1\n",
      "Overriding: temperature = 0.46\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8602, time 732.11ms, mfu -100.00%\n",
      "iter 100: loss 2.6539, time 30.34ms, mfu 0.04%\n",
      "iter 200: loss 2.2550, time 30.54ms, mfu 0.04%\n",
      "step 250: train loss 2.5833, val loss 2.5915\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 300: loss 2.2192, time 31.36ms, mfu 0.04%\n",
      "iter 400: loss 2.2363, time 31.40ms, mfu 0.04%\n",
      "step 500: train loss 2.3622, val loss 2.4169\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 500: loss 2.0162, time 645.17ms, mfu 0.04%\n",
      "iter 600: loss 1.9987, time 29.46ms, mfu 0.04%\n",
      "iter 700: loss 1.9199, time 33.09ms, mfu 0.04%\n",
      "step 750: train loss 2.2137, val loss 2.2313\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 800: loss 1.9460, time 36.64ms, mfu 0.04%\n",
      "iter 900: loss 1.7713, time 56.98ms, mfu 0.04%\n",
      "step 1000: train loss 2.0986, val loss 2.1570\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 1000: loss 1.8226, time 513.67ms, mfu 0.03%\n",
      "iter 1100: loss 1.7913, time 36.47ms, mfu 0.03%\n",
      "iter 1200: loss 1.6904, time 43.44ms, mfu 0.03%\n",
      "step 1250: train loss 1.9945, val loss 2.1304\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 1300: loss 1.6738, time 34.89ms, mfu 0.03%\n",
      "iter 1400: loss 1.6167, time 34.07ms, mfu 0.03%\n",
      "step 1500: train loss 1.9482, val loss 2.0342\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 1500: loss 1.6409, time 742.60ms, mfu 0.03%\n",
      "iter 1600: loss 1.6854, time 64.31ms, mfu 0.03%\n",
      "iter 1700: loss 1.6037, time 40.52ms, mfu 0.03%\n",
      "step 1750: train loss 1.8891, val loss 2.0257\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 1800: loss 1.5540, time 154.94ms, mfu 0.03%\n",
      "iter 1900: loss 1.6023, time 37.55ms, mfu 0.03%\n",
      "step 2000: train loss 1.8313, val loss 2.0288\n",
      "iter 2000: loss 1.4764, time 695.06ms, mfu 0.03%\n",
      "iter 2100: loss 1.4984, time 42.94ms, mfu 0.03%\n",
      "iter 2200: loss 1.5752, time 80.41ms, mfu 0.03%\n",
      "step 2250: train loss 1.8514, val loss 1.9810\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 2300: loss 1.5468, time 39.32ms, mfu 0.03%\n",
      "iter 2400: loss 1.4925, time 64.43ms, mfu 0.03%\n",
      "step 2500: train loss 1.8074, val loss 1.9046\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 2500: loss 1.5116, time 676.80ms, mfu 0.02%\n",
      "iter 2600: loss 1.4570, time 42.39ms, mfu 0.02%\n",
      "iter 2700: loss 1.3732, time 91.69ms, mfu 0.02%\n",
      "step 2750: train loss 1.7904, val loss 1.9386\n",
      "iter 2800: loss 1.5357, time 42.34ms, mfu 0.02%\n",
      "iter 2900: loss 1.4545, time 39.39ms, mfu 0.02%\n",
      "step 3000: train loss 1.7546, val loss 1.8752\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 3000: loss 1.5749, time 652.26ms, mfu 0.02%\n",
      "iter 3100: loss 1.5304, time 72.37ms, mfu 0.02%\n",
      "iter 3200: loss 1.4749, time 37.77ms, mfu 0.02%\n",
      "step 3250: train loss 1.6640, val loss 1.8539\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 3300: loss 1.4876, time 78.56ms, mfu 0.02%\n",
      "iter 3400: loss 1.4842, time 41.29ms, mfu 0.02%\n",
      "step 3500: train loss 1.7004, val loss 1.8606\n",
      "iter 3500: loss 1.5732, time 968.69ms, mfu 0.02%\n",
      "iter 3600: loss 1.4464, time 40.64ms, mfu 0.02%\n",
      "iter 3700: loss 1.4165, time 41.38ms, mfu 0.02%\n",
      "step 3750: train loss 1.6598, val loss 1.8325\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 3800: loss 1.3894, time 65.86ms, mfu 0.02%\n",
      "iter 3900: loss 1.2835, time 39.22ms, mfu 0.02%\n",
      "step 4000: train loss 1.6491, val loss 1.8340\n",
      "iter 4000: loss 1.3922, time 739.46ms, mfu 0.02%\n",
      "iter 4100: loss 1.4424, time 93.77ms, mfu 0.02%\n",
      "iter 4200: loss 1.3384, time 42.53ms, mfu 0.02%\n",
      "step 4250: train loss 1.5941, val loss 1.7758\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 4300: loss 1.3960, time 82.05ms, mfu 0.02%\n",
      "iter 4400: loss 1.3948, time 47.49ms, mfu 0.02%\n",
      "step 4500: train loss 1.6014, val loss 1.7877\n",
      "iter 4500: loss 1.3391, time 828.51ms, mfu 0.02%\n",
      "iter 4600: loss 1.4108, time 86.95ms, mfu 0.02%\n",
      "iter 4700: loss 1.2833, time 149.47ms, mfu 0.02%\n",
      "step 4750: train loss 1.6106, val loss 1.7640\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 4800: loss 1.3428, time 81.09ms, mfu 0.02%\n",
      "iter 4900: loss 1.3854, time 106.22ms, mfu 0.02%\n",
      "step 5000: train loss 1.5717, val loss 1.7594\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 5000: loss 1.2600, time 695.45ms, mfu 0.02%\n",
      "iter 5100: loss 1.2722, time 49.82ms, mfu 0.02%\n",
      "iter 5200: loss 1.2892, time 42.81ms, mfu 0.02%\n",
      "step 5250: train loss 1.5546, val loss 1.7533\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 5300: loss 1.3431, time 43.82ms, mfu 0.02%\n",
      "iter 5400: loss 1.2609, time 42.90ms, mfu 0.02%\n",
      "step 5500: train loss 1.5687, val loss 1.7106\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 5500: loss 1.2506, time 895.35ms, mfu 0.02%\n",
      "iter 5600: loss 1.2817, time 56.63ms, mfu 0.02%\n",
      "iter 5700: loss 1.2653, time 77.24ms, mfu 0.02%\n",
      "step 5750: train loss 1.5343, val loss 1.7174\n",
      "iter 5800: loss 1.3479, time 44.51ms, mfu 0.02%\n",
      "iter 5900: loss 1.3381, time 44.01ms, mfu 0.02%\n",
      "step 6000: train loss 1.5744, val loss 1.6747\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 6000: loss 1.3004, time 1117.64ms, mfu 0.02%\n",
      "iter 6100: loss 1.3581, time 136.95ms, mfu 0.02%\n",
      "iter 6200: loss 1.1628, time 171.98ms, mfu 0.02%\n",
      "step 6250: train loss 1.5303, val loss 1.6901\n",
      "iter 6300: loss 1.2159, time 70.01ms, mfu 0.02%\n",
      "iter 6400: loss 1.3247, time 93.68ms, mfu 0.02%\n",
      "step 6500: train loss 1.5015, val loss 1.6716\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 6500: loss 1.2183, time 920.86ms, mfu 0.02%\n",
      "iter 6600: loss 1.1961, time 42.22ms, mfu 0.02%\n",
      "iter 6700: loss 1.2131, time 79.07ms, mfu 0.02%\n",
      "step 6750: train loss 1.5147, val loss 1.6939\n",
      "iter 6800: loss 1.2750, time 41.65ms, mfu 0.02%\n",
      "iter 6900: loss 1.3552, time 79.38ms, mfu 0.02%\n",
      "step 7000: train loss 1.4841, val loss 1.6687\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 7000: loss 1.2846, time 1833.65ms, mfu 0.02%\n",
      "iter 7100: loss 1.3203, time 200.24ms, mfu 0.02%\n",
      "iter 7200: loss 1.2499, time 65.56ms, mfu 0.02%\n",
      "step 7250: train loss 1.5083, val loss 1.6773\n",
      "iter 7300: loss 1.1935, time 88.27ms, mfu 0.02%\n",
      "iter 7400: loss 1.1817, time 57.67ms, mfu 0.02%\n",
      "step 7500: train loss 1.4946, val loss 1.6586\n",
      "saving checkpoint to out/proposed_margin_1_0.46\n",
      "iter 7500: loss 1.2777, time 1077.29ms, mfu 0.01%\n",
      "iter 7600: loss 1.2279, time 89.18ms, mfu 0.01%\n",
      "iter 7700: loss 1.3292, time 43.79ms, mfu 0.02%\n",
      "step 7750: train loss 1.5017, val loss 1.7076\n",
      "iter 7800: loss 1.1855, time 90.81ms, mfu 0.02%\n",
      "iter 7900: loss 1.2341, time 97.20ms, mfu 0.02%\n",
      "step 8000: train loss 1.5076, val loss 1.6743\n",
      "iter 8000: loss 1.1815, time 902.69ms, mfu 0.01%\n",
      "done for margin=1; T=0.46\n"
     ]
    }
   ],
   "source": [
    "margin = 1\n",
    "temperature = 0.46\n",
    "args = {\n",
    "    \"out_dir\": f\"out/proposed_margin_{margin}_{T}\",\n",
    "    \"model_type\": \"proposed\",\n",
    "    \"margin\": float(margin),\n",
    "    \"temperature\": temperature\n",
    "}\n",
    "run_training(args)\n",
    "print(f'done for margin={margin}; T={T}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45a94fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.5\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.5\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.7428, time 543.37ms, mfu -100.00%\n",
      "iter 100: loss 2.3062, time 98.14ms, mfu 0.01%\n",
      "iter 200: loss 2.0388, time 45.45ms, mfu 0.01%\n",
      "step 250: train loss 3.4804, val loss 3.4946\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 300: loss 2.0462, time 92.14ms, mfu 0.01%\n",
      "iter 400: loss 1.9750, time 77.40ms, mfu 0.01%\n",
      "step 500: train loss 3.3373, val loss 3.3614\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 500: loss 1.7964, time 559.38ms, mfu 0.01%\n",
      "iter 600: loss 1.7720, time 45.63ms, mfu 0.01%\n",
      "iter 700: loss 1.6967, time 59.15ms, mfu 0.02%\n",
      "step 750: train loss 3.2146, val loss 3.2280\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 800: loss 1.7445, time 56.71ms, mfu 0.02%\n",
      "iter 900: loss 1.5866, time 68.72ms, mfu 0.02%\n",
      "step 1000: train loss 3.1466, val loss 3.1712\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 1000: loss 1.6185, time 751.55ms, mfu 0.02%\n",
      "iter 1100: loss 1.5768, time 81.73ms, mfu 0.02%\n",
      "iter 1200: loss 1.5380, time 90.72ms, mfu 0.02%\n",
      "step 1250: train loss 3.0818, val loss 3.1410\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 1300: loss 1.4764, time 75.89ms, mfu 0.02%\n",
      "iter 1400: loss 1.4572, time 84.77ms, mfu 0.02%\n",
      "step 1500: train loss 3.0587, val loss 3.0979\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 1500: loss 1.4574, time 692.17ms, mfu 0.01%\n",
      "iter 1600: loss 1.4851, time 79.48ms, mfu 0.01%\n",
      "iter 1700: loss 1.4102, time 104.01ms, mfu 0.01%\n",
      "step 1750: train loss 3.0360, val loss 3.0843\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 1800: loss 1.4058, time 66.52ms, mfu 0.01%\n",
      "iter 1900: loss 1.4275, time 49.46ms, mfu 0.02%\n",
      "step 2000: train loss 3.0098, val loss 3.0767\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 2000: loss 1.3243, time 737.70ms, mfu 0.01%\n",
      "iter 2100: loss 1.3612, time 91.45ms, mfu 0.01%\n",
      "iter 2200: loss 1.4330, time 112.66ms, mfu 0.01%\n",
      "step 2250: train loss 3.0200, val loss 3.0637\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 2300: loss 1.4051, time 83.96ms, mfu 0.01%\n",
      "iter 2400: loss 1.3666, time 49.76ms, mfu 0.02%\n",
      "step 2500: train loss 3.0028, val loss 3.0375\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 2500: loss 1.3680, time 839.87ms, mfu 0.01%\n",
      "iter 2600: loss 1.3375, time 317.86ms, mfu 0.01%\n",
      "iter 2700: loss 1.2625, time 51.16ms, mfu 0.01%\n",
      "step 2750: train loss 2.9940, val loss 3.0516\n",
      "iter 2800: loss 1.3881, time 35.57ms, mfu 0.02%\n",
      "iter 2900: loss 1.3374, time 166.47ms, mfu 0.02%\n",
      "step 3000: train loss 2.9759, val loss 3.0172\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 3000: loss 1.3974, time 695.14ms, mfu 0.01%\n",
      "iter 3100: loss 1.4145, time 34.77ms, mfu 0.02%\n",
      "iter 3200: loss 1.3212, time 38.39ms, mfu 0.02%\n",
      "step 3250: train loss 2.9439, val loss 3.0142\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 3300: loss 1.3530, time 51.80ms, mfu 0.02%\n",
      "iter 3400: loss 1.3617, time 44.21ms, mfu 0.02%\n",
      "step 3500: train loss 2.9498, val loss 3.0005\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 3500: loss 1.4267, time 1016.89ms, mfu 0.02%\n",
      "iter 3600: loss 1.2941, time 64.34ms, mfu 0.02%\n",
      "iter 3700: loss 1.3000, time 44.74ms, mfu 0.02%\n",
      "step 3750: train loss 2.9419, val loss 2.9966\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 3800: loss 1.2725, time 47.22ms, mfu 0.02%\n",
      "iter 3900: loss 1.1899, time 52.07ms, mfu 0.02%\n",
      "step 4000: train loss 2.9349, val loss 3.0029\n",
      "iter 4000: loss 1.2706, time 801.16ms, mfu 0.02%\n",
      "iter 4100: loss 1.3230, time 73.95ms, mfu 0.02%\n",
      "iter 4200: loss 1.2101, time 68.84ms, mfu 0.02%\n",
      "step 4250: train loss 2.9125, val loss 2.9838\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 4300: loss 1.2365, time 438.53ms, mfu 0.02%\n",
      "iter 4400: loss 1.2796, time 44.93ms, mfu 0.02%\n",
      "step 4500: train loss 2.9178, val loss 2.9814\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 4500: loss 1.2393, time 1390.75ms, mfu 0.02%\n",
      "iter 4600: loss 1.3067, time 68.98ms, mfu 0.02%\n",
      "iter 4700: loss 1.2133, time 44.85ms, mfu 0.02%\n",
      "step 4750: train loss 2.9189, val loss 2.9740\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 4800: loss 1.2236, time 56.40ms, mfu 0.02%\n",
      "iter 4900: loss 1.2734, time 66.62ms, mfu 0.02%\n",
      "step 5000: train loss 2.9088, val loss 2.9744\n",
      "iter 5000: loss 1.1766, time 979.41ms, mfu 0.02%\n",
      "iter 5100: loss 1.1612, time 86.40ms, mfu 0.02%\n",
      "iter 5200: loss 1.1709, time 54.21ms, mfu 0.02%\n",
      "step 5250: train loss 2.8976, val loss 2.9776\n",
      "iter 5300: loss 1.2199, time 403.22ms, mfu 0.02%\n",
      "iter 5400: loss 1.1665, time 51.98ms, mfu 0.02%\n",
      "step 5500: train loss 2.9174, val loss 2.9701\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 5500: loss 1.1567, time 2352.54ms, mfu 0.02%\n",
      "iter 5600: loss 1.1666, time 78.25ms, mfu 0.02%\n",
      "iter 5700: loss 1.1631, time 53.05ms, mfu 0.02%\n",
      "step 5750: train loss 2.8965, val loss 2.9637\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 5800: loss 1.2580, time 51.57ms, mfu 0.02%\n",
      "iter 5900: loss 1.2211, time 100.49ms, mfu 0.02%\n",
      "step 6000: train loss 2.9136, val loss 2.9582\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 6000: loss 1.1877, time 1374.27ms, mfu 0.02%\n",
      "iter 6100: loss 1.2592, time 52.61ms, mfu 0.02%\n",
      "iter 6200: loss 1.0850, time 99.50ms, mfu 0.02%\n",
      "step 6250: train loss 2.9011, val loss 2.9618\n",
      "iter 6300: loss 1.1419, time 54.18ms, mfu 0.02%\n",
      "iter 6400: loss 1.2168, time 121.35ms, mfu 0.02%\n",
      "step 6500: train loss 2.8746, val loss 2.9463\n",
      "saving checkpoint to out/proposed_margin_0.5\n",
      "iter 6500: loss 1.1207, time 2245.76ms, mfu 0.01%\n",
      "iter 6600: loss 1.1042, time 75.48ms, mfu 0.01%\n",
      "iter 6700: loss 1.1189, time 107.99ms, mfu 0.01%\n",
      "step 6750: train loss 2.8905, val loss 2.9531\n",
      "iter 6800: loss 1.1900, time 118.37ms, mfu 0.01%\n",
      "iter 6900: loss 1.2542, time 111.07ms, mfu 0.01%\n",
      "step 7000: train loss 2.8856, val loss 2.9580\n",
      "iter 7000: loss 1.1858, time 2281.40ms, mfu 0.01%\n",
      "iter 7100: loss 1.2359, time 302.82ms, mfu 0.01%\n",
      "iter 7200: loss 1.1536, time 65.63ms, mfu 0.01%\n",
      "step 7250: train loss 2.8872, val loss 2.9549\n",
      "iter 7300: loss 1.1197, time 84.76ms, mfu 0.01%\n",
      "iter 7400: loss 1.0954, time 601.05ms, mfu 0.01%\n",
      "step 7500: train loss 2.8828, val loss 2.9514\n",
      "iter 7500: loss 1.1835, time 1179.99ms, mfu 0.01%\n",
      "iter 7600: loss 1.1442, time 194.79ms, mfu 0.01%\n",
      "iter 7700: loss 1.2126, time 102.35ms, mfu 0.01%\n",
      "step 7750: train loss 2.8871, val loss 2.9666\n",
      "iter 7800: loss 1.1097, time 120.21ms, mfu 0.01%\n",
      "iter 7900: loss 1.1380, time 154.62ms, mfu 0.01%\n",
      "step 8000: train loss 2.8858, val loss 2.9485\n",
      "iter 8000: loss 1.0771, time 1695.12ms, mfu 0.01%\n",
      "done for margin=0.5\n"
     ]
    }
   ],
   "source": [
    "margin = 0.5\n",
    "args = {\n",
    "    \"out_dir\": f\"out/proposed_margin_{margin}\",\n",
    "    \"model_type\": \"proposed\",\n",
    "    \"margin\": margin\n",
    "}\n",
    "run_training(args)\n",
    "print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "478bb899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/cwt\n",
      "Overriding: model_type = cwt\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 6.7290, val loss 6.7216\n",
      "iter 0: loss 6.7299, time 832.24ms, mfu -100.00%\n",
      "iter 100: loss 5.8031, time 44.54ms, mfu 0.03%\n",
      "iter 200: loss 5.6847, time 33.57ms, mfu 0.03%\n",
      "step 250: train loss 5.6660, val loss 5.6388\n",
      "saving checkpoint to out/cwt\n",
      "iter 300: loss 5.4639, time 54.20ms, mfu 0.03%\n",
      "iter 400: loss 5.4288, time 73.41ms, mfu 0.03%\n",
      "step 500: train loss 5.5441, val loss 5.5116\n",
      "saving checkpoint to out/cwt\n",
      "iter 500: loss 5.5349, time 1193.77ms, mfu 0.03%\n",
      "iter 600: loss 5.4766, time 90.04ms, mfu 0.02%\n",
      "iter 700: loss 5.5226, time 140.22ms, mfu 0.02%\n",
      "step 750: train loss 5.3572, val loss 5.4054\n",
      "saving checkpoint to out/cwt\n",
      "iter 800: loss 5.1547, time 110.18ms, mfu 0.02%\n",
      "iter 900: loss 5.2756, time 86.71ms, mfu 0.02%\n",
      "step 1000: train loss 5.3113, val loss 5.3220\n",
      "saving checkpoint to out/cwt\n",
      "iter 1000: loss 5.0579, time 2064.67ms, mfu 0.02%\n",
      "iter 1100: loss 5.3139, time 72.87ms, mfu 0.02%\n",
      "iter 1200: loss 5.2113, time 107.28ms, mfu 0.02%\n",
      "step 1250: train loss 5.2140, val loss 5.3128\n",
      "saving checkpoint to out/cwt\n",
      "iter 1300: loss 5.1784, time 95.15ms, mfu 0.02%\n",
      "iter 1400: loss 5.2369, time 165.14ms, mfu 0.02%\n",
      "step 1500: train loss 5.1608, val loss 5.1868\n",
      "saving checkpoint to out/cwt\n",
      "iter 1500: loss 5.1176, time 1432.27ms, mfu 0.02%\n",
      "iter 1600: loss 4.9103, time 98.14ms, mfu 0.01%\n",
      "iter 1700: loss 5.1580, time 145.34ms, mfu 0.01%\n",
      "step 1750: train loss 5.1146, val loss 5.1835\n",
      "saving checkpoint to out/cwt\n",
      "iter 1800: loss 5.1056, time 77.76ms, mfu 0.01%\n",
      "iter 1900: loss 5.1511, time 90.67ms, mfu 0.01%\n",
      "step 2000: train loss 5.0384, val loss 5.1822\n",
      "saving checkpoint to out/cwt\n",
      "iter 2000: loss 5.0001, time 1462.88ms, mfu 0.01%\n",
      "iter 2100: loss 5.1743, time 95.56ms, mfu 0.01%\n",
      "iter 2200: loss 4.9213, time 114.05ms, mfu 0.01%\n",
      "step 2250: train loss 5.0649, val loss 5.1255\n",
      "saving checkpoint to out/cwt\n",
      "iter 2300: loss 4.9269, time 102.82ms, mfu 0.01%\n",
      "iter 2400: loss 5.0597, time 79.43ms, mfu 0.01%\n",
      "step 2500: train loss 4.9565, val loss 5.1128\n",
      "saving checkpoint to out/cwt\n",
      "iter 2500: loss 5.0116, time 1667.89ms, mfu 0.01%\n",
      "iter 2600: loss 5.0338, time 233.81ms, mfu 0.01%\n",
      "iter 2700: loss 4.8827, time 85.93ms, mfu 0.01%\n",
      "step 2750: train loss 4.9350, val loss 5.0553\n",
      "saving checkpoint to out/cwt\n",
      "iter 2800: loss 5.1743, time 153.25ms, mfu 0.01%\n",
      "iter 2900: loss 4.9666, time 98.20ms, mfu 0.01%\n",
      "step 3000: train loss 4.9389, val loss 5.0329\n",
      "saving checkpoint to out/cwt\n",
      "iter 3000: loss 4.8616, time 1223.09ms, mfu 0.01%\n",
      "iter 3100: loss 4.9318, time 112.23ms, mfu 0.01%\n",
      "iter 3200: loss 5.1145, time 111.94ms, mfu 0.01%\n",
      "step 3250: train loss 4.9330, val loss 5.0687\n",
      "iter 3300: loss 4.8762, time 207.20ms, mfu 0.01%\n",
      "iter 3400: loss 5.1401, time 217.77ms, mfu 0.01%\n",
      "step 3500: train loss 4.9255, val loss 5.0559\n",
      "iter 3500: loss 4.9481, time 1150.09ms, mfu 0.01%\n",
      "iter 3600: loss 5.0972, time 103.74ms, mfu 0.01%\n",
      "iter 3700: loss 5.0465, time 137.24ms, mfu 0.01%\n",
      "step 3750: train loss 4.8625, val loss 5.0643\n",
      "iter 3800: loss 4.9737, time 295.01ms, mfu 0.01%\n",
      "iter 3900: loss 4.7052, time 152.56ms, mfu 0.01%\n",
      "step 4000: train loss 4.9042, val loss 5.0524\n",
      "iter 4000: loss 4.9798, time 1191.47ms, mfu 0.01%\n",
      "iter 4100: loss 5.0144, time 140.10ms, mfu 0.01%\n",
      "iter 4200: loss 4.8675, time 102.92ms, mfu 0.01%\n",
      "step 4250: train loss 4.8755, val loss 4.9591\n",
      "saving checkpoint to out/cwt\n",
      "iter 4300: loss 4.6578, time 107.35ms, mfu 0.01%\n",
      "iter 4400: loss 5.1348, time 176.69ms, mfu 0.01%\n",
      "step 4500: train loss 4.8139, val loss 5.0121\n",
      "iter 4500: loss 4.9433, time 2257.80ms, mfu 0.01%\n",
      "iter 4600: loss 5.0021, time 103.12ms, mfu 0.01%\n",
      "iter 4700: loss 4.5350, time 59.97ms, mfu 0.01%\n",
      "step 4750: train loss 4.8645, val loss 4.9902\n",
      "iter 4800: loss 4.8773, time 178.91ms, mfu 0.01%\n",
      "iter 4900: loss 4.8929, time 109.18ms, mfu 0.01%\n",
      "step 5000: train loss 4.8443, val loss 4.9701\n",
      "iter 5000: loss 4.7248, time 1402.25ms, mfu 0.01%\n",
      "iter 5100: loss 4.8339, time 65.19ms, mfu 0.01%\n",
      "iter 5200: loss 4.8559, time 67.20ms, mfu 0.01%\n",
      "step 5250: train loss 4.7607, val loss 4.9353\n",
      "saving checkpoint to out/cwt\n",
      "iter 5300: loss 4.9663, time 114.28ms, mfu 0.01%\n",
      "iter 5400: loss 4.8886, time 121.24ms, mfu 0.01%\n",
      "step 5500: train loss 4.8160, val loss 4.9129\n",
      "saving checkpoint to out/cwt\n",
      "iter 5500: loss 4.8057, time 1139.23ms, mfu 0.01%\n",
      "iter 5600: loss 4.4681, time 70.66ms, mfu 0.01%\n",
      "iter 5700: loss 4.7890, time 77.05ms, mfu 0.01%\n",
      "step 5750: train loss 4.7675, val loss 4.8850\n",
      "saving checkpoint to out/cwt\n",
      "iter 5800: loss 4.9193, time 201.73ms, mfu 0.01%\n",
      "iter 5900: loss 4.9337, time 54.97ms, mfu 0.01%\n",
      "step 6000: train loss 4.7662, val loss 4.8939\n",
      "iter 6000: loss 4.6258, time 1063.85ms, mfu 0.01%\n",
      "iter 6100: loss 4.9318, time 345.65ms, mfu 0.01%\n",
      "iter 6200: loss 4.6725, time 52.59ms, mfu 0.01%\n",
      "step 6250: train loss 4.7819, val loss 4.8888\n",
      "iter 6300: loss 4.7181, time 87.74ms, mfu 0.01%\n",
      "iter 6400: loss 4.8775, time 53.23ms, mfu 0.01%\n",
      "step 6500: train loss 4.7622, val loss 4.8797\n",
      "saving checkpoint to out/cwt\n",
      "iter 6500: loss 4.7959, time 1092.74ms, mfu 0.01%\n",
      "iter 6600: loss 4.7078, time 69.59ms, mfu 0.01%\n",
      "iter 6700: loss 4.8009, time 58.79ms, mfu 0.01%\n",
      "step 6750: train loss 4.7561, val loss 4.8937\n",
      "iter 6800: loss 4.8914, time 94.28ms, mfu 0.01%\n",
      "iter 6900: loss 4.9533, time 261.54ms, mfu 0.01%\n",
      "step 7000: train loss 4.7040, val loss 4.8623\n",
      "saving checkpoint to out/cwt\n",
      "iter 7000: loss 4.7401, time 1434.20ms, mfu 0.01%\n",
      "iter 7100: loss 4.8921, time 129.38ms, mfu 0.01%\n",
      "iter 7200: loss 4.4916, time 126.31ms, mfu 0.01%\n",
      "step 7250: train loss 4.7367, val loss 4.9089\n",
      "iter 7300: loss 4.7578, time 111.78ms, mfu 0.01%\n",
      "iter 7400: loss 4.7104, time 58.90ms, mfu 0.01%\n",
      "step 7500: train loss 4.8309, val loss 4.8781\n",
      "iter 7500: loss 4.5232, time 1201.91ms, mfu 0.01%\n",
      "iter 7600: loss 4.6617, time 80.96ms, mfu 0.01%\n",
      "iter 7700: loss 4.5838, time 107.89ms, mfu 0.01%\n",
      "step 7750: train loss 4.7164, val loss 4.8953\n",
      "iter 7800: loss 4.6971, time 104.85ms, mfu 0.01%\n",
      "iter 7900: loss 4.9465, time 107.15ms, mfu 0.01%\n",
      "step 8000: train loss 4.7303, val loss 4.8708\n",
      "iter 8000: loss 4.7525, time 1180.11ms, mfu 0.01%\n",
      "done for cwt\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"out_dir\": f\"out/cwt\",\n",
    "    \"model_type\": \"cwt\",\n",
    "}\n",
    "run_training(args)\n",
    "print(f'done for cwt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42c8c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.0041, time 517.46ms, mfu -100.00%\n",
      "iter 100: loss 1.7956, time 30.44ms, mfu 0.04%\n",
      "iter 200: loss 1.5636, time 56.78ms, mfu 0.04%\n",
      "step 250: train loss 4.6943, val loss 4.6961\n",
      "saving checkpoint to out/proposed_margin_0.0\n",
      "iter 300: loss 1.5715, time 46.74ms, mfu 0.04%\n",
      "iter 400: loss 1.5429, time 108.84ms, mfu 0.04%\n",
      "step 500: train loss 4.7189, val loss 4.7199\n",
      "iter 500: loss 1.9487, time 531.53ms, mfu 0.03%\n",
      "iter 600: loss 1.8921, time 57.05ms, mfu 0.03%\n",
      "iter 700: loss 1.4734, time 32.79ms, mfu 0.03%\n",
      "step 750: train loss 4.7523, val loss 4.7533\n",
      "iter 800: loss 1.5563, time 73.65ms, mfu 0.03%\n",
      "iter 900: loss 1.4065, time 36.01ms, mfu 0.03%\n",
      "step 1000: train loss 4.7812, val loss 4.7809\n",
      "iter 1000: loss 1.5165, time 786.26ms, mfu 0.03%\n",
      "iter 1100: loss 1.3245, time 82.94ms, mfu 0.03%\n",
      "iter 1200: loss 1.3537, time 79.86ms, mfu 0.03%\n",
      "step 1250: train loss 4.7923, val loss 4.7926\n",
      "iter 1300: loss 1.3334, time 80.22ms, mfu 0.03%\n",
      "iter 1400: loss 1.6759, time 80.14ms, mfu 0.02%\n",
      "step 1500: train loss 4.7976, val loss 4.7974\n",
      "iter 1500: loss 1.5061, time 738.63ms, mfu 0.02%\n",
      "iter 1600: loss 1.3879, time 45.83ms, mfu 0.02%\n",
      "iter 1700: loss 1.4807, time 50.39ms, mfu 0.02%\n",
      "step 1750: train loss 4.7985, val loss 4.7986\n",
      "iter 1800: loss 1.2522, time 42.75ms, mfu 0.02%\n",
      "iter 1900: loss 1.4038, time 112.81ms, mfu 0.02%\n",
      "step 2000: train loss 4.7922, val loss 4.7929\n",
      "iter 2000: loss 1.3047, time 563.98ms, mfu 0.02%\n",
      "iter 2100: loss 1.3003, time 39.20ms, mfu 0.02%\n",
      "iter 2200: loss 1.3667, time 87.08ms, mfu 0.02%\n",
      "step 2250: train loss 4.7992, val loss 4.7991\n",
      "iter 2300: loss 1.2625, time 90.90ms, mfu 0.02%\n",
      "iter 2400: loss 1.3230, time 72.56ms, mfu 0.02%\n",
      "step 2500: train loss 4.8074, val loss 4.8085\n",
      "iter 2500: loss 1.3380, time 846.16ms, mfu 0.02%\n",
      "iter 2600: loss 1.2765, time 41.56ms, mfu 0.02%\n",
      "iter 2700: loss 1.3387, time 92.91ms, mfu 0.02%\n",
      "step 2750: train loss 4.8060, val loss 4.8059\n",
      "iter 2800: loss 1.3864, time 95.11ms, mfu 0.02%\n",
      "iter 2900: loss 1.2140, time 55.48ms, mfu 0.02%\n",
      "step 3000: train loss 4.8145, val loss 4.8146\n",
      "iter 3000: loss 1.4121, time 993.03ms, mfu 0.02%\n",
      "iter 3100: loss 1.3729, time 41.61ms, mfu 0.02%\n",
      "iter 3200: loss 1.3198, time 42.06ms, mfu 0.02%\n",
      "step 3250: train loss 4.8152, val loss 4.8149\n",
      "iter 3300: loss 1.3824, time 70.28ms, mfu 0.02%\n",
      "iter 3400: loss 1.4512, time 51.68ms, mfu 0.02%\n",
      "step 3500: train loss 4.8136, val loss 4.8139\n",
      "iter 3500: loss 1.3548, time 910.17ms, mfu 0.02%\n",
      "iter 3600: loss 1.3513, time 41.62ms, mfu 0.02%\n",
      "iter 3700: loss 1.2109, time 60.88ms, mfu 0.02%\n",
      "step 3750: train loss 4.8159, val loss 4.8167\n",
      "iter 3800: loss 1.1746, time 43.37ms, mfu 0.02%\n",
      "iter 3900: loss 1.1586, time 42.08ms, mfu 0.02%\n",
      "step 4000: train loss 4.8282, val loss 4.8283\n",
      "iter 4000: loss 1.2874, time 943.09ms, mfu 0.02%\n",
      "iter 4100: loss 1.2126, time 89.68ms, mfu 0.02%\n",
      "iter 4200: loss 1.3877, time 41.62ms, mfu 0.02%\n",
      "step 4250: train loss 4.8511, val loss 4.8507\n",
      "iter 4300: loss 1.5512, time 74.96ms, mfu 0.02%\n",
      "iter 4400: loss 1.5197, time 49.76ms, mfu 0.02%\n",
      "step 4500: train loss 4.8463, val loss 4.8465\n",
      "iter 4500: loss 1.5595, time 1011.20ms, mfu 0.02%\n",
      "iter 4600: loss 1.4925, time 98.73ms, mfu 0.02%\n",
      "iter 4700: loss 1.3498, time 99.66ms, mfu 0.02%\n",
      "step 4750: train loss 4.8461, val loss 4.8463\n",
      "iter 4800: loss 1.3397, time 47.88ms, mfu 0.02%\n",
      "iter 4900: loss 1.3733, time 84.46ms, mfu 0.02%\n",
      "step 5000: train loss 4.8468, val loss 4.8469\n",
      "iter 5000: loss 1.2767, time 883.33ms, mfu 0.02%\n",
      "iter 5100: loss 1.1901, time 236.40ms, mfu 0.02%\n",
      "iter 5200: loss 1.2542, time 42.62ms, mfu 0.02%\n",
      "step 5250: train loss 4.8451, val loss 4.8452\n",
      "iter 5300: loss 1.2688, time 79.71ms, mfu 0.02%\n",
      "iter 5400: loss 1.2455, time 96.00ms, mfu 0.02%\n",
      "step 5500: train loss 4.8508, val loss 4.8508\n",
      "iter 5500: loss 1.2788, time 907.82ms, mfu 0.01%\n",
      "iter 5600: loss 1.1987, time 86.28ms, mfu 0.01%\n",
      "iter 5700: loss 1.1676, time 69.97ms, mfu 0.02%\n",
      "step 5750: train loss 4.8512, val loss 4.8512\n",
      "iter 5800: loss 1.3908, time 92.47ms, mfu 0.02%\n",
      "iter 5900: loss 1.0955, time 120.48ms, mfu 0.01%\n",
      "step 6000: train loss 4.8523, val loss 4.8526\n",
      "iter 6000: loss 1.0992, time 1132.11ms, mfu 0.01%\n",
      "iter 6100: loss 1.2522, time 82.84ms, mfu 0.01%\n",
      "iter 6200: loss 1.1048, time 48.26ms, mfu 0.01%\n",
      "step 6250: train loss 4.8544, val loss 4.8545\n",
      "iter 6300: loss 1.0820, time 56.49ms, mfu 0.02%\n",
      "iter 6400: loss 1.1770, time 100.29ms, mfu 0.02%\n",
      "step 6500: train loss 4.8542, val loss 4.8543\n",
      "iter 6500: loss 1.1828, time 621.72ms, mfu 0.01%\n",
      "iter 6600: loss 1.1210, time 85.86ms, mfu 0.01%\n",
      "iter 6700: loss 1.1339, time 91.53ms, mfu 0.01%\n",
      "step 6750: train loss 4.8559, val loss 4.8559\n",
      "iter 6800: loss 1.2014, time 43.81ms, mfu 0.02%\n",
      "iter 6900: loss 1.1723, time 45.17ms, mfu 0.02%\n",
      "step 7000: train loss 4.8567, val loss 4.8567\n",
      "iter 7000: loss 1.1167, time 804.49ms, mfu 0.02%\n",
      "iter 7100: loss 1.2266, time 115.30ms, mfu 0.01%\n",
      "iter 7200: loss 1.2698, time 47.74ms, mfu 0.02%\n",
      "step 7250: train loss 4.8576, val loss 4.8577\n",
      "iter 7300: loss 0.9865, time 45.48ms, mfu 0.02%\n",
      "iter 7400: loss 1.0884, time 42.44ms, mfu 0.02%\n",
      "step 7500: train loss 4.8588, val loss 4.8587\n",
      "iter 7500: loss 1.1250, time 945.47ms, mfu 0.02%\n",
      "iter 7600: loss 1.0243, time 103.04ms, mfu 0.02%\n",
      "iter 7700: loss 1.1523, time 84.41ms, mfu 0.02%\n",
      "step 7750: train loss 4.8591, val loss 4.8591\n",
      "iter 7800: loss 1.0807, time 99.11ms, mfu 0.02%\n",
      "iter 7900: loss 1.2228, time 79.54ms, mfu 0.02%\n",
      "step 8000: train loss 4.8590, val loss 4.8592\n",
      "iter 8000: loss 1.0593, time 1136.11ms, mfu 0.01%\n",
      "done for margin=0.0\n"
     ]
    }
   ],
   "source": [
    "margin = 0.0\n",
    "args = {\n",
    "    \"out_dir\": f\"out/proposed_margin_{margin}\",\n",
    "    \"model_type\": \"proposed\",\n",
    "    \"margin\": margin\n",
    "}\n",
    "run_training(args)\n",
    "print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "093e5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/baseline_logged\n",
      "Overriding: log_output_loss = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8461, val loss 4.8466\n",
      "iter 0: loss 4.8414, time 541.10ms, mfu -100.00%\n",
      "iter 100: loss 2.6718, time 39.18ms, mfu 0.03%\n",
      "iter 200: loss 2.5101, time 42.73ms, mfu 0.03%\n",
      "step 250: train loss 2.4667, val loss 2.4733\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 300: loss 2.4614, time 52.63ms, mfu 0.03%\n",
      "iter 400: loss 2.3067, time 89.88ms, mfu 0.03%\n",
      "step 500: train loss 2.2724, val loss 2.3051\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 500: loss 2.3376, time 939.88ms, mfu 0.03%\n",
      "iter 600: loss 2.1825, time 119.44ms, mfu 0.03%\n",
      "iter 700: loss 2.0942, time 138.72ms, mfu 0.02%\n",
      "step 750: train loss 2.1556, val loss 2.1864\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 800: loss 2.0839, time 97.82ms, mfu 0.02%\n",
      "iter 900: loss 2.0045, time 93.14ms, mfu 0.02%\n",
      "step 1000: train loss 2.0006, val loss 2.1153\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 1000: loss 1.9256, time 1228.39ms, mfu 0.02%\n",
      "iter 1100: loss 2.0820, time 93.18ms, mfu 0.02%\n",
      "iter 1200: loss 1.9831, time 400.85ms, mfu 0.02%\n",
      "step 1250: train loss 1.9693, val loss 2.0460\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 1300: loss 2.1127, time 98.35ms, mfu 0.02%\n",
      "iter 1400: loss 1.9555, time 98.56ms, mfu 0.02%\n",
      "step 1500: train loss 1.9207, val loss 2.0118\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 1500: loss 1.9484, time 1576.48ms, mfu 0.02%\n",
      "iter 1600: loss 1.9708, time 64.44ms, mfu 0.02%\n",
      "iter 1700: loss 1.7493, time 96.63ms, mfu 0.02%\n",
      "step 1750: train loss 1.8119, val loss 1.9539\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 1800: loss 2.0617, time 140.42ms, mfu 0.02%\n",
      "iter 1900: loss 1.7860, time 114.88ms, mfu 0.01%\n",
      "step 2000: train loss 1.8013, val loss 1.9610\n",
      "iter 2000: loss 1.7720, time 1051.07ms, mfu 0.01%\n",
      "iter 2100: loss 1.7850, time 63.43ms, mfu 0.01%\n",
      "iter 2200: loss 2.0514, time 84.13ms, mfu 0.01%\n",
      "step 2250: train loss 1.7811, val loss 1.8794\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 2300: loss 1.8855, time 67.61ms, mfu 0.01%\n",
      "iter 2400: loss 1.8130, time 66.54ms, mfu 0.02%\n",
      "step 2500: train loss 1.7377, val loss 1.9056\n",
      "iter 2500: loss 1.8054, time 1141.21ms, mfu 0.01%\n",
      "iter 2600: loss 1.6777, time 125.44ms, mfu 0.01%\n",
      "iter 2700: loss 1.6971, time 148.10ms, mfu 0.01%\n",
      "step 2750: train loss 1.7427, val loss 1.8098\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 2800: loss 1.6897, time 91.27ms, mfu 0.01%\n",
      "iter 2900: loss 1.6856, time 75.06ms, mfu 0.01%\n",
      "step 3000: train loss 1.6560, val loss 1.8631\n",
      "iter 3000: loss 1.7407, time 953.05ms, mfu 0.01%\n",
      "iter 3100: loss 1.6734, time 97.67ms, mfu 0.01%\n",
      "iter 3200: loss 1.5546, time 80.57ms, mfu 0.01%\n",
      "step 3250: train loss 1.6685, val loss 1.8429\n",
      "iter 3300: loss 1.6091, time 71.02ms, mfu 0.01%\n",
      "iter 3400: loss 1.6421, time 72.24ms, mfu 0.01%\n",
      "step 3500: train loss 1.6459, val loss 1.8317\n",
      "iter 3500: loss 1.5805, time 1148.19ms, mfu 0.01%\n",
      "iter 3600: loss 1.6782, time 154.55ms, mfu 0.01%\n",
      "iter 3700: loss 1.7268, time 60.36ms, mfu 0.01%\n",
      "step 3750: train loss 1.6155, val loss 1.7866\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 3800: loss 1.6138, time 144.86ms, mfu 0.01%\n",
      "iter 3900: loss 1.7138, time 119.06ms, mfu 0.01%\n",
      "step 4000: train loss 1.5786, val loss 1.7571\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 4000: loss 1.5271, time 1184.99ms, mfu 0.01%\n",
      "iter 4100: loss 1.5475, time 85.33ms, mfu 0.01%\n",
      "iter 4200: loss 1.7016, time 77.38ms, mfu 0.01%\n",
      "step 4250: train loss 1.6076, val loss 1.7516\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 4300: loss 1.4652, time 128.46ms, mfu 0.01%\n",
      "iter 4400: loss 1.5757, time 65.46ms, mfu 0.01%\n",
      "step 4500: train loss 1.5379, val loss 1.7550\n",
      "iter 4500: loss 1.4372, time 1382.89ms, mfu 0.01%\n",
      "iter 4600: loss 1.5467, time 84.73ms, mfu 0.01%\n",
      "iter 4700: loss 1.4955, time 63.02ms, mfu 0.01%\n",
      "step 4750: train loss 1.5185, val loss 1.7434\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 4800: loss 1.5056, time 266.06ms, mfu 0.01%\n",
      "iter 4900: loss 1.4049, time 344.16ms, mfu 0.01%\n",
      "step 5000: train loss 1.5213, val loss 1.6770\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 5000: loss 1.6143, time 1561.75ms, mfu 0.01%\n",
      "iter 5100: loss 1.4638, time 139.24ms, mfu 0.01%\n",
      "iter 5200: loss 1.4971, time 104.83ms, mfu 0.01%\n",
      "step 5250: train loss 1.5445, val loss 1.7055\n",
      "iter 5300: loss 1.4399, time 121.12ms, mfu 0.01%\n",
      "iter 5400: loss 1.5182, time 257.76ms, mfu 0.01%\n",
      "step 5500: train loss 1.4972, val loss 1.6415\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 5500: loss 1.4056, time 1179.36ms, mfu 0.01%\n",
      "iter 5600: loss 1.4810, time 100.35ms, mfu 0.01%\n",
      "iter 5700: loss 1.5361, time 59.78ms, mfu 0.01%\n",
      "step 5750: train loss 1.5115, val loss 1.6833\n",
      "iter 5800: loss 1.4630, time 110.23ms, mfu 0.01%\n",
      "iter 5900: loss 1.4643, time 113.50ms, mfu 0.01%\n",
      "step 6000: train loss 1.4672, val loss 1.6512\n",
      "iter 6000: loss 1.5247, time 1687.98ms, mfu 0.01%\n",
      "iter 6100: loss 1.5580, time 107.40ms, mfu 0.01%\n",
      "iter 6200: loss 1.4699, time 124.89ms, mfu 0.01%\n",
      "step 6250: train loss 1.4532, val loss 1.6674\n",
      "iter 6300: loss 1.5087, time 67.38ms, mfu 0.01%\n",
      "iter 6400: loss 1.5992, time 118.65ms, mfu 0.01%\n",
      "step 6500: train loss 1.4283, val loss 1.6382\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 6500: loss 1.5348, time 1252.55ms, mfu 0.01%\n",
      "iter 6600: loss 1.3765, time 93.50ms, mfu 0.01%\n",
      "iter 6700: loss 1.5549, time 207.30ms, mfu 0.01%\n",
      "step 6750: train loss 1.4346, val loss 1.6525\n",
      "iter 6800: loss 1.4348, time 89.12ms, mfu 0.01%\n",
      "iter 6900: loss 1.3915, time 80.09ms, mfu 0.01%\n",
      "step 7000: train loss 1.4521, val loss 1.6360\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 7000: loss 1.4809, time 1409.34ms, mfu 0.01%\n",
      "iter 7100: loss 1.4320, time 118.05ms, mfu 0.01%\n",
      "iter 7200: loss 1.3838, time 93.00ms, mfu 0.01%\n",
      "step 7250: train loss 1.4552, val loss 1.6335\n",
      "saving checkpoint to out/baseline_logged\n",
      "iter 7300: loss 1.3855, time 128.84ms, mfu 0.01%\n",
      "iter 7400: loss 1.5276, time 99.77ms, mfu 0.01%\n",
      "step 7500: train loss 1.4119, val loss 1.6414\n",
      "iter 7500: loss 1.3847, time 5706.31ms, mfu 0.01%\n",
      "iter 7600: loss 1.4730, time 283.67ms, mfu 0.01%\n",
      "iter 7700: loss 1.4023, time 95.49ms, mfu 0.01%\n",
      "step 7750: train loss 1.4088, val loss 1.6396\n",
      "iter 7800: loss 1.4304, time 163.11ms, mfu 0.01%\n",
      "iter 7900: loss 1.4294, time 155.94ms, mfu 0.01%\n",
      "step 8000: train loss 1.3903, val loss 1.6341\n",
      "iter 8000: loss 1.3335, time 1269.87ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "#baseline logged\n",
    "args = {\n",
    "    \"out_dir\": \"out/baseline_logged\",\n",
    "    \"log_output_loss\": \"True\"\n",
    "}\n",
    "run_training(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a4826e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_2.0_logged\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: log_output_loss = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8461, val loss 4.8466\n",
      "iter 0: loss 4.8414, time 1069.66ms, mfu -100.00%\n",
      "iter 100: loss 2.4817, time 61.71ms, mfu 0.02%\n",
      "iter 200: loss 2.3891, time 138.84ms, mfu 0.02%\n",
      "step 250: train loss 2.6335, val loss 2.6386\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 300: loss 2.3083, time 133.23ms, mfu 0.02%\n",
      "iter 400: loss 2.1692, time 66.47ms, mfu 0.02%\n",
      "step 500: train loss 2.4369, val loss 2.4660\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 500: loss 2.2093, time 3207.32ms, mfu 0.02%\n",
      "iter 600: loss 2.0457, time 65.94ms, mfu 0.02%\n",
      "iter 700: loss 1.9501, time 295.28ms, mfu 0.02%\n",
      "step 750: train loss 2.2969, val loss 2.3324\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 800: loss 1.9157, time 79.02ms, mfu 0.02%\n",
      "iter 900: loss 1.8558, time 158.20ms, mfu 0.02%\n",
      "step 1000: train loss 2.1567, val loss 2.2668\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 1000: loss 1.7998, time 5967.33ms, mfu 0.01%\n",
      "iter 1100: loss 1.9274, time 145.51ms, mfu 0.01%\n",
      "iter 1200: loss 1.8439, time 679.29ms, mfu 0.01%\n",
      "step 1250: train loss 2.1227, val loss 2.1807\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 1300: loss 1.9696, time 158.89ms, mfu 0.01%\n",
      "iter 1400: loss 1.7993, time 78.99ms, mfu 0.01%\n",
      "step 1500: train loss 2.0685, val loss 2.1436\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 1500: loss 1.7503, time 3096.94ms, mfu 0.01%\n",
      "iter 1600: loss 1.7712, time 100.09ms, mfu 0.01%\n",
      "iter 1700: loss 1.6079, time 414.59ms, mfu 0.01%\n",
      "step 1750: train loss 1.9774, val loss 2.0877\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 1800: loss 1.8970, time 121.14ms, mfu 0.01%\n",
      "iter 1900: loss 1.6476, time 309.84ms, mfu 0.01%\n",
      "step 2000: train loss 1.9521, val loss 2.0797\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 2000: loss 1.6508, time 1486.71ms, mfu 0.01%\n",
      "iter 2100: loss 1.6273, time 127.44ms, mfu 0.01%\n",
      "iter 2200: loss 1.8760, time 64.68ms, mfu 0.01%\n",
      "step 2250: train loss 1.9378, val loss 2.0203\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 2300: loss 1.7160, time 141.72ms, mfu 0.01%\n",
      "iter 2400: loss 1.6443, time 423.45ms, mfu 0.01%\n",
      "step 2500: train loss 1.8906, val loss 2.0274\n",
      "iter 2500: loss 1.6497, time 1518.85ms, mfu 0.01%\n",
      "iter 2600: loss 1.5312, time 107.98ms, mfu 0.01%\n",
      "iter 2700: loss 1.5551, time 131.37ms, mfu 0.01%\n",
      "step 2750: train loss 1.8905, val loss 1.9460\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 2800: loss 1.5377, time 67.08ms, mfu 0.01%\n",
      "iter 2900: loss 1.5380, time 157.93ms, mfu 0.01%\n",
      "step 3000: train loss 1.8118, val loss 1.9698\n",
      "iter 3000: loss 1.5917, time 1619.22ms, mfu 0.01%\n",
      "iter 3100: loss 1.5327, time 142.11ms, mfu 0.01%\n",
      "iter 3200: loss 1.4407, time 1101.13ms, mfu 0.01%\n",
      "step 3250: train loss 1.8340, val loss 1.9663\n",
      "iter 3300: loss 1.4701, time 85.09ms, mfu 0.01%\n",
      "iter 3400: loss 1.5417, time 408.54ms, mfu 0.01%\n",
      "step 3500: train loss 1.8040, val loss 1.9428\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 3500: loss 1.4595, time 1967.93ms, mfu 0.01%\n",
      "iter 3600: loss 1.5441, time 71.66ms, mfu 0.01%\n",
      "iter 3700: loss 1.5824, time 71.92ms, mfu 0.01%\n",
      "step 3750: train loss 1.7876, val loss 1.9166\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 3800: loss 1.4761, time 166.64ms, mfu 0.01%\n",
      "iter 3900: loss 1.5794, time 377.76ms, mfu 0.01%\n",
      "step 4000: train loss 1.7621, val loss 1.9032\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 4000: loss 1.4159, time 960.65ms, mfu 0.01%\n",
      "iter 4100: loss 1.4295, time 64.36ms, mfu 0.01%\n",
      "iter 4200: loss 1.5714, time 75.89ms, mfu 0.01%\n",
      "step 4250: train loss 1.7793, val loss 1.8762\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 4300: loss 1.3838, time 66.65ms, mfu 0.01%\n",
      "iter 4400: loss 1.4489, time 94.57ms, mfu 0.01%\n",
      "step 4500: train loss 1.7240, val loss 1.8755\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 4500: loss 1.3344, time 1459.83ms, mfu 0.01%\n",
      "iter 4600: loss 1.4623, time 96.57ms, mfu 0.01%\n",
      "iter 4700: loss 1.4164, time 200.13ms, mfu 0.01%\n",
      "step 4750: train loss 1.7082, val loss 1.8578\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 4800: loss 1.4054, time 115.52ms, mfu 0.01%\n",
      "iter 4900: loss 1.3341, time 98.59ms, mfu 0.01%\n",
      "step 5000: train loss 1.7104, val loss 1.8210\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 5000: loss 1.4854, time 1256.59ms, mfu 0.01%\n",
      "iter 5100: loss 1.3668, time 179.43ms, mfu 0.01%\n",
      "iter 5200: loss 1.3722, time 257.47ms, mfu 0.01%\n",
      "step 5250: train loss 1.7298, val loss 1.8568\n",
      "iter 5300: loss 1.3208, time 124.38ms, mfu 0.01%\n",
      "iter 5400: loss 1.4148, time 85.84ms, mfu 0.01%\n",
      "step 5500: train loss 1.6862, val loss 1.7962\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 5500: loss 1.2956, time 1028.13ms, mfu 0.01%\n",
      "iter 5600: loss 1.3863, time 196.81ms, mfu 0.01%\n",
      "iter 5700: loss 1.4266, time 288.27ms, mfu 0.01%\n",
      "step 5750: train loss 1.6988, val loss 1.8324\n",
      "iter 5800: loss 1.3614, time 93.48ms, mfu 0.01%\n",
      "iter 5900: loss 1.3641, time 54.31ms, mfu 0.01%\n",
      "step 6000: train loss 1.6633, val loss 1.8025\n",
      "iter 6000: loss 1.4037, time 1094.59ms, mfu 0.01%\n",
      "iter 6100: loss 1.4540, time 47.60ms, mfu 0.01%\n",
      "iter 6200: loss 1.3775, time 115.09ms, mfu 0.01%\n",
      "step 6250: train loss 1.6464, val loss 1.8114\n",
      "iter 6300: loss 1.3937, time 658.82ms, mfu 0.01%\n",
      "iter 6400: loss 1.4467, time 126.69ms, mfu 0.01%\n",
      "step 6500: train loss 1.6248, val loss 1.7779\n",
      "saving checkpoint to out/proposed_margin_2.0_logged\n",
      "iter 6500: loss 1.4039, time 1635.51ms, mfu 0.01%\n",
      "iter 6600: loss 1.2819, time 74.38ms, mfu 0.01%\n",
      "iter 6700: loss 1.4344, time 48.29ms, mfu 0.01%\n",
      "step 6750: train loss 1.6328, val loss 1.7901\n",
      "iter 6800: loss 1.3291, time 92.71ms, mfu 0.01%\n",
      "iter 6900: loss 1.3009, time 114.31ms, mfu 0.01%\n",
      "step 7000: train loss 1.6465, val loss 1.7849\n",
      "iter 7000: loss 1.3698, time 1382.76ms, mfu 0.01%\n",
      "iter 7100: loss 1.3320, time 89.92ms, mfu 0.01%\n",
      "iter 7200: loss 1.2813, time 160.98ms, mfu 0.01%\n",
      "step 7250: train loss 1.6554, val loss 1.7823\n",
      "iter 7300: loss 1.3196, time 271.80ms, mfu 0.01%\n",
      "iter 7400: loss 1.4145, time 94.28ms, mfu 0.01%\n",
      "step 7500: train loss 1.6127, val loss 1.7785\n",
      "iter 7500: loss 1.2928, time 1128.45ms, mfu 0.01%\n",
      "iter 7600: loss 1.3893, time 121.15ms, mfu 0.01%\n",
      "iter 7700: loss 1.3217, time 150.82ms, mfu 0.01%\n",
      "step 7750: train loss 1.6176, val loss 1.7824\n",
      "iter 7800: loss 1.3370, time 127.69ms, mfu 0.01%\n",
      "iter 7900: loss 1.3174, time 68.03ms, mfu 0.01%\n",
      "step 8000: train loss 1.5963, val loss 1.7798\n",
      "iter 8000: loss 1.2625, time 1443.18ms, mfu 0.01%\n",
      "done for margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [2.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_logged\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"log_output_loss\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "177b230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/baseline_wo_weight_tying\n",
      "Overriding: weight_tying = False\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8700, val loss 4.8689\n",
      "iter 0: loss 4.8684, time 2142.40ms, mfu -100.00%\n",
      "iter 100: loss 2.7888, time 46.78ms, mfu 0.03%\n",
      "iter 200: loss 2.5128, time 27.83ms, mfu 0.03%\n",
      "step 250: train loss 2.4961, val loss 2.5075\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 300: loss 2.4462, time 98.75ms, mfu 0.03%\n",
      "iter 400: loss 2.4346, time 36.17ms, mfu 0.03%\n",
      "step 500: train loss 2.2580, val loss 2.3055\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 500: loss 2.2391, time 577.91ms, mfu 0.03%\n",
      "iter 600: loss 2.1708, time 29.44ms, mfu 0.03%\n",
      "iter 700: loss 2.1296, time 36.76ms, mfu 0.03%\n",
      "step 750: train loss 2.1192, val loss 2.1478\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 800: loss 2.2195, time 31.75ms, mfu 0.03%\n",
      "iter 900: loss 1.9716, time 60.60ms, mfu 0.03%\n",
      "step 1000: train loss 1.9855, val loss 2.0573\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 1000: loss 2.0740, time 613.91ms, mfu 0.03%\n",
      "iter 1100: loss 1.9853, time 70.03ms, mfu 0.03%\n",
      "iter 1200: loss 1.8696, time 55.15ms, mfu 0.03%\n",
      "step 1250: train loss 1.8736, val loss 2.0416\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 1300: loss 1.8789, time 89.38ms, mfu 0.02%\n",
      "iter 1400: loss 1.8166, time 42.82ms, mfu 0.03%\n",
      "step 1500: train loss 1.8533, val loss 1.9653\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 1500: loss 1.7913, time 641.05ms, mfu 0.02%\n",
      "iter 1600: loss 1.8624, time 35.34ms, mfu 0.02%\n",
      "iter 1700: loss 1.7394, time 42.69ms, mfu 0.02%\n",
      "step 1750: train loss 1.7922, val loss 1.9194\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 1800: loss 1.7495, time 109.74ms, mfu 0.02%\n",
      "iter 1900: loss 1.8049, time 119.04ms, mfu 0.02%\n",
      "step 2000: train loss 1.7373, val loss 1.9364\n",
      "iter 2000: loss 1.6247, time 703.00ms, mfu 0.02%\n",
      "iter 2100: loss 1.7094, time 32.83ms, mfu 0.02%\n",
      "iter 2200: loss 1.7623, time 59.24ms, mfu 0.02%\n",
      "step 2250: train loss 1.7490, val loss 1.8873\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 2300: loss 1.7495, time 35.63ms, mfu 0.02%\n",
      "iter 2400: loss 1.7120, time 45.87ms, mfu 0.02%\n",
      "step 2500: train loss 1.7005, val loss 1.8240\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 2500: loss 1.6430, time 684.42ms, mfu 0.02%\n",
      "iter 2600: loss 1.6756, time 55.02ms, mfu 0.02%\n",
      "iter 2700: loss 1.5593, time 47.91ms, mfu 0.02%\n",
      "step 2750: train loss 1.6929, val loss 1.8590\n",
      "iter 2800: loss 1.7450, time 58.65ms, mfu 0.02%\n",
      "iter 2900: loss 1.6314, time 68.39ms, mfu 0.02%\n",
      "step 3000: train loss 1.6598, val loss 1.7969\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 3000: loss 1.7793, time 1773.02ms, mfu 0.02%\n",
      "iter 3100: loss 1.7521, time 61.53ms, mfu 0.02%\n",
      "iter 3200: loss 1.7048, time 36.56ms, mfu 0.02%\n",
      "step 3250: train loss 1.5871, val loss 1.7948\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 3300: loss 1.6806, time 58.46ms, mfu 0.02%\n",
      "iter 3400: loss 1.6837, time 51.53ms, mfu 0.02%\n",
      "step 3500: train loss 1.6131, val loss 1.7934\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 3500: loss 1.7917, time 715.00ms, mfu 0.02%\n",
      "iter 3600: loss 1.6103, time 37.22ms, mfu 0.02%\n",
      "iter 3700: loss 1.6166, time 61.64ms, mfu 0.02%\n",
      "step 3750: train loss 1.5735, val loss 1.7666\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 3800: loss 1.6217, time 37.37ms, mfu 0.02%\n",
      "iter 3900: loss 1.4391, time 38.94ms, mfu 0.02%\n",
      "step 4000: train loss 1.5586, val loss 1.7646\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 4000: loss 1.5820, time 629.89ms, mfu 0.02%\n",
      "iter 4100: loss 1.6722, time 72.96ms, mfu 0.02%\n",
      "iter 4200: loss 1.4481, time 56.28ms, mfu 0.02%\n",
      "step 4250: train loss 1.5165, val loss 1.7080\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 4300: loss 1.5498, time 49.58ms, mfu 0.02%\n",
      "iter 4400: loss 1.5905, time 48.83ms, mfu 0.02%\n",
      "step 4500: train loss 1.5150, val loss 1.7243\n",
      "iter 4500: loss 1.5181, time 733.61ms, mfu 0.02%\n",
      "iter 4600: loss 1.6067, time 50.37ms, mfu 0.02%\n",
      "iter 4700: loss 1.4901, time 133.82ms, mfu 0.02%\n",
      "step 4750: train loss 1.5144, val loss 1.7072\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 4800: loss 1.5094, time 49.39ms, mfu 0.02%\n",
      "iter 4900: loss 1.5674, time 34.69ms, mfu 0.02%\n",
      "step 5000: train loss 1.4839, val loss 1.6803\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 5000: loss 1.4338, time 764.81ms, mfu 0.02%\n",
      "iter 5100: loss 1.4428, time 82.96ms, mfu 0.02%\n",
      "iter 5200: loss 1.4538, time 54.61ms, mfu 0.02%\n",
      "step 5250: train loss 1.4695, val loss 1.6886\n",
      "iter 5300: loss 1.5024, time 49.76ms, mfu 0.02%\n",
      "iter 5400: loss 1.3850, time 36.76ms, mfu 0.02%\n",
      "step 5500: train loss 1.4778, val loss 1.6481\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 5500: loss 1.4141, time 794.64ms, mfu 0.02%\n",
      "iter 5600: loss 1.4428, time 57.37ms, mfu 0.02%\n",
      "iter 5700: loss 1.4360, time 36.75ms, mfu 0.02%\n",
      "step 5750: train loss 1.4548, val loss 1.6568\n",
      "iter 5800: loss 1.5690, time 80.85ms, mfu 0.02%\n",
      "iter 5900: loss 1.5029, time 53.51ms, mfu 0.02%\n",
      "step 6000: train loss 1.4800, val loss 1.6279\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 6000: loss 1.4664, time 668.51ms, mfu 0.02%\n",
      "iter 6100: loss 1.5933, time 41.86ms, mfu 0.02%\n",
      "iter 6200: loss 1.3273, time 67.61ms, mfu 0.02%\n",
      "step 6250: train loss 1.4431, val loss 1.6248\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 6300: loss 1.3647, time 101.31ms, mfu 0.02%\n",
      "iter 6400: loss 1.4993, time 60.58ms, mfu 0.02%\n",
      "step 6500: train loss 1.4165, val loss 1.6097\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 6500: loss 1.3578, time 887.30ms, mfu 0.02%\n",
      "iter 6600: loss 1.3751, time 73.67ms, mfu 0.02%\n",
      "iter 6700: loss 1.3717, time 100.78ms, mfu 0.02%\n",
      "step 6750: train loss 1.4255, val loss 1.6282\n",
      "iter 6800: loss 1.4160, time 64.87ms, mfu 0.02%\n",
      "iter 6900: loss 1.5325, time 90.35ms, mfu 0.02%\n",
      "step 7000: train loss 1.4007, val loss 1.6179\n",
      "iter 7000: loss 1.4435, time 836.07ms, mfu 0.02%\n",
      "iter 7100: loss 1.5363, time 58.80ms, mfu 0.02%\n",
      "iter 7200: loss 1.4473, time 42.06ms, mfu 0.02%\n",
      "step 7250: train loss 1.4172, val loss 1.6228\n",
      "iter 7300: loss 1.3683, time 58.59ms, mfu 0.02%\n",
      "iter 7400: loss 1.3369, time 42.84ms, mfu 0.02%\n",
      "step 7500: train loss 1.4140, val loss 1.5995\n",
      "saving checkpoint to out/baseline_wo_weight_tying\n",
      "iter 7500: loss 1.4082, time 1094.88ms, mfu 0.02%\n",
      "iter 7600: loss 1.4250, time 48.18ms, mfu 0.02%\n",
      "iter 7700: loss 1.4846, time 93.97ms, mfu 0.02%\n",
      "step 7750: train loss 1.4096, val loss 1.6597\n",
      "iter 7800: loss 1.3843, time 47.20ms, mfu 0.02%\n",
      "iter 7900: loss 1.3718, time 74.93ms, mfu 0.02%\n",
      "step 8000: train loss 1.4271, val loss 1.6164\n",
      "iter 8000: loss 1.3429, time 825.43ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "#baseline wo weight_tying\n",
    "args = {\n",
    "    \"out_dir\": \"out/baseline_wo_weight_tying\",\n",
    "    \"weight_tying\": \"False\"\n",
    "}\n",
    "run_training(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173683a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6f6356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_wo_weight_tying\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: weight_tying = False\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8700, val loss 4.8689\n",
      "iter 0: loss 4.8677, time 1718.11ms, mfu -100.00%\n",
      "iter 100: loss 2.4883, time 82.66ms, mfu 0.02%\n",
      "iter 200: loss 2.1870, time 43.21ms, mfu 0.02%\n",
      "step 250: train loss 3.0150, val loss 3.0321\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 300: loss 2.1580, time 45.27ms, mfu 0.02%\n",
      "iter 400: loss 2.1499, time 64.95ms, mfu 0.02%\n",
      "step 500: train loss 2.8401, val loss 2.8834\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 500: loss 2.0053, time 1128.01ms, mfu 0.02%\n",
      "iter 600: loss 1.9244, time 44.52ms, mfu 0.02%\n",
      "iter 700: loss 1.8613, time 56.21ms, mfu 0.02%\n",
      "step 750: train loss 2.6898, val loss 2.7108\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 800: loss 1.8950, time 44.39ms, mfu 0.02%\n",
      "iter 900: loss 1.7262, time 50.10ms, mfu 0.02%\n",
      "step 1000: train loss 2.5938, val loss 2.6338\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 1000: loss 1.7837, time 1140.86ms, mfu 0.02%\n",
      "iter 1100: loss 1.7275, time 44.08ms, mfu 0.02%\n",
      "iter 1200: loss 1.6491, time 99.41ms, mfu 0.02%\n",
      "step 1250: train loss 2.5134, val loss 2.5984\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 1300: loss 1.6430, time 74.27ms, mfu 0.02%\n",
      "iter 1400: loss 1.5842, time 48.18ms, mfu 0.02%\n",
      "step 1500: train loss 2.4920, val loss 2.5492\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 1500: loss 1.5406, time 1247.54ms, mfu 0.02%\n",
      "iter 1600: loss 1.6177, time 107.34ms, mfu 0.02%\n",
      "iter 1700: loss 1.5441, time 46.75ms, mfu 0.02%\n",
      "step 1750: train loss 2.4506, val loss 2.5293\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 1800: loss 1.5365, time 46.92ms, mfu 0.02%\n",
      "iter 1900: loss 1.5446, time 49.27ms, mfu 0.02%\n",
      "step 2000: train loss 2.3852, val loss 2.4912\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 2000: loss 1.4250, time 823.49ms, mfu 0.02%\n",
      "iter 2100: loss 1.5006, time 47.05ms, mfu 0.02%\n",
      "iter 2200: loss 1.5068, time 66.74ms, mfu 0.02%\n",
      "step 2250: train loss 2.4038, val loss 2.4693\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 2300: loss 1.5102, time 77.88ms, mfu 0.02%\n",
      "iter 2400: loss 1.4637, time 116.42ms, mfu 0.02%\n",
      "step 2500: train loss 2.3684, val loss 2.4270\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 2500: loss 1.4583, time 906.73ms, mfu 0.02%\n",
      "iter 2600: loss 1.4463, time 52.86ms, mfu 0.02%\n",
      "iter 2700: loss 1.3852, time 48.45ms, mfu 0.02%\n",
      "step 2750: train loss 2.3584, val loss 2.4411\n",
      "iter 2800: loss 1.4928, time 47.53ms, mfu 0.02%\n",
      "iter 2900: loss 1.4509, time 45.86ms, mfu 0.02%\n",
      "step 3000: train loss 2.3207, val loss 2.3823\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 3000: loss 1.5341, time 1191.94ms, mfu 0.02%\n",
      "iter 3100: loss 1.5238, time 52.93ms, mfu 0.02%\n",
      "iter 3200: loss 1.4725, time 80.02ms, mfu 0.02%\n",
      "step 3250: train loss 2.2691, val loss 2.3731\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 3300: loss 1.4622, time 71.58ms, mfu 0.02%\n",
      "iter 3400: loss 1.4787, time 93.97ms, mfu 0.02%\n",
      "step 3500: train loss 2.2926, val loss 2.3700\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 3500: loss 1.5424, time 2339.10ms, mfu 0.02%\n",
      "iter 3600: loss 1.4473, time 179.19ms, mfu 0.02%\n",
      "iter 3700: loss 1.4062, time 52.69ms, mfu 0.02%\n",
      "step 3750: train loss 2.2671, val loss 2.3515\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 3800: loss 1.3722, time 140.55ms, mfu 0.02%\n",
      "iter 3900: loss 1.2708, time 72.35ms, mfu 0.02%\n",
      "step 4000: train loss 2.2550, val loss 2.3608\n",
      "iter 4000: loss 1.3390, time 820.42ms, mfu 0.01%\n",
      "iter 4100: loss 1.4394, time 102.42ms, mfu 0.01%\n",
      "iter 4200: loss 1.2914, time 137.60ms, mfu 0.01%\n",
      "step 4250: train loss 2.2286, val loss 2.3285\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 4300: loss 1.3508, time 120.06ms, mfu 0.01%\n",
      "iter 4400: loss 1.3625, time 78.99ms, mfu 0.01%\n",
      "step 4500: train loss 2.2238, val loss 2.3171\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 4500: loss 1.3105, time 1107.74ms, mfu 0.01%\n",
      "iter 4600: loss 1.4170, time 57.97ms, mfu 0.01%\n",
      "iter 4700: loss 1.2787, time 66.51ms, mfu 0.01%\n",
      "step 4750: train loss 2.2341, val loss 2.3160\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 4800: loss 1.3206, time 54.13ms, mfu 0.02%\n",
      "iter 4900: loss 1.3576, time 103.62ms, mfu 0.01%\n",
      "step 5000: train loss 2.2058, val loss 2.2993\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 5000: loss 1.2552, time 1464.82ms, mfu 0.01%\n",
      "iter 5100: loss 1.2617, time 61.49ms, mfu 0.01%\n",
      "iter 5200: loss 1.2799, time 76.23ms, mfu 0.01%\n",
      "step 5250: train loss 2.1990, val loss 2.3140\n",
      "iter 5300: loss 1.3320, time 109.92ms, mfu 0.01%\n",
      "iter 5400: loss 1.2328, time 100.28ms, mfu 0.01%\n",
      "step 5500: train loss 2.2095, val loss 2.2889\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 5500: loss 1.2452, time 1060.82ms, mfu 0.01%\n",
      "iter 5600: loss 1.2590, time 99.78ms, mfu 0.01%\n",
      "iter 5700: loss 1.2632, time 111.37ms, mfu 0.01%\n",
      "step 5750: train loss 2.1819, val loss 2.2772\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 5800: loss 1.3473, time 59.78ms, mfu 0.01%\n",
      "iter 5900: loss 1.3101, time 65.42ms, mfu 0.01%\n",
      "step 6000: train loss 2.2125, val loss 2.2729\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 6000: loss 1.2751, time 996.73ms, mfu 0.01%\n",
      "iter 6100: loss 1.3384, time 99.19ms, mfu 0.01%\n",
      "iter 6200: loss 1.1426, time 886.17ms, mfu 0.01%\n",
      "step 6250: train loss 2.1893, val loss 2.2759\n",
      "iter 6300: loss 1.2081, time 63.43ms, mfu 0.01%\n",
      "iter 6400: loss 1.2878, time 69.51ms, mfu 0.01%\n",
      "step 6500: train loss 2.1600, val loss 2.2643\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 6500: loss 1.2092, time 1454.56ms, mfu 0.01%\n",
      "iter 6600: loss 1.1859, time 55.59ms, mfu 0.01%\n",
      "iter 6700: loss 1.1911, time 83.51ms, mfu 0.01%\n",
      "step 6750: train loss 2.1671, val loss 2.2681\n",
      "iter 6800: loss 1.2513, time 87.18ms, mfu 0.01%\n",
      "iter 6900: loss 1.3476, time 56.87ms, mfu 0.01%\n",
      "step 7000: train loss 2.1568, val loss 2.2622\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 7000: loss 1.2478, time 1296.68ms, mfu 0.01%\n",
      "iter 7100: loss 1.3209, time 89.07ms, mfu 0.01%\n",
      "iter 7200: loss 1.2222, time 60.48ms, mfu 0.01%\n",
      "step 7250: train loss 2.1636, val loss 2.2585\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n",
      "iter 7300: loss 1.2099, time 83.46ms, mfu 0.01%\n",
      "iter 7400: loss 1.1767, time 114.55ms, mfu 0.01%\n",
      "step 7500: train loss 2.1560, val loss 2.2497\n",
      "saving checkpoint to out/proposed_margin_1.0_wo_weight_tying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7500: loss 1.2681, time 2155.44ms, mfu 0.01%\n",
      "iter 7600: loss 1.2169, time 44.11ms, mfu 0.01%\n",
      "iter 7700: loss 1.3002, time 187.94ms, mfu 0.01%\n",
      "step 7750: train loss 2.1564, val loss 2.2718\n",
      "iter 7800: loss 1.2005, time 77.79ms, mfu 0.01%\n",
      "iter 7900: loss 1.2238, time 86.51ms, mfu 0.01%\n",
      "step 8000: train loss 2.1613, val loss 2.2572\n",
      "iter 8000: loss 1.1454, time 1014.13ms, mfu 0.01%\n",
      "done for margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_2.0_wo_weight_tying\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: weight_tying = False\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8700, val loss 4.8689\n",
      "iter 0: loss 4.8684, time 1829.08ms, mfu -100.00%\n",
      "iter 100: loss 2.6637, time 112.63ms, mfu 0.01%\n",
      "iter 200: loss 2.3596, time 54.90ms, mfu 0.01%\n",
      "step 250: train loss 2.6479, val loss 2.6640\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 300: loss 2.3113, time 67.59ms, mfu 0.01%\n",
      "iter 400: loss 2.2949, time 69.89ms, mfu 0.01%\n",
      "step 500: train loss 2.4674, val loss 2.5144\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 500: loss 2.1545, time 1240.75ms, mfu 0.01%\n",
      "iter 600: loss 2.0751, time 81.85ms, mfu 0.01%\n",
      "iter 700: loss 2.0362, time 117.57ms, mfu 0.01%\n",
      "step 750: train loss 2.2922, val loss 2.3172\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 800: loss 2.0773, time 78.55ms, mfu 0.01%\n",
      "iter 900: loss 1.8683, time 61.56ms, mfu 0.01%\n",
      "step 1000: train loss 2.1724, val loss 2.2261\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 1000: loss 1.9196, time 1302.87ms, mfu 0.01%\n",
      "iter 1100: loss 1.8827, time 53.14ms, mfu 0.01%\n",
      "iter 1200: loss 1.8003, time 48.76ms, mfu 0.02%\n",
      "step 1250: train loss 2.0845, val loss 2.1980\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 1300: loss 1.7772, time 89.85ms, mfu 0.02%\n",
      "iter 1400: loss 1.7262, time 57.01ms, mfu 0.02%\n",
      "step 1500: train loss 2.0457, val loss 2.1222\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 1500: loss 1.6920, time 1077.02ms, mfu 0.01%\n",
      "iter 1600: loss 1.7540, time 54.99ms, mfu 0.02%\n",
      "iter 1700: loss 1.6425, time 172.53ms, mfu 0.01%\n",
      "step 1750: train loss 1.9849, val loss 2.1013\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 1800: loss 1.6478, time 96.02ms, mfu 0.01%\n",
      "iter 1900: loss 1.6837, time 66.22ms, mfu 0.02%\n",
      "step 2000: train loss 1.9365, val loss 2.0892\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 2000: loss 1.5240, time 892.24ms, mfu 0.01%\n",
      "iter 2100: loss 1.6113, time 55.52ms, mfu 0.01%\n",
      "iter 2200: loss 1.6577, time 59.16ms, mfu 0.02%\n",
      "step 2250: train loss 1.9500, val loss 2.0474\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 2300: loss 1.6259, time 58.43ms, mfu 0.02%\n",
      "iter 2400: loss 1.5832, time 59.91ms, mfu 0.02%\n",
      "step 2500: train loss 1.9010, val loss 1.9867\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 2500: loss 1.5755, time 1056.20ms, mfu 0.02%\n",
      "iter 2600: loss 1.5421, time 54.72ms, mfu 0.02%\n",
      "iter 2700: loss 1.5074, time 83.88ms, mfu 0.02%\n",
      "step 2750: train loss 1.8903, val loss 2.0067\n",
      "iter 2800: loss 1.6218, time 58.90ms, mfu 0.02%\n",
      "iter 2900: loss 1.5547, time 60.37ms, mfu 0.02%\n",
      "step 3000: train loss 1.8510, val loss 1.9444\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 3000: loss 1.6587, time 1529.54ms, mfu 0.02%\n",
      "iter 3100: loss 1.6461, time 88.67ms, mfu 0.02%\n",
      "iter 3200: loss 1.5985, time 111.19ms, mfu 0.02%\n",
      "step 3250: train loss 1.8001, val loss 1.9473\n",
      "iter 3300: loss 1.5508, time 60.66ms, mfu 0.02%\n",
      "iter 3400: loss 1.6090, time 55.41ms, mfu 0.02%\n",
      "step 3500: train loss 1.8125, val loss 1.9234\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 3500: loss 1.6546, time 1665.66ms, mfu 0.01%\n",
      "iter 3600: loss 1.5591, time 61.27ms, mfu 0.02%\n",
      "iter 3700: loss 1.5151, time 103.77ms, mfu 0.02%\n",
      "step 3750: train loss 1.7814, val loss 1.9061\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 3800: loss 1.5006, time 71.32ms, mfu 0.02%\n",
      "iter 3900: loss 1.3674, time 105.25ms, mfu 0.02%\n",
      "step 4000: train loss 1.7726, val loss 1.9154\n",
      "iter 4000: loss 1.4591, time 1111.82ms, mfu 0.01%\n",
      "iter 4100: loss 1.5400, time 54.30ms, mfu 0.01%\n",
      "iter 4200: loss 1.4051, time 90.53ms, mfu 0.01%\n",
      "step 4250: train loss 1.7300, val loss 1.8671\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 4300: loss 1.4653, time 80.53ms, mfu 0.01%\n",
      "iter 4400: loss 1.5036, time 78.21ms, mfu 0.02%\n",
      "step 4500: train loss 1.7213, val loss 1.8665\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 4500: loss 1.4066, time 1290.31ms, mfu 0.01%\n",
      "iter 4600: loss 1.5323, time 116.18ms, mfu 0.01%\n",
      "iter 4700: loss 1.4005, time 83.20ms, mfu 0.01%\n",
      "step 4750: train loss 1.7371, val loss 1.8574\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 4800: loss 1.4210, time 80.60ms, mfu 0.01%\n",
      "iter 4900: loss 1.4727, time 104.20ms, mfu 0.01%\n",
      "step 5000: train loss 1.6962, val loss 1.8355\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 5000: loss 1.3396, time 1406.58ms, mfu 0.01%\n",
      "iter 5100: loss 1.3640, time 79.67ms, mfu 0.01%\n",
      "iter 5200: loss 1.3871, time 86.24ms, mfu 0.01%\n",
      "step 5250: train loss 1.6815, val loss 1.8432\n",
      "iter 5300: loss 1.4314, time 113.41ms, mfu 0.01%\n",
      "iter 5400: loss 1.3226, time 65.57ms, mfu 0.01%\n",
      "step 5500: train loss 1.6889, val loss 1.8008\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 5500: loss 1.3184, time 1089.47ms, mfu 0.01%\n",
      "iter 5600: loss 1.3583, time 78.34ms, mfu 0.01%\n",
      "iter 5700: loss 1.3430, time 93.28ms, mfu 0.01%\n",
      "step 5750: train loss 1.6661, val loss 1.8106\n",
      "iter 5800: loss 1.4653, time 108.72ms, mfu 0.01%\n",
      "iter 5900: loss 1.3961, time 54.58ms, mfu 0.01%\n",
      "step 6000: train loss 1.6908, val loss 1.7834\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 6000: loss 1.3851, time 1476.79ms, mfu 0.01%\n",
      "iter 6100: loss 1.4504, time 82.28ms, mfu 0.01%\n",
      "iter 6200: loss 1.2676, time 88.39ms, mfu 0.01%\n",
      "step 6250: train loss 1.6637, val loss 1.7973\n",
      "iter 6300: loss 1.2976, time 107.29ms, mfu 0.01%\n",
      "iter 6400: loss 1.3992, time 51.85ms, mfu 0.01%\n",
      "step 6500: train loss 1.6347, val loss 1.7743\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 6500: loss 1.3017, time 1311.56ms, mfu 0.01%\n",
      "iter 6600: loss 1.2518, time 66.81ms, mfu 0.01%\n",
      "iter 6700: loss 1.2995, time 58.97ms, mfu 0.01%\n",
      "step 6750: train loss 1.6510, val loss 1.7824\n",
      "iter 6800: loss 1.3665, time 99.33ms, mfu 0.01%\n",
      "iter 6900: loss 1.4513, time 117.03ms, mfu 0.01%\n",
      "step 7000: train loss 1.6241, val loss 1.7842\n",
      "iter 7000: loss 1.3433, time 1137.79ms, mfu 0.01%\n",
      "iter 7100: loss 1.4379, time 94.18ms, mfu 0.01%\n",
      "iter 7200: loss 1.3351, time 72.05ms, mfu 0.01%\n",
      "step 7250: train loss 1.6314, val loss 1.7707\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7300: loss 1.2987, time 124.75ms, mfu 0.01%\n",
      "iter 7400: loss 1.2709, time 103.24ms, mfu 0.01%\n",
      "step 7500: train loss 1.6354, val loss 1.7624\n",
      "saving checkpoint to out/proposed_margin_2.0_wo_weight_tying\n",
      "iter 7500: loss 1.3706, time 952.79ms, mfu 0.01%\n",
      "iter 7600: loss 1.3447, time 131.77ms, mfu 0.01%\n",
      "iter 7700: loss 1.4027, time 58.58ms, mfu 0.01%\n",
      "step 7750: train loss 1.6281, val loss 1.7984\n",
      "iter 7800: loss 1.2826, time 55.58ms, mfu 0.01%\n",
      "iter 7900: loss 1.3168, time 58.12ms, mfu 0.01%\n",
      "step 8000: train loss 1.6309, val loss 1.7704\n",
      "iter 8000: loss 1.2439, time 1516.71ms, mfu 0.01%\n",
      "done for margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.0, 2.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_wo_weight_tying\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "    \"weight_tying\": \"False\"\n",
    "        \n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ada5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8321, time 790.20ms, mfu -100.00%\n",
      "iter 100: loss 2.2954, time 41.96ms, mfu 0.03%\n",
      "iter 200: loss 2.2313, time 72.19ms, mfu 0.03%\n",
      "step 250: train loss 2.9990, val loss 3.0231\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 300: loss 2.0604, time 49.32ms, mfu 0.03%\n",
      "iter 400: loss 2.0050, time 45.94ms, mfu 0.03%\n",
      "step 500: train loss 2.8153, val loss 2.8303\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 500: loss 2.0611, time 783.21ms, mfu 0.03%\n",
      "iter 600: loss 1.9170, time 82.57ms, mfu 0.03%\n",
      "iter 700: loss 1.8252, time 65.53ms, mfu 0.02%\n",
      "step 750: train loss 2.6668, val loss 2.6879\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 800: loss 1.9168, time 49.10ms, mfu 0.02%\n",
      "iter 900: loss 1.7611, time 82.34ms, mfu 0.02%\n",
      "step 1000: train loss 2.5638, val loss 2.6231\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 1000: loss 1.7720, time 2071.95ms, mfu 0.02%\n",
      "iter 1100: loss 1.5983, time 87.81ms, mfu 0.02%\n",
      "iter 1200: loss 1.6180, time 49.63ms, mfu 0.02%\n",
      "step 1250: train loss 2.5106, val loss 2.5465\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 1300: loss 1.6070, time 103.60ms, mfu 0.02%\n",
      "iter 1400: loss 1.6197, time 79.08ms, mfu 0.02%\n",
      "step 1500: train loss 2.4273, val loss 2.4958\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 1500: loss 1.5548, time 905.52ms, mfu 0.02%\n",
      "iter 1600: loss 1.6580, time 112.82ms, mfu 0.02%\n",
      "iter 1700: loss 1.4832, time 46.97ms, mfu 0.02%\n",
      "step 1750: train loss 2.4055, val loss 2.4520\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 1800: loss 1.5811, time 81.88ms, mfu 0.02%\n",
      "iter 1900: loss 1.5048, time 74.41ms, mfu 0.02%\n",
      "step 2000: train loss 2.3661, val loss 2.4421\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 2000: loss 1.5917, time 792.75ms, mfu 0.02%\n",
      "iter 2100: loss 1.4910, time 131.29ms, mfu 0.02%\n",
      "iter 2200: loss 1.4694, time 50.30ms, mfu 0.02%\n",
      "step 2250: train loss 2.3168, val loss 2.4027\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 2300: loss 1.4922, time 63.85ms, mfu 0.02%\n",
      "iter 2400: loss 1.4291, time 68.17ms, mfu 0.02%\n",
      "step 2500: train loss 2.3229, val loss 2.3857\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 2500: loss 1.4806, time 765.02ms, mfu 0.02%\n",
      "iter 2600: loss 1.3890, time 80.04ms, mfu 0.02%\n",
      "iter 2700: loss 1.4029, time 50.84ms, mfu 0.02%\n",
      "step 2750: train loss 2.2938, val loss 2.3647\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 2800: loss 1.5794, time 72.69ms, mfu 0.02%\n",
      "iter 2900: loss 1.4139, time 76.66ms, mfu 0.02%\n",
      "step 3000: train loss 2.3162, val loss 2.3821\n",
      "iter 3000: loss 1.4256, time 775.22ms, mfu 0.02%\n",
      "iter 3100: loss 1.3309, time 51.35ms, mfu 0.02%\n",
      "iter 3200: loss 1.3870, time 57.19ms, mfu 0.02%\n",
      "step 3250: train loss 2.2580, val loss 2.3348\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 3300: loss 1.4548, time 74.89ms, mfu 0.02%\n",
      "iter 3400: loss 1.3886, time 86.99ms, mfu 0.02%\n",
      "step 3500: train loss 2.2511, val loss 2.3458\n",
      "iter 3500: loss 1.3097, time 915.90ms, mfu 0.02%\n",
      "iter 3600: loss 1.3206, time 99.25ms, mfu 0.01%\n",
      "iter 3700: loss 1.3043, time 49.42ms, mfu 0.02%\n",
      "step 3750: train loss 2.2306, val loss 2.3185\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 3800: loss 1.3686, time 48.80ms, mfu 0.02%\n",
      "iter 3900: loss 1.2497, time 167.26ms, mfu 0.02%\n",
      "step 4000: train loss 2.2095, val loss 2.3258\n",
      "iter 4000: loss 1.4377, time 986.28ms, mfu 0.01%\n",
      "iter 4100: loss 1.2467, time 60.86ms, mfu 0.02%\n",
      "iter 4200: loss 1.2157, time 58.92ms, mfu 0.02%\n",
      "step 4250: train loss 2.2048, val loss 2.3087\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 4300: loss 1.2560, time 80.77ms, mfu 0.02%\n",
      "iter 4400: loss 1.4070, time 88.60ms, mfu 0.02%\n",
      "step 4500: train loss 2.2065, val loss 2.3051\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 4500: loss 1.3328, time 1020.32ms, mfu 0.01%\n",
      "iter 4600: loss 1.3987, time 99.11ms, mfu 0.01%\n",
      "iter 4700: loss 1.2862, time 91.89ms, mfu 0.01%\n",
      "step 4750: train loss 2.1961, val loss 2.3152\n",
      "iter 4800: loss 1.3321, time 91.78ms, mfu 0.01%\n",
      "iter 4900: loss 1.1905, time 92.02ms, mfu 0.01%\n",
      "step 5000: train loss 2.1585, val loss 2.2751\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 5000: loss 1.3985, time 2300.33ms, mfu 0.01%\n",
      "iter 5100: loss 1.2805, time 113.70ms, mfu 0.01%\n",
      "iter 5200: loss 1.2853, time 135.93ms, mfu 0.01%\n",
      "step 5250: train loss 2.1756, val loss 2.2583\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 5300: loss 1.1286, time 89.12ms, mfu 0.01%\n",
      "iter 5400: loss 1.3074, time 69.16ms, mfu 0.01%\n",
      "step 5500: train loss 2.1877, val loss 2.2518\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 5500: loss 1.3521, time 1176.53ms, mfu 0.01%\n",
      "iter 5600: loss 1.3129, time 92.25ms, mfu 0.01%\n",
      "iter 5700: loss 1.2297, time 103.91ms, mfu 0.01%\n",
      "step 5750: train loss 2.1505, val loss 2.2441\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 5800: loss 1.2401, time 101.49ms, mfu 0.01%\n",
      "iter 5900: loss 1.2564, time 67.06ms, mfu 0.01%\n",
      "step 6000: train loss 2.1451, val loss 2.2538\n",
      "iter 6000: loss 1.2721, time 1730.53ms, mfu 0.01%\n",
      "iter 6100: loss 1.3133, time 144.14ms, mfu 0.01%\n",
      "iter 6200: loss 1.2413, time 95.94ms, mfu 0.01%\n",
      "step 6250: train loss 2.1603, val loss 2.2454\n",
      "iter 6300: loss 1.2899, time 127.63ms, mfu 0.01%\n",
      "iter 6400: loss 1.1436, time 110.61ms, mfu 0.01%\n",
      "step 6500: train loss 2.1494, val loss 2.2505\n",
      "iter 6500: loss 1.2508, time 1140.89ms, mfu 0.01%\n",
      "iter 6600: loss 1.2914, time 123.44ms, mfu 0.01%\n",
      "iter 6700: loss 1.2035, time 163.87ms, mfu 0.01%\n",
      "step 6750: train loss 2.1435, val loss 2.2454\n",
      "iter 6800: loss 1.2416, time 105.70ms, mfu 0.01%\n",
      "iter 6900: loss 1.2433, time 84.13ms, mfu 0.01%\n",
      "step 7000: train loss 2.1260, val loss 2.2471\n",
      "iter 7000: loss 1.2980, time 957.36ms, mfu 0.01%\n",
      "iter 7100: loss 1.2555, time 100.71ms, mfu 0.01%\n",
      "iter 7200: loss 1.1649, time 226.95ms, mfu 0.01%\n",
      "step 7250: train loss 2.1300, val loss 2.2441\n",
      "iter 7300: loss 1.2778, time 112.58ms, mfu 0.01%\n",
      "iter 7400: loss 1.2713, time 95.80ms, mfu 0.01%\n",
      "step 7500: train loss 2.1181, val loss 2.2400\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 7500: loss 1.2653, time 1434.58ms, mfu 0.01%\n",
      "iter 7600: loss 1.1978, time 94.75ms, mfu 0.01%\n",
      "iter 7700: loss 1.1764, time 108.11ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7750: train loss 2.1264, val loss 2.2368\n",
      "saving checkpoint to out/proposed_margin_1.0_separated_embeddings\n",
      "iter 7800: loss 1.2721, time 100.54ms, mfu 0.01%\n",
      "iter 7900: loss 1.2210, time 89.21ms, mfu 0.01%\n",
      "step 8000: train loss 2.1067, val loss 2.2398\n",
      "iter 8000: loss 1.2777, time 1698.93ms, mfu 0.01%\n",
      "done for margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_2.0_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8685, time 2163.57ms, mfu -100.00%\n",
      "iter 100: loss 2.4931, time 225.45ms, mfu 0.01%\n",
      "iter 200: loss 2.3889, time 153.41ms, mfu 0.01%\n",
      "step 250: train loss 2.6380, val loss 2.6641\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 300: loss 2.1905, time 139.69ms, mfu 0.01%\n",
      "iter 400: loss 2.1480, time 84.99ms, mfu 0.01%\n",
      "step 500: train loss 2.4245, val loss 2.4466\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 500: loss 2.2213, time 1417.97ms, mfu 0.01%\n",
      "iter 600: loss 2.0906, time 99.91ms, mfu 0.01%\n",
      "iter 700: loss 1.9896, time 79.41ms, mfu 0.01%\n",
      "step 750: train loss 2.2990, val loss 2.3161\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 800: loss 2.0726, time 84.20ms, mfu 0.01%\n",
      "iter 900: loss 1.9112, time 126.00ms, mfu 0.01%\n",
      "step 1000: train loss 2.1971, val loss 2.2607\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 1000: loss 1.9363, time 1307.93ms, mfu 0.01%\n",
      "iter 1100: loss 1.7479, time 83.15ms, mfu 0.01%\n",
      "iter 1200: loss 1.7417, time 140.07ms, mfu 0.01%\n",
      "step 1250: train loss 2.1067, val loss 2.1667\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 1300: loss 1.7859, time 123.55ms, mfu 0.01%\n",
      "iter 1400: loss 1.7903, time 102.85ms, mfu 0.01%\n",
      "step 1500: train loss 2.0297, val loss 2.1158\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 1500: loss 1.6845, time 1514.75ms, mfu 0.01%\n",
      "iter 1600: loss 1.8133, time 269.38ms, mfu 0.01%\n",
      "iter 1700: loss 1.5863, time 154.35ms, mfu 0.01%\n",
      "step 1750: train loss 1.9966, val loss 2.0523\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 1800: loss 1.7209, time 178.09ms, mfu 0.01%\n",
      "iter 1900: loss 1.6420, time 380.01ms, mfu 0.01%\n",
      "step 2000: train loss 1.9575, val loss 2.0526\n",
      "iter 2000: loss 1.7410, time 1319.73ms, mfu 0.01%\n",
      "iter 2100: loss 1.6595, time 104.74ms, mfu 0.01%\n",
      "iter 2200: loss 1.6339, time 111.78ms, mfu 0.01%\n",
      "step 2250: train loss 1.8896, val loss 2.0020\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 2300: loss 1.6313, time 86.95ms, mfu 0.01%\n",
      "iter 2400: loss 1.5737, time 188.04ms, mfu 0.01%\n",
      "step 2500: train loss 1.9000, val loss 1.9844\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 2500: loss 1.6260, time 1482.89ms, mfu 0.01%\n",
      "iter 2600: loss 1.5279, time 107.97ms, mfu 0.01%\n",
      "iter 2700: loss 1.5438, time 150.32ms, mfu 0.01%\n",
      "step 2750: train loss 1.8708, val loss 1.9519\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 2800: loss 1.7169, time 89.92ms, mfu 0.01%\n",
      "iter 2900: loss 1.5253, time 92.36ms, mfu 0.01%\n",
      "step 3000: train loss 1.8763, val loss 1.9705\n",
      "iter 3000: loss 1.5350, time 1381.95ms, mfu 0.01%\n",
      "iter 3100: loss 1.4502, time 277.27ms, mfu 0.01%\n",
      "iter 3200: loss 1.4994, time 130.42ms, mfu 0.01%\n",
      "step 3250: train loss 1.8185, val loss 1.9283\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 3300: loss 1.5888, time 100.29ms, mfu 0.01%\n",
      "iter 3400: loss 1.5126, time 224.96ms, mfu 0.01%\n",
      "step 3500: train loss 1.8002, val loss 1.9237\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 3500: loss 1.4065, time 1446.19ms, mfu 0.01%\n",
      "iter 3600: loss 1.4300, time 157.18ms, mfu 0.01%\n",
      "iter 3700: loss 1.4443, time 95.08ms, mfu 0.01%\n",
      "step 3750: train loss 1.7822, val loss 1.8899\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 3800: loss 1.4831, time 341.09ms, mfu 0.01%\n",
      "iter 3900: loss 1.3519, time 99.12ms, mfu 0.01%\n",
      "step 4000: train loss 1.7508, val loss 1.9078\n",
      "iter 4000: loss 1.5599, time 1072.65ms, mfu 0.01%\n",
      "iter 4100: loss 1.3371, time 112.58ms, mfu 0.01%\n",
      "iter 4200: loss 1.3392, time 192.22ms, mfu 0.01%\n",
      "step 4250: train loss 1.7467, val loss 1.8853\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 4300: loss 1.3460, time 111.75ms, mfu 0.01%\n",
      "iter 4400: loss 1.5259, time 84.53ms, mfu 0.01%\n",
      "step 4500: train loss 1.7382, val loss 1.8888\n",
      "iter 4500: loss 1.4910, time 1550.60ms, mfu 0.01%\n",
      "iter 4600: loss 1.5153, time 147.70ms, mfu 0.01%\n",
      "iter 4700: loss 1.3954, time 117.41ms, mfu 0.01%\n",
      "step 4750: train loss 1.7328, val loss 1.8829\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 4800: loss 1.4565, time 141.17ms, mfu 0.01%\n",
      "iter 4900: loss 1.3017, time 137.46ms, mfu 0.01%\n",
      "step 5000: train loss 1.6798, val loss 1.8514\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 5000: loss 1.5249, time 1354.32ms, mfu 0.01%\n",
      "iter 5100: loss 1.4021, time 102.41ms, mfu 0.01%\n",
      "iter 5200: loss 1.3860, time 217.01ms, mfu 0.01%\n",
      "step 5250: train loss 1.6973, val loss 1.8200\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 5300: loss 1.2260, time 87.72ms, mfu 0.01%\n",
      "iter 5400: loss 1.4098, time 102.08ms, mfu 0.01%\n",
      "step 5500: train loss 1.7181, val loss 1.8044\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 5500: loss 1.4779, time 1448.01ms, mfu 0.01%\n",
      "iter 5600: loss 1.4214, time 152.41ms, mfu 0.01%\n",
      "iter 5700: loss 1.3204, time 96.98ms, mfu 0.01%\n",
      "step 5750: train loss 1.6599, val loss 1.7976\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 5800: loss 1.3518, time 95.77ms, mfu 0.01%\n",
      "iter 5900: loss 1.3611, time 95.04ms, mfu 0.01%\n",
      "step 6000: train loss 1.6570, val loss 1.8129\n",
      "iter 6000: loss 1.4084, time 1780.67ms, mfu 0.01%\n",
      "iter 6100: loss 1.4420, time 163.61ms, mfu 0.01%\n",
      "iter 6200: loss 1.3242, time 91.77ms, mfu 0.01%\n",
      "step 6250: train loss 1.6758, val loss 1.7900\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 6300: loss 1.4208, time 134.56ms, mfu 0.01%\n",
      "iter 6400: loss 1.2393, time 87.08ms, mfu 0.01%\n",
      "step 6500: train loss 1.6603, val loss 1.7952\n",
      "iter 6500: loss 1.3357, time 1533.13ms, mfu 0.01%\n",
      "iter 6600: loss 1.4170, time 118.11ms, mfu 0.01%\n",
      "iter 6700: loss 1.3437, time 192.94ms, mfu 0.01%\n",
      "step 6750: train loss 1.6486, val loss 1.7875\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 6800: loss 1.3499, time 127.72ms, mfu 0.01%\n",
      "iter 6900: loss 1.3404, time 134.22ms, mfu 0.01%\n",
      "step 7000: train loss 1.6266, val loss 1.7991\n",
      "iter 7000: loss 1.4373, time 1681.65ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7100: loss 1.3753, time 106.14ms, mfu 0.01%\n",
      "iter 7200: loss 1.2651, time 140.58ms, mfu 0.01%\n",
      "step 7250: train loss 1.6273, val loss 1.7867\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 7300: loss 1.3721, time 323.87ms, mfu 0.01%\n",
      "iter 7400: loss 1.3809, time 111.44ms, mfu 0.01%\n",
      "step 7500: train loss 1.6153, val loss 1.7770\n",
      "saving checkpoint to out/proposed_margin_2.0_separated_embeddings\n",
      "iter 7500: loss 1.3317, time 1953.87ms, mfu 0.01%\n",
      "iter 7600: loss 1.3200, time 116.14ms, mfu 0.01%\n",
      "iter 7700: loss 1.2579, time 130.29ms, mfu 0.01%\n",
      "step 7750: train loss 1.6349, val loss 1.7921\n",
      "iter 7800: loss 1.3961, time 92.00ms, mfu 0.01%\n",
      "iter 7900: loss 1.3337, time 89.16ms, mfu 0.01%\n",
      "step 8000: train loss 1.6162, val loss 1.7917\n",
      "iter 8000: loss 1.3627, time 1587.85ms, mfu 0.01%\n",
      "done for margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.0, 2.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_separated_embeddings\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "905499d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_757/3992427102.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.0_separated_embeddings/ckpt_fixed.pt\n",
      "Fixed checkpoint saved to out/proposed_margin_2.0_separated_embeddings/ckpt_fixed.pt\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "for margin in [1.0, 2.0]:\n",
    "\n",
    "    # Path to your original checkpoint\n",
    "    ckpt_path = f\"out/proposed_margin_{margin}_separated_embeddings/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"out/proposed_margin_{margin}_separated_embeddings/ckpt_fixed.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9bebc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv out/proposed_margin_1.0_separated_embeddings/ckpt.pt out/proposed_margin_1.0_separated_embeddings/ckpt_original.pt\n",
    "!mv out/proposed_margin_2.0_separated_embeddings/ckpt.pt out/proposed_margin_2.0_separated_embeddings/ckpt_original.pt\n",
    "\n",
    "!mv out/proposed_margin_1.0_separated_embeddings/ckpt_fixed.pt out/proposed_margin_1.0_separated_embeddings/ckpt.pt\n",
    "!mv out/proposed_margin_2.0_separated_embeddings/ckpt_fixed.pt out/proposed_margin_2.0_separated_embeddings/ckpt.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56769aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_1.0_1.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 1.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8114, time 977.37ms, mfu -100.00%\n",
      "iter 100: loss 2.3186, time 59.47ms, mfu 0.02%\n",
      "iter 200: loss 2.0532, time 85.92ms, mfu 0.02%\n",
      "step 250: train loss 2.9723, val loss 2.9867\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 300: loss 2.0173, time 55.31ms, mfu 0.02%\n",
      "iter 400: loss 1.9894, time 80.72ms, mfu 0.02%\n",
      "step 500: train loss 2.8051, val loss 2.8390\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 500: loss 1.8144, time 1154.13ms, mfu 0.02%\n",
      "iter 600: loss 1.7918, time 118.25ms, mfu 0.02%\n",
      "iter 700: loss 1.6811, time 53.15ms, mfu 0.02%\n",
      "step 750: train loss 2.6424, val loss 2.6567\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 800: loss 1.7685, time 56.11ms, mfu 0.02%\n",
      "iter 900: loss 1.5971, time 71.34ms, mfu 0.02%\n",
      "step 1000: train loss 2.5455, val loss 2.5876\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 1000: loss 1.6413, time 1314.22ms, mfu 0.02%\n",
      "iter 1100: loss 1.5645, time 79.46ms, mfu 0.02%\n",
      "iter 1200: loss 1.5325, time 135.84ms, mfu 0.02%\n",
      "step 1250: train loss 2.4487, val loss 2.5443\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 1300: loss 1.5096, time 90.52ms, mfu 0.02%\n",
      "iter 1400: loss 1.4668, time 95.71ms, mfu 0.02%\n",
      "step 1500: train loss 2.4244, val loss 2.4877\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 1500: loss 1.4044, time 1350.99ms, mfu 0.01%\n",
      "iter 1600: loss 1.4654, time 72.45ms, mfu 0.01%\n",
      "iter 1700: loss 1.4061, time 101.14ms, mfu 0.01%\n",
      "step 1750: train loss 2.3852, val loss 2.4689\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 1800: loss 1.4110, time 50.75ms, mfu 0.02%\n",
      "iter 1900: loss 1.4439, time 113.41ms, mfu 0.02%\n",
      "step 2000: train loss 2.3284, val loss 2.4465\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 2000: loss 1.2966, time 1247.02ms, mfu 0.01%\n",
      "iter 2100: loss 1.3764, time 54.66ms, mfu 0.01%\n",
      "iter 2200: loss 1.4022, time 53.86ms, mfu 0.02%\n",
      "step 2250: train loss 2.3401, val loss 2.4187\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 2300: loss 1.4035, time 88.67ms, mfu 0.02%\n",
      "iter 2400: loss 1.3627, time 97.99ms, mfu 0.02%\n",
      "step 2500: train loss 2.3004, val loss 2.3640\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 2500: loss 1.3291, time 1432.01ms, mfu 0.01%\n",
      "iter 2600: loss 1.3181, time 95.72ms, mfu 0.01%\n",
      "iter 2700: loss 1.2539, time 171.79ms, mfu 0.01%\n",
      "step 2750: train loss 2.2956, val loss 2.3883\n",
      "iter 2800: loss 1.3868, time 73.11ms, mfu 0.01%\n",
      "iter 2900: loss 1.3369, time 48.62ms, mfu 0.01%\n",
      "step 3000: train loss 2.2555, val loss 2.3305\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 3000: loss 1.3944, time 1190.76ms, mfu 0.01%\n",
      "iter 3100: loss 1.4181, time 99.33ms, mfu 0.01%\n",
      "iter 3200: loss 1.3352, time 82.16ms, mfu 0.01%\n",
      "step 3250: train loss 2.2077, val loss 2.3319\n",
      "iter 3300: loss 1.3437, time 112.76ms, mfu 0.01%\n",
      "iter 3400: loss 1.3551, time 90.87ms, mfu 0.01%\n",
      "step 3500: train loss 2.2277, val loss 2.3204\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 3500: loss 1.4387, time 1154.49ms, mfu 0.01%\n",
      "iter 3600: loss 1.3170, time 101.58ms, mfu 0.01%\n",
      "iter 3700: loss 1.3125, time 52.52ms, mfu 0.01%\n",
      "step 3750: train loss 2.2029, val loss 2.2997\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 3800: loss 1.2937, time 81.89ms, mfu 0.01%\n",
      "iter 3900: loss 1.1419, time 68.02ms, mfu 0.01%\n",
      "step 4000: train loss 2.1906, val loss 2.3126\n",
      "iter 4000: loss 1.2605, time 777.48ms, mfu 0.01%\n",
      "iter 4100: loss 1.3177, time 57.68ms, mfu 0.01%\n",
      "iter 4200: loss 1.1787, time 116.33ms, mfu 0.01%\n",
      "step 4250: train loss 2.1547, val loss 2.2686\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 4300: loss 1.2295, time 210.83ms, mfu 0.01%\n",
      "iter 4400: loss 1.2711, time 91.19ms, mfu 0.01%\n",
      "step 4500: train loss 2.1675, val loss 2.2757\n",
      "iter 4500: loss 1.2053, time 919.60ms, mfu 0.01%\n",
      "iter 4600: loss 1.3097, time 57.46ms, mfu 0.01%\n",
      "iter 4700: loss 1.1457, time 130.52ms, mfu 0.01%\n",
      "step 4750: train loss 2.1762, val loss 2.2714\n",
      "iter 4800: loss 1.2213, time 56.57ms, mfu 0.01%\n",
      "iter 4900: loss 1.2858, time 55.68ms, mfu 0.01%\n",
      "step 5000: train loss 2.1475, val loss 2.2584\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 5000: loss 1.1822, time 928.93ms, mfu 0.01%\n",
      "iter 5100: loss 1.1705, time 82.67ms, mfu 0.01%\n",
      "iter 5200: loss 1.1820, time 110.10ms, mfu 0.01%\n",
      "step 5250: train loss 2.1225, val loss 2.2529\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 5300: loss 1.2337, time 54.87ms, mfu 0.01%\n",
      "iter 5400: loss 1.1000, time 53.01ms, mfu 0.02%\n",
      "step 5500: train loss 2.1443, val loss 2.2358\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 5500: loss 1.1422, time 1177.47ms, mfu 0.01%\n",
      "iter 5600: loss 1.1451, time 114.08ms, mfu 0.01%\n",
      "iter 5700: loss 1.1579, time 52.89ms, mfu 0.01%\n",
      "step 5750: train loss 2.1242, val loss 2.2363\n",
      "iter 5800: loss 1.2644, time 92.61ms, mfu 0.01%\n",
      "iter 5900: loss 1.2002, time 104.02ms, mfu 0.01%\n",
      "step 6000: train loss 2.1524, val loss 2.2210\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 6000: loss 1.1580, time 1296.25ms, mfu 0.01%\n",
      "iter 6100: loss 1.2598, time 75.85ms, mfu 0.01%\n",
      "iter 6200: loss 1.0639, time 107.33ms, mfu 0.01%\n",
      "step 6250: train loss 2.1290, val loss 2.2278\n",
      "iter 6300: loss 1.1076, time 55.20ms, mfu 0.01%\n",
      "iter 6400: loss 1.2013, time 53.19ms, mfu 0.02%\n",
      "step 6500: train loss 2.1037, val loss 2.2191\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 6500: loss 1.0785, time 944.74ms, mfu 0.01%\n",
      "iter 6600: loss 1.0502, time 91.73ms, mfu 0.01%\n",
      "iter 6700: loss 1.0622, time 96.75ms, mfu 0.01%\n",
      "step 6750: train loss 2.1215, val loss 2.2282\n",
      "iter 6800: loss 1.1494, time 92.56ms, mfu 0.01%\n",
      "iter 6900: loss 1.2463, time 81.35ms, mfu 0.01%\n",
      "step 7000: train loss 2.1008, val loss 2.2231\n",
      "iter 7000: loss 1.1674, time 965.66ms, mfu 0.01%\n",
      "iter 7100: loss 1.2242, time 61.21ms, mfu 0.01%\n",
      "iter 7200: loss 1.1322, time 125.08ms, mfu 0.01%\n",
      "step 7250: train loss 2.1129, val loss 2.2196\n",
      "iter 7300: loss 1.0816, time 99.23ms, mfu 0.01%\n",
      "iter 7400: loss 1.0669, time 64.38ms, mfu 0.01%\n",
      "step 7500: train loss 2.1060, val loss 2.2084\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0\n",
      "iter 7500: loss 1.1591, time 1184.22ms, mfu 0.01%\n",
      "iter 7600: loss 1.1343, time 60.54ms, mfu 0.01%\n",
      "iter 7700: loss 1.1983, time 127.76ms, mfu 0.01%\n",
      "step 7750: train loss 2.1093, val loss 2.2376\n",
      "iter 7800: loss 1.0959, time 83.83ms, mfu 0.01%\n",
      "iter 7900: loss 1.0850, time 92.38ms, mfu 0.01%\n",
      "step 8000: train loss 2.1126, val loss 2.2156\n",
      "iter 8000: loss 1.0489, time 863.05ms, mfu 0.01%\n",
      "done for softminus_c=1.0, margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_1.0_5.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 5.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 5.4550, time 1951.82ms, mfu -100.00%\n",
      "iter 100: loss 2.6128, time 74.45ms, mfu 0.02%\n",
      "iter 200: loss 2.2725, time 90.51ms, mfu 0.02%\n",
      "step 250: train loss 3.9077, val loss 3.9122\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 300: loss 2.2120, time 94.61ms, mfu 0.02%\n",
      "iter 400: loss 2.1932, time 50.25ms, mfu 0.02%\n",
      "step 500: train loss 3.8033, val loss 3.8184\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 500: loss 1.9630, time 879.15ms, mfu 0.02%\n",
      "iter 600: loss 1.9121, time 90.66ms, mfu 0.02%\n",
      "iter 700: loss 1.8588, time 76.33ms, mfu 0.02%\n",
      "step 750: train loss 3.7490, val loss 3.7563\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 800: loss 1.9077, time 66.01ms, mfu 0.02%\n",
      "iter 900: loss 1.6991, time 119.00ms, mfu 0.02%\n",
      "step 1000: train loss 3.6966, val loss 3.7189\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 1000: loss 1.8167, time 1326.47ms, mfu 0.01%\n",
      "iter 1100: loss 1.6978, time 107.05ms, mfu 0.01%\n",
      "iter 1200: loss 1.6402, time 120.37ms, mfu 0.01%\n",
      "step 1250: train loss 3.6400, val loss 3.6805\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 1300: loss 1.6523, time 135.63ms, mfu 0.01%\n",
      "iter 1400: loss 1.5876, time 49.99ms, mfu 0.01%\n",
      "step 1500: train loss 3.6434, val loss 3.6733\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 1500: loss 1.5892, time 1447.95ms, mfu 0.01%\n",
      "iter 1600: loss 1.5950, time 69.45ms, mfu 0.01%\n",
      "iter 1700: loss 1.5384, time 50.77ms, mfu 0.01%\n",
      "step 1750: train loss 3.6056, val loss 3.6365\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 1800: loss 1.5529, time 102.00ms, mfu 0.01%\n",
      "iter 1900: loss 1.5924, time 68.39ms, mfu 0.02%\n",
      "step 2000: train loss 3.5845, val loss 3.6348\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 2000: loss 1.4153, time 1057.16ms, mfu 0.01%\n",
      "iter 2100: loss 1.5052, time 87.34ms, mfu 0.01%\n",
      "iter 2200: loss 1.5933, time 59.19ms, mfu 0.01%\n",
      "step 2250: train loss 3.6129, val loss 3.6459\n",
      "iter 2300: loss 1.5436, time 81.64ms, mfu 0.01%\n",
      "iter 2400: loss 1.5154, time 194.08ms, mfu 0.01%\n",
      "step 2500: train loss 3.5929, val loss 3.6186\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 2500: loss 1.4835, time 1027.09ms, mfu 0.01%\n",
      "iter 2600: loss 1.4777, time 68.10ms, mfu 0.01%\n",
      "iter 2700: loss 1.3954, time 121.64ms, mfu 0.01%\n",
      "step 2750: train loss 3.5605, val loss 3.6028\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 2800: loss 1.5188, time 99.89ms, mfu 0.01%\n",
      "iter 2900: loss 1.4581, time 54.09ms, mfu 0.01%\n",
      "step 3000: train loss 3.5559, val loss 3.5835\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 3000: loss 1.5488, time 1261.07ms, mfu 0.01%\n",
      "iter 3100: loss 1.5730, time 202.84ms, mfu 0.01%\n",
      "iter 3200: loss 1.5096, time 102.74ms, mfu 0.01%\n",
      "step 3250: train loss 3.5455, val loss 3.6045\n",
      "iter 3300: loss 1.4938, time 106.32ms, mfu 0.01%\n",
      "iter 3400: loss 1.5141, time 225.08ms, mfu 0.01%\n",
      "step 3500: train loss 3.5465, val loss 3.5816\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 3500: loss 1.6022, time 1248.45ms, mfu 0.01%\n",
      "iter 3600: loss 1.4206, time 94.19ms, mfu 0.01%\n",
      "iter 3700: loss 1.4273, time 306.94ms, mfu 0.01%\n",
      "step 3750: train loss 3.5197, val loss 3.5611\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 3800: loss 1.4375, time 108.23ms, mfu 0.01%\n",
      "iter 3900: loss 1.2994, time 50.65ms, mfu 0.01%\n",
      "step 4000: train loss 3.5128, val loss 3.5681\n",
      "iter 4000: loss 1.3691, time 1074.02ms, mfu 0.01%\n",
      "iter 4100: loss 1.5025, time 75.34ms, mfu 0.01%\n",
      "iter 4200: loss 1.3246, time 96.61ms, mfu 0.01%\n",
      "step 4250: train loss 3.4964, val loss 3.5475\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 4300: loss 1.3801, time 164.60ms, mfu 0.01%\n",
      "iter 4400: loss 1.4256, time 111.56ms, mfu 0.01%\n",
      "step 4500: train loss 3.5025, val loss 3.5507\n",
      "iter 4500: loss 1.3474, time 950.30ms, mfu 0.01%\n",
      "iter 4600: loss 1.4541, time 51.59ms, mfu 0.01%\n",
      "iter 4700: loss 1.3345, time 115.80ms, mfu 0.01%\n",
      "step 4750: train loss 3.4857, val loss 3.5262\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 4800: loss 1.3431, time 48.70ms, mfu 0.01%\n",
      "iter 4900: loss 1.4402, time 295.29ms, mfu 0.01%\n",
      "step 5000: train loss 3.4766, val loss 3.5178\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 5000: loss 1.3139, time 1118.36ms, mfu 0.01%\n",
      "iter 5100: loss 1.2990, time 54.64ms, mfu 0.01%\n",
      "iter 5200: loss 1.3164, time 59.87ms, mfu 0.01%\n",
      "step 5250: train loss 3.4382, val loss 3.5067\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 5300: loss 1.3618, time 93.37ms, mfu 0.01%\n",
      "iter 5400: loss 1.2271, time 109.03ms, mfu 0.01%\n",
      "step 5500: train loss 3.4621, val loss 3.4999\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 5500: loss 1.2638, time 976.57ms, mfu 0.01%\n",
      "iter 5600: loss 1.2957, time 85.13ms, mfu 0.01%\n",
      "iter 5700: loss 1.2886, time 74.02ms, mfu 0.01%\n",
      "step 5750: train loss 3.4466, val loss 3.4946\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 5800: loss 1.3876, time 86.07ms, mfu 0.01%\n",
      "iter 5900: loss 1.3520, time 53.34ms, mfu 0.01%\n",
      "step 6000: train loss 3.4399, val loss 3.4634\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 6000: loss 1.2933, time 1139.00ms, mfu 0.01%\n",
      "iter 6100: loss 1.4303, time 83.02ms, mfu 0.01%\n",
      "iter 6200: loss 1.1840, time 124.82ms, mfu 0.01%\n",
      "step 6250: train loss 3.4401, val loss 3.4819\n",
      "iter 6300: loss 1.2360, time 88.87ms, mfu 0.01%\n",
      "iter 6400: loss 1.3491, time 57.05ms, mfu 0.01%\n",
      "step 6500: train loss 3.4095, val loss 3.4639\n",
      "iter 6500: loss 1.2259, time 1083.07ms, mfu 0.01%\n",
      "iter 6600: loss 1.2254, time 98.85ms, mfu 0.01%\n",
      "iter 6700: loss 1.2106, time 198.15ms, mfu 0.01%\n",
      "step 6750: train loss 3.4062, val loss 3.4557\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 6800: loss 1.2964, time 148.00ms, mfu 0.01%\n",
      "iter 6900: loss 1.3918, time 102.27ms, mfu 0.01%\n",
      "step 7000: train loss 3.3899, val loss 3.4524\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 7000: loss 1.2871, time 890.04ms, mfu 0.01%\n",
      "iter 7100: loss 1.3750, time 131.25ms, mfu 0.01%\n",
      "iter 7200: loss 1.2644, time 63.30ms, mfu 0.01%\n",
      "step 7250: train loss 3.4039, val loss 3.4505\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 7300: loss 1.2062, time 67.75ms, mfu 0.01%\n",
      "iter 7400: loss 1.1700, time 89.35ms, mfu 0.01%\n",
      "step 7500: train loss 3.3977, val loss 3.4426\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n",
      "iter 7500: loss 1.2862, time 1458.53ms, mfu 0.01%\n",
      "iter 7600: loss 1.2621, time 97.04ms, mfu 0.01%\n",
      "iter 7700: loss 1.3422, time 95.13ms, mfu 0.01%\n",
      "step 7750: train loss 3.3976, val loss 3.4608\n",
      "iter 7800: loss 1.2306, time 83.91ms, mfu 0.01%\n",
      "iter 7900: loss 1.2417, time 122.22ms, mfu 0.01%\n",
      "step 8000: train loss 3.3911, val loss 3.4355\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8000: loss 1.1937, time 975.81ms, mfu 0.01%\n",
      "done for softminus_c=5.0, margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_2.0_1.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 1.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8597, time 1202.04ms, mfu -100.00%\n",
      "iter 100: loss 2.5051, time 55.16ms, mfu 0.02%\n",
      "iter 200: loss 2.2661, time 54.71ms, mfu 0.02%\n",
      "step 250: train loss 2.6576, val loss 2.6679\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 300: loss 2.1932, time 60.51ms, mfu 0.02%\n",
      "iter 400: loss 2.1682, time 77.81ms, mfu 0.02%\n",
      "step 500: train loss 2.4407, val loss 2.4782\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 500: loss 1.9774, time 1062.32ms, mfu 0.02%\n",
      "iter 600: loss 1.9544, time 101.87ms, mfu 0.02%\n",
      "iter 700: loss 1.8971, time 101.24ms, mfu 0.02%\n",
      "step 750: train loss 2.3001, val loss 2.3173\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 800: loss 1.9831, time 60.75ms, mfu 0.02%\n",
      "iter 900: loss 1.7626, time 64.16ms, mfu 0.02%\n",
      "step 1000: train loss 2.1893, val loss 2.2365\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 1000: loss 1.8286, time 1197.40ms, mfu 0.02%\n",
      "iter 1100: loss 1.7458, time 170.14ms, mfu 0.02%\n",
      "iter 1200: loss 1.7078, time 544.76ms, mfu 0.01%\n",
      "step 1250: train loss 2.0823, val loss 2.2005\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 1300: loss 1.6789, time 61.74ms, mfu 0.02%\n",
      "iter 1400: loss 1.6137, time 85.34ms, mfu 0.02%\n",
      "step 1500: train loss 2.0509, val loss 2.1352\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 1500: loss 1.5844, time 1495.59ms, mfu 0.01%\n",
      "iter 1600: loss 1.6696, time 98.07ms, mfu 0.01%\n",
      "iter 1700: loss 1.5549, time 179.62ms, mfu 0.01%\n",
      "step 1750: train loss 1.9872, val loss 2.0915\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 1800: loss 1.5498, time 117.76ms, mfu 0.01%\n",
      "iter 1900: loss 1.5601, time 58.89ms, mfu 0.01%\n",
      "step 2000: train loss 1.9440, val loss 2.0994\n",
      "iter 2000: loss 1.4384, time 1528.40ms, mfu 0.01%\n",
      "iter 2100: loss 1.5156, time 75.43ms, mfu 0.01%\n",
      "iter 2200: loss 1.5456, time 106.17ms, mfu 0.01%\n",
      "step 2250: train loss 1.9531, val loss 2.0525\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 2300: loss 1.5666, time 92.61ms, mfu 0.01%\n",
      "iter 2400: loss 1.5041, time 79.64ms, mfu 0.01%\n",
      "step 2500: train loss 1.9069, val loss 1.9859\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 2500: loss 1.4863, time 1250.18ms, mfu 0.01%\n",
      "iter 2600: loss 1.4509, time 174.21ms, mfu 0.01%\n",
      "iter 2700: loss 1.3813, time 58.11ms, mfu 0.01%\n",
      "step 2750: train loss 1.8955, val loss 2.0110\n",
      "iter 2800: loss 1.5466, time 55.93ms, mfu 0.01%\n",
      "iter 2900: loss 1.4654, time 73.81ms, mfu 0.01%\n",
      "step 3000: train loss 1.8588, val loss 1.9538\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 3000: loss 1.5723, time 1216.87ms, mfu 0.01%\n",
      "iter 3100: loss 1.5640, time 60.53ms, mfu 0.01%\n",
      "iter 3200: loss 1.5062, time 97.46ms, mfu 0.01%\n",
      "step 3250: train loss 1.7926, val loss 1.9445\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 3300: loss 1.4531, time 65.14ms, mfu 0.01%\n",
      "iter 3400: loss 1.4871, time 63.63ms, mfu 0.01%\n",
      "step 3500: train loss 1.8170, val loss 1.9361\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 3500: loss 1.5817, time 1390.87ms, mfu 0.01%\n",
      "iter 3600: loss 1.4746, time 68.04ms, mfu 0.01%\n",
      "iter 3700: loss 1.4236, time 77.89ms, mfu 0.01%\n",
      "step 3750: train loss 1.7821, val loss 1.9107\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 3800: loss 1.4174, time 138.32ms, mfu 0.01%\n",
      "iter 3900: loss 1.2674, time 63.59ms, mfu 0.01%\n",
      "step 4000: train loss 1.7636, val loss 1.9091\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 4000: loss 1.3600, time 1074.88ms, mfu 0.01%\n",
      "iter 4100: loss 1.4847, time 101.43ms, mfu 0.01%\n",
      "iter 4200: loss 1.3177, time 143.44ms, mfu 0.01%\n",
      "step 4250: train loss 1.7233, val loss 1.8637\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 4300: loss 1.3854, time 120.70ms, mfu 0.01%\n",
      "iter 4400: loss 1.3948, time 72.02ms, mfu 0.01%\n",
      "step 4500: train loss 1.7313, val loss 1.8823\n",
      "iter 4500: loss 1.3361, time 1141.85ms, mfu 0.01%\n",
      "iter 4600: loss 1.4320, time 108.26ms, mfu 0.01%\n",
      "iter 4700: loss 1.2819, time 85.65ms, mfu 0.01%\n",
      "step 4750: train loss 1.7365, val loss 1.8569\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 4800: loss 1.3637, time 67.45ms, mfu 0.01%\n",
      "iter 4900: loss 1.4157, time 87.18ms, mfu 0.01%\n",
      "step 5000: train loss 1.7003, val loss 1.8417\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 5000: loss 1.2952, time 1223.25ms, mfu 0.01%\n",
      "iter 5100: loss 1.2951, time 72.47ms, mfu 0.01%\n",
      "iter 5200: loss 1.3218, time 81.22ms, mfu 0.01%\n",
      "step 5250: train loss 1.6793, val loss 1.8435\n",
      "iter 5300: loss 1.3604, time 129.27ms, mfu 0.01%\n",
      "iter 5400: loss 1.2276, time 101.63ms, mfu 0.01%\n",
      "step 5500: train loss 1.6901, val loss 1.8081\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 5500: loss 1.2542, time 1431.43ms, mfu 0.01%\n",
      "iter 5600: loss 1.2563, time 85.35ms, mfu 0.01%\n",
      "iter 5700: loss 1.2732, time 239.66ms, mfu 0.01%\n",
      "step 5750: train loss 1.6643, val loss 1.8088\n",
      "iter 5800: loss 1.3813, time 73.72ms, mfu 0.01%\n",
      "iter 5900: loss 1.3149, time 117.48ms, mfu 0.01%\n",
      "step 6000: train loss 1.6974, val loss 1.7824\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 6000: loss 1.2836, time 1214.08ms, mfu 0.01%\n",
      "iter 6100: loss 1.3987, time 121.50ms, mfu 0.01%\n",
      "iter 6200: loss 1.1563, time 191.59ms, mfu 0.01%\n",
      "step 6250: train loss 1.6647, val loss 1.7971\n",
      "iter 6300: loss 1.2311, time 81.63ms, mfu 0.01%\n",
      "iter 6400: loss 1.3173, time 124.29ms, mfu 0.01%\n",
      "step 6500: train loss 1.6304, val loss 1.7744\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 6500: loss 1.1909, time 1499.64ms, mfu 0.01%\n",
      "iter 6600: loss 1.1862, time 119.44ms, mfu 0.01%\n",
      "iter 6700: loss 1.1867, time 111.79ms, mfu 0.01%\n",
      "step 6750: train loss 1.6472, val loss 1.7847\n",
      "iter 6800: loss 1.2740, time 101.78ms, mfu 0.01%\n",
      "iter 6900: loss 1.3566, time 109.62ms, mfu 0.01%\n",
      "step 7000: train loss 1.6211, val loss 1.7777\n",
      "iter 7000: loss 1.2647, time 1440.44ms, mfu 0.01%\n",
      "iter 7100: loss 1.3664, time 128.22ms, mfu 0.01%\n",
      "iter 7200: loss 1.2720, time 119.94ms, mfu 0.01%\n",
      "step 7250: train loss 1.6403, val loss 1.7753\n",
      "iter 7300: loss 1.2008, time 60.67ms, mfu 0.01%\n",
      "iter 7400: loss 1.1882, time 60.19ms, mfu 0.01%\n",
      "step 7500: train loss 1.6347, val loss 1.7675\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0\n",
      "iter 7500: loss 1.2666, time 1353.89ms, mfu 0.01%\n",
      "iter 7600: loss 1.2659, time 131.16ms, mfu 0.01%\n",
      "iter 7700: loss 1.3115, time 147.40ms, mfu 0.01%\n",
      "step 7750: train loss 1.6314, val loss 1.8003\n",
      "iter 7800: loss 1.2161, time 107.60ms, mfu 0.01%\n",
      "iter 7900: loss 1.1980, time 139.01ms, mfu 0.01%\n",
      "step 8000: train loss 1.6444, val loss 1.7804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8000: loss 1.1782, time 1263.53ms, mfu 0.01%\n",
      "done for softminus_c=1.0, margin=2.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_2.0_5.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 5.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 5.0013, time 1838.55ms, mfu -100.00%\n",
      "iter 100: loss 2.5513, time 66.07ms, mfu 0.02%\n",
      "iter 200: loss 2.3054, time 67.89ms, mfu 0.02%\n",
      "step 250: train loss 3.3719, val loss 3.3835\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 300: loss 2.2352, time 66.82ms, mfu 0.02%\n",
      "iter 400: loss 2.2136, time 78.01ms, mfu 0.02%\n",
      "step 500: train loss 3.2014, val loss 3.2250\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 500: loss 2.0040, time 1288.77ms, mfu 0.02%\n",
      "iter 600: loss 1.9372, time 110.54ms, mfu 0.02%\n",
      "iter 700: loss 1.8857, time 121.81ms, mfu 0.02%\n",
      "step 750: train loss 3.0680, val loss 3.0804\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 800: loss 1.9390, time 76.23ms, mfu 0.02%\n",
      "iter 900: loss 1.7331, time 132.30ms, mfu 0.02%\n",
      "step 1000: train loss 2.9922, val loss 3.0243\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 1000: loss 1.8463, time 1224.15ms, mfu 0.01%\n",
      "iter 1100: loss 1.7519, time 83.98ms, mfu 0.01%\n",
      "iter 1200: loss 1.6952, time 89.35ms, mfu 0.01%\n",
      "step 1250: train loss 2.9181, val loss 2.9851\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 1300: loss 1.6837, time 100.47ms, mfu 0.01%\n",
      "iter 1400: loss 1.6372, time 74.73ms, mfu 0.01%\n",
      "step 1500: train loss 2.8835, val loss 2.9310\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 1500: loss 1.5936, time 1308.60ms, mfu 0.01%\n",
      "iter 1600: loss 1.6105, time 98.22ms, mfu 0.01%\n",
      "iter 1700: loss 1.5522, time 201.12ms, mfu 0.01%\n",
      "step 1750: train loss 2.8467, val loss 2.9034\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 1800: loss 1.5993, time 133.85ms, mfu 0.01%\n",
      "iter 1900: loss 1.6019, time 117.11ms, mfu 0.01%\n",
      "step 2000: train loss 2.8133, val loss 2.8975\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 2000: loss 1.4663, time 1197.19ms, mfu 0.01%\n",
      "iter 2100: loss 1.5485, time 96.12ms, mfu 0.01%\n",
      "iter 2200: loss 1.6171, time 105.05ms, mfu 0.01%\n",
      "step 2250: train loss 2.8243, val loss 2.8854\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 2300: loss 1.5777, time 79.41ms, mfu 0.01%\n",
      "iter 2400: loss 1.5497, time 124.60ms, mfu 0.01%\n",
      "step 2500: train loss 2.7973, val loss 2.8386\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 2500: loss 1.5071, time 1211.80ms, mfu 0.01%\n",
      "iter 2600: loss 1.5158, time 104.06ms, mfu 0.01%\n",
      "iter 2700: loss 1.4062, time 133.71ms, mfu 0.01%\n",
      "step 2750: train loss 2.7877, val loss 2.8575\n",
      "iter 2800: loss 1.5652, time 88.05ms, mfu 0.01%\n",
      "iter 2900: loss 1.5069, time 71.51ms, mfu 0.01%\n",
      "step 3000: train loss 2.7428, val loss 2.7954\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 3000: loss 1.6071, time 1476.84ms, mfu 0.01%\n",
      "iter 3100: loss 1.6024, time 191.88ms, mfu 0.01%\n",
      "iter 3200: loss 1.5468, time 107.64ms, mfu 0.01%\n",
      "step 3250: train loss 2.7157, val loss 2.8079\n",
      "iter 3300: loss 1.5118, time 154.53ms, mfu 0.01%\n",
      "iter 3400: loss 1.5415, time 74.03ms, mfu 0.01%\n",
      "step 3500: train loss 2.7184, val loss 2.7875\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 3500: loss 1.6505, time 1396.39ms, mfu 0.01%\n",
      "iter 3600: loss 1.4753, time 216.06ms, mfu 0.01%\n",
      "iter 3700: loss 1.4838, time 99.36ms, mfu 0.01%\n",
      "step 3750: train loss 2.7077, val loss 2.7750\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 3800: loss 1.4302, time 146.16ms, mfu 0.01%\n",
      "iter 3900: loss 1.3184, time 136.92ms, mfu 0.01%\n",
      "step 4000: train loss 2.6915, val loss 2.7787\n",
      "iter 4000: loss 1.4176, time 1417.78ms, mfu 0.01%\n",
      "iter 4100: loss 1.5401, time 124.57ms, mfu 0.01%\n",
      "iter 4200: loss 1.3589, time 167.78ms, mfu 0.01%\n",
      "step 4250: train loss 2.6671, val loss 2.7577\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 4300: loss 1.4241, time 122.67ms, mfu 0.01%\n",
      "iter 4400: loss 1.4345, time 71.09ms, mfu 0.01%\n",
      "step 4500: train loss 2.6630, val loss 2.7480\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 4500: loss 1.3549, time 1403.46ms, mfu 0.01%\n",
      "iter 4600: loss 1.4718, time 104.25ms, mfu 0.01%\n",
      "iter 4700: loss 1.3527, time 68.08ms, mfu 0.01%\n",
      "step 4750: train loss 2.6728, val loss 2.7454\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 4800: loss 1.3711, time 85.90ms, mfu 0.01%\n",
      "iter 4900: loss 1.4375, time 66.47ms, mfu 0.01%\n",
      "step 5000: train loss 2.6527, val loss 2.7297\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 5000: loss 1.3308, time 1485.73ms, mfu 0.01%\n",
      "iter 5100: loss 1.3153, time 70.43ms, mfu 0.01%\n",
      "iter 5200: loss 1.3252, time 117.76ms, mfu 0.01%\n",
      "step 5250: train loss 2.6260, val loss 2.7289\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 5300: loss 1.3764, time 135.66ms, mfu 0.01%\n",
      "iter 5400: loss 1.2672, time 70.92ms, mfu 0.01%\n",
      "step 5500: train loss 2.6437, val loss 2.7108\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 5500: loss 1.2824, time 1580.34ms, mfu 0.01%\n",
      "iter 5600: loss 1.2997, time 99.65ms, mfu 0.01%\n",
      "iter 5700: loss 1.3289, time 111.25ms, mfu 0.01%\n",
      "step 5750: train loss 2.6253, val loss 2.7113\n",
      "iter 5800: loss 1.4127, time 117.84ms, mfu 0.01%\n",
      "iter 5900: loss 1.3831, time 79.74ms, mfu 0.01%\n",
      "step 6000: train loss 2.6431, val loss 2.6896\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 6000: loss 1.3219, time 1199.48ms, mfu 0.01%\n",
      "iter 6100: loss 1.4221, time 153.77ms, mfu 0.01%\n",
      "iter 6200: loss 1.2036, time 83.67ms, mfu 0.01%\n",
      "step 6250: train loss 2.6216, val loss 2.7012\n",
      "iter 6300: loss 1.2665, time 69.44ms, mfu 0.01%\n",
      "iter 6400: loss 1.3544, time 127.04ms, mfu 0.01%\n",
      "step 6500: train loss 2.5900, val loss 2.6786\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 6500: loss 1.2438, time 1268.09ms, mfu 0.01%\n",
      "iter 6600: loss 1.2258, time 171.35ms, mfu 0.01%\n",
      "iter 6700: loss 1.2359, time 82.56ms, mfu 0.01%\n",
      "step 6750: train loss 2.6070, val loss 2.6861\n",
      "iter 6800: loss 1.3128, time 106.75ms, mfu 0.01%\n",
      "iter 6900: loss 1.3984, time 124.42ms, mfu 0.01%\n",
      "step 7000: train loss 2.5877, val loss 2.6854\n",
      "iter 7000: loss 1.3024, time 1331.02ms, mfu 0.01%\n",
      "iter 7100: loss 1.4207, time 60.06ms, mfu 0.01%\n",
      "iter 7200: loss 1.2836, time 102.01ms, mfu 0.01%\n",
      "step 7250: train loss 2.5962, val loss 2.6791\n",
      "iter 7300: loss 1.2645, time 98.96ms, mfu 0.01%\n",
      "iter 7400: loss 1.1988, time 87.39ms, mfu 0.01%\n",
      "step 7500: train loss 2.5939, val loss 2.6741\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 7500: loss 1.3107, time 1336.17ms, mfu 0.01%\n",
      "iter 7600: loss 1.2843, time 103.98ms, mfu 0.01%\n",
      "iter 7700: loss 1.3557, time 89.34ms, mfu 0.01%\n",
      "step 7750: train loss 2.5875, val loss 2.6891\n",
      "iter 7800: loss 1.2626, time 110.19ms, mfu 0.01%\n",
      "iter 7900: loss 1.2642, time 106.42ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000: train loss 2.5912, val loss 2.6705\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0\n",
      "iter 8000: loss 1.2005, time 2032.09ms, mfu 0.01%\n",
      "done for softminus_c=5.0, margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.0, 2.0]:\n",
    "    for softminus_c in [1.0, 5.0]:\n",
    "        args = {\n",
    "            \"out_dir\": f\"out/softminus_margin_{margin}_{softminus_c}\",\n",
    "            \"model_type\": \"proposed\",\n",
    "            \"margin\": margin,\n",
    "            \"softminus\": \"True\",\n",
    "            \"softminus_c\": softminus_c\n",
    "        }\n",
    "        run_training(args)\n",
    "        print(f'done for softminus_c={softminus_c}, margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac467f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_1.0_1.0_threshold_detached\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 1.0\n",
      "Overriding: detach_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8114, time 832.62ms, mfu -100.00%\n",
      "iter 100: loss 2.8732, time 31.62ms, mfu 0.04%\n",
      "iter 200: loss 2.8165, time 75.15ms, mfu 0.04%\n",
      "step 250: train loss 3.9915, val loss 3.9944\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 300: loss 2.8425, time 73.25ms, mfu 0.04%\n",
      "iter 400: loss 2.8838, time 75.05ms, mfu 0.03%\n",
      "step 500: train loss 3.9558, val loss 3.9670\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 500: loss 2.6983, time 1558.45ms, mfu 0.03%\n",
      "iter 600: loss 2.7748, time 89.84ms, mfu 0.03%\n",
      "iter 700: loss 2.7187, time 79.96ms, mfu 0.03%\n",
      "step 750: train loss 3.9905, val loss 3.9839\n",
      "iter 800: loss 2.9077, time 63.30ms, mfu 0.03%\n",
      "iter 900: loss 2.7184, time 37.71ms, mfu 0.03%\n",
      "step 1000: train loss 3.9477, val loss 3.9502\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 1000: loss 2.8485, time 947.19ms, mfu 0.03%\n",
      "iter 1100: loss 2.7696, time 143.50ms, mfu 0.02%\n",
      "iter 1200: loss 2.6975, time 81.40ms, mfu 0.02%\n",
      "step 1250: train loss 3.9055, val loss 3.9139\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 1300: loss 2.7469, time 179.33ms, mfu 0.02%\n",
      "iter 1400: loss 2.6902, time 80.75ms, mfu 0.02%\n",
      "step 1500: train loss 3.9379, val loss 3.9454\n",
      "iter 1500: loss 2.7521, time 563.68ms, mfu 0.02%\n",
      "iter 1600: loss 2.8928, time 51.67ms, mfu 0.02%\n",
      "iter 1700: loss 2.6771, time 55.57ms, mfu 0.02%\n",
      "step 1750: train loss 3.9070, val loss 3.9261\n",
      "iter 1800: loss 2.7120, time 43.82ms, mfu 0.02%\n",
      "iter 1900: loss 2.6853, time 34.89ms, mfu 0.02%\n",
      "step 2000: train loss 3.9082, val loss 3.9220\n",
      "iter 2000: loss 2.6976, time 611.72ms, mfu 0.02%\n",
      "iter 2100: loss 2.5934, time 34.88ms, mfu 0.02%\n",
      "iter 2200: loss 2.7642, time 56.27ms, mfu 0.02%\n",
      "step 2250: train loss 3.9243, val loss 3.9281\n",
      "iter 2300: loss 2.7761, time 49.36ms, mfu 0.02%\n",
      "iter 2400: loss 2.7013, time 35.40ms, mfu 0.02%\n",
      "step 2500: train loss 3.9132, val loss 3.9040\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 2500: loss 2.6887, time 779.68ms, mfu 0.02%\n",
      "iter 2600: loss 2.6646, time 145.84ms, mfu 0.02%\n",
      "iter 2700: loss 2.7061, time 136.80ms, mfu 0.02%\n",
      "step 2750: train loss 3.8760, val loss 3.8737\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 2800: loss 2.6370, time 56.29ms, mfu 0.02%\n",
      "iter 2900: loss 2.6592, time 65.00ms, mfu 0.02%\n",
      "step 3000: train loss 3.8843, val loss 3.8940\n",
      "iter 3000: loss 3.0553, time 782.93ms, mfu 0.02%\n",
      "iter 3100: loss 2.8050, time 383.08ms, mfu 0.02%\n",
      "iter 3200: loss 2.6857, time 41.20ms, mfu 0.02%\n",
      "step 3250: train loss 3.8579, val loss 3.8636\n",
      "saving checkpoint to out/softminus_margin_1.0_1.0_threshold_detached\n",
      "iter 3300: loss 2.7534, time 41.21ms, mfu 0.02%\n",
      "iter 3400: loss 2.6253, time 42.71ms, mfu 0.02%\n",
      "step 3500: train loss 3.8883, val loss 3.8851\n",
      "iter 3500: loss 2.7493, time 573.72ms, mfu 0.02%\n",
      "iter 3600: loss 2.6651, time 199.32ms, mfu 0.02%\n",
      "iter 3700: loss 2.5954, time 65.09ms, mfu 0.02%\n",
      "step 3750: train loss 3.9056, val loss 3.8993\n",
      "iter 3800: loss 2.6150, time 53.52ms, mfu 0.02%\n",
      "iter 3900: loss 5.3179, time 52.12ms, mfu 0.02%\n",
      "step 4000: train loss 4.7951, val loss 4.8016\n",
      "iter 4000: loss 4.6144, time 1086.31ms, mfu 0.02%\n",
      "iter 4100: loss 2.9720, time 250.23ms, mfu 0.02%\n",
      "iter 4200: loss 2.9551, time 159.99ms, mfu 0.02%\n",
      "step 4250: train loss 3.9874, val loss 4.0249\n",
      "iter 4300: loss 3.1632, time 152.31ms, mfu 0.01%\n",
      "iter 4400: loss 2.9039, time 44.57ms, mfu 0.02%\n",
      "step 4500: train loss 3.9385, val loss 3.9402\n",
      "iter 4500: loss 2.9109, time 845.13ms, mfu 0.01%\n",
      "iter 4600: loss 2.9626, time 161.00ms, mfu 0.01%\n",
      "iter 4700: loss 3.2013, time 45.55ms, mfu 0.02%\n",
      "step 4750: train loss 3.9048, val loss 3.9204\n",
      "iter 4800: loss 2.9894, time 63.91ms, mfu 0.02%\n",
      "iter 4900: loss 3.0233, time 115.12ms, mfu 0.02%\n",
      "step 5000: train loss 3.9051, val loss 3.9275\n",
      "iter 5000: loss 3.0143, time 996.41ms, mfu 0.01%\n",
      "iter 5100: loss 2.9401, time 87.31ms, mfu 0.01%\n",
      "iter 5200: loss 2.8784, time 620.25ms, mfu 0.01%\n",
      "step 5250: train loss 3.9245, val loss 3.9351\n",
      "iter 5300: loss 2.8440, time 68.19ms, mfu 0.01%\n",
      "iter 5400: loss 2.7227, time 251.16ms, mfu 0.01%\n",
      "step 5500: train loss 3.8981, val loss 3.9133\n",
      "iter 5500: loss 2.8448, time 1197.08ms, mfu 0.01%\n",
      "iter 5600: loss 2.9938, time 151.82ms, mfu 0.01%\n",
      "iter 5700: loss 2.7908, time 102.34ms, mfu 0.01%\n",
      "step 5750: train loss 3.9198, val loss 3.9350\n",
      "iter 5800: loss 2.7790, time 104.54ms, mfu 0.01%\n",
      "iter 5900: loss 2.7551, time 91.04ms, mfu 0.01%\n",
      "step 6000: train loss 3.9160, val loss 3.9071\n",
      "iter 6000: loss 2.8280, time 2743.03ms, mfu 0.01%\n",
      "iter 6100: loss 2.8050, time 79.72ms, mfu 0.01%\n",
      "iter 6200: loss 2.7812, time 55.03ms, mfu 0.01%\n",
      "step 6250: train loss 3.9327, val loss 3.9399\n",
      "iter 6300: loss 2.8204, time 63.46ms, mfu 0.01%\n",
      "iter 6400: loss 2.7530, time 83.29ms, mfu 0.01%\n",
      "step 6500: train loss 3.9129, val loss 3.9156\n",
      "iter 6500: loss 2.6685, time 973.18ms, mfu 0.01%\n",
      "iter 6600: loss 2.7116, time 160.50ms, mfu 0.01%\n",
      "iter 6700: loss 2.7125, time 62.67ms, mfu 0.01%\n",
      "step 6750: train loss 3.9245, val loss 3.9303\n",
      "iter 6800: loss 2.6898, time 58.20ms, mfu 0.01%\n",
      "iter 6900: loss 2.7316, time 158.41ms, mfu 0.01%\n",
      "step 7000: train loss 3.9239, val loss 3.9255\n",
      "iter 7000: loss 2.7758, time 2258.41ms, mfu 0.01%\n",
      "iter 7100: loss 2.7723, time 181.11ms, mfu 0.01%\n",
      "iter 7200: loss 2.8994, time 135.63ms, mfu 0.01%\n",
      "step 7250: train loss 3.9407, val loss 3.9360\n",
      "iter 7300: loss 2.7342, time 135.63ms, mfu 0.01%\n",
      "iter 7400: loss 2.6959, time 101.50ms, mfu 0.01%\n",
      "step 7500: train loss 3.9121, val loss 3.9362\n",
      "iter 7500: loss 2.8359, time 1046.15ms, mfu 0.01%\n",
      "iter 7600: loss 2.8360, time 562.60ms, mfu 0.01%\n",
      "iter 7700: loss 2.8164, time 81.40ms, mfu 0.01%\n",
      "step 7750: train loss 3.9428, val loss 3.9472\n",
      "iter 7800: loss 2.7751, time 124.74ms, mfu 0.01%\n",
      "iter 7900: loss 2.6300, time 101.16ms, mfu 0.01%\n",
      "step 8000: train loss 3.9423, val loss 3.9437\n",
      "iter 8000: loss 2.7171, time 1328.75ms, mfu 0.01%\n",
      "done for softminus_c=1.0, margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_1.0_5.0_threshold_detached\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 5.0\n",
      "Overriding: detach_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 5.4550, time 2811.93ms, mfu -100.00%\n",
      "iter 100: loss 2.5614, time 236.29ms, mfu 0.01%\n",
      "iter 200: loss 2.3029, time 70.01ms, mfu 0.01%\n",
      "step 250: train loss 4.1766, val loss 4.1840\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 300: loss 2.3721, time 60.67ms, mfu 0.01%\n",
      "iter 400: loss 2.3988, time 58.77ms, mfu 0.01%\n",
      "step 500: train loss 3.9895, val loss 3.9968\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 500: loss 2.5213, time 2248.31ms, mfu 0.01%\n",
      "iter 600: loss 2.4599, time 63.13ms, mfu 0.01%\n",
      "iter 700: loss 2.1359, time 117.24ms, mfu 0.01%\n",
      "step 750: train loss 3.9011, val loss 3.8924\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 800: loss 2.4195, time 343.38ms, mfu 0.01%\n",
      "iter 900: loss 2.4307, time 194.08ms, mfu 0.01%\n",
      "step 1000: train loss 3.8720, val loss 3.8759\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 1000: loss 2.3659, time 1082.09ms, mfu 0.01%\n",
      "iter 1100: loss 2.9259, time 71.34ms, mfu 0.01%\n",
      "iter 1200: loss 2.2596, time 60.00ms, mfu 0.01%\n",
      "step 1250: train loss 3.6894, val loss 3.7042\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 1300: loss 2.2739, time 113.14ms, mfu 0.01%\n",
      "iter 1400: loss 2.3249, time 74.63ms, mfu 0.01%\n",
      "step 1500: train loss 3.7096, val loss 3.7148\n",
      "iter 1500: loss 2.3371, time 896.99ms, mfu 0.01%\n",
      "iter 1600: loss 2.3670, time 118.79ms, mfu 0.01%\n",
      "iter 1700: loss 2.5174, time 101.82ms, mfu 0.01%\n",
      "step 1750: train loss 3.9453, val loss 3.9537\n",
      "iter 1800: loss 3.1297, time 292.51ms, mfu 0.01%\n",
      "iter 1900: loss 2.2144, time 91.80ms, mfu 0.01%\n",
      "step 2000: train loss 3.8409, val loss 3.8527\n",
      "iter 2000: loss 2.0879, time 3725.00ms, mfu 0.01%\n",
      "iter 2100: loss 2.1699, time 65.18ms, mfu 0.01%\n",
      "iter 2200: loss 2.3110, time 83.82ms, mfu 0.01%\n",
      "step 2250: train loss 3.9018, val loss 3.9019\n",
      "iter 2300: loss 2.3423, time 60.73ms, mfu 0.01%\n",
      "iter 2400: loss 2.5541, time 110.16ms, mfu 0.01%\n",
      "step 2500: train loss 4.1840, val loss 4.1795\n",
      "iter 2500: loss 2.5206, time 1998.07ms, mfu 0.01%\n",
      "iter 2600: loss 2.1041, time 61.90ms, mfu 0.01%\n",
      "iter 2700: loss 2.0843, time 92.80ms, mfu 0.01%\n",
      "step 2750: train loss 3.8197, val loss 3.8157\n",
      "iter 2800: loss 2.2979, time 122.24ms, mfu 0.01%\n",
      "iter 2900: loss 2.3005, time 61.36ms, mfu 0.01%\n",
      "step 3000: train loss 3.5956, val loss 3.5875\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 3000: loss 2.6788, time 2091.46ms, mfu 0.01%\n",
      "iter 3100: loss 2.2975, time 116.63ms, mfu 0.01%\n",
      "iter 3200: loss 2.6125, time 150.51ms, mfu 0.01%\n",
      "step 3250: train loss 3.8896, val loss 3.9080\n",
      "iter 3300: loss 2.7682, time 63.48ms, mfu 0.01%\n",
      "iter 3400: loss 2.2095, time 78.04ms, mfu 0.01%\n",
      "step 3500: train loss 3.8464, val loss 3.8415\n",
      "iter 3500: loss 2.1729, time 1069.45ms, mfu 0.01%\n",
      "iter 3600: loss 2.7770, time 135.39ms, mfu 0.01%\n",
      "iter 3700: loss 2.0673, time 92.74ms, mfu 0.01%\n",
      "step 3750: train loss 3.5431, val loss 3.5276\n",
      "saving checkpoint to out/softminus_margin_1.0_5.0_threshold_detached\n",
      "iter 3800: loss 2.0876, time 139.21ms, mfu 0.01%\n",
      "iter 3900: loss 2.5455, time 64.78ms, mfu 0.01%\n",
      "step 4000: train loss 4.0203, val loss 4.0245\n",
      "iter 4000: loss 2.1165, time 1157.78ms, mfu 0.01%\n",
      "iter 4100: loss 2.3828, time 96.67ms, mfu 0.01%\n",
      "iter 4200: loss 2.1743, time 58.36ms, mfu 0.01%\n",
      "step 4250: train loss 3.8817, val loss 3.9166\n",
      "iter 4300: loss 2.6791, time 205.38ms, mfu 0.01%\n",
      "iter 4400: loss 1.9784, time 57.37ms, mfu 0.01%\n",
      "step 4500: train loss 3.6124, val loss 3.6080\n",
      "iter 4500: loss 2.1067, time 5405.51ms, mfu 0.01%\n",
      "iter 4600: loss 2.2449, time 179.54ms, mfu 0.01%\n",
      "iter 4700: loss 2.1584, time 277.31ms, mfu 0.01%\n",
      "step 4750: train loss 3.7177, val loss 3.7073\n",
      "iter 4800: loss 2.0814, time 80.36ms, mfu 0.01%\n",
      "iter 4900: loss 2.2554, time 96.58ms, mfu 0.01%\n",
      "step 5000: train loss 3.9222, val loss 3.9141\n",
      "iter 5000: loss 2.2841, time 965.50ms, mfu 0.01%\n",
      "iter 5100: loss 2.1912, time 324.27ms, mfu 0.01%\n",
      "iter 5200: loss 2.2159, time 612.83ms, mfu 0.01%\n",
      "step 5250: train loss 3.9208, val loss 3.9204\n",
      "iter 5300: loss 2.2816, time 93.06ms, mfu 0.01%\n",
      "iter 5400: loss 2.1723, time 371.07ms, mfu 0.01%\n",
      "step 5500: train loss 3.8276, val loss 3.8237\n",
      "iter 5500: loss 2.2075, time 3116.79ms, mfu 0.01%\n",
      "iter 5600: loss 2.4425, time 68.99ms, mfu 0.01%\n",
      "iter 5700: loss 2.2085, time 54.23ms, mfu 0.01%\n",
      "step 5750: train loss 3.9949, val loss 3.9994\n",
      "iter 5800: loss 2.2920, time 174.13ms, mfu 0.01%\n",
      "iter 5900: loss 2.2376, time 120.10ms, mfu 0.01%\n",
      "step 6000: train loss 3.8724, val loss 3.8613\n",
      "iter 6000: loss 2.2640, time 1109.78ms, mfu 0.01%\n",
      "iter 6100: loss 2.3470, time 109.55ms, mfu 0.01%\n",
      "iter 6200: loss 2.2936, time 624.81ms, mfu 0.01%\n",
      "step 6250: train loss 4.0083, val loss 4.0103\n",
      "iter 6300: loss 2.2598, time 96.70ms, mfu 0.01%\n",
      "iter 6400: loss 2.3596, time 51.06ms, mfu 0.01%\n",
      "step 6500: train loss 4.0123, val loss 4.0150\n",
      "iter 6500: loss 2.2320, time 955.28ms, mfu 0.01%\n",
      "iter 6600: loss 2.2057, time 130.96ms, mfu 0.01%\n",
      "iter 6700: loss 2.2998, time 100.82ms, mfu 0.01%\n",
      "step 6750: train loss 3.8036, val loss 3.7983\n",
      "iter 6800: loss 2.2106, time 163.25ms, mfu 0.01%\n",
      "iter 6900: loss 2.2560, time 74.05ms, mfu 0.01%\n",
      "step 7000: train loss 3.9875, val loss 3.9773\n",
      "iter 7000: loss 2.3839, time 1026.50ms, mfu 0.01%\n",
      "iter 7100: loss 2.3414, time 81.67ms, mfu 0.01%\n",
      "iter 7200: loss 2.4303, time 89.88ms, mfu 0.01%\n",
      "step 7250: train loss 3.8532, val loss 3.8394\n",
      "iter 7300: loss 2.2523, time 100.15ms, mfu 0.01%\n",
      "iter 7400: loss 2.2398, time 92.74ms, mfu 0.01%\n",
      "step 7500: train loss 3.8597, val loss 3.8602\n",
      "iter 7500: loss 2.3009, time 1525.39ms, mfu 0.01%\n",
      "iter 7600: loss 2.3205, time 60.67ms, mfu 0.01%\n",
      "iter 7700: loss 2.3228, time 52.53ms, mfu 0.01%\n",
      "step 7750: train loss 3.9200, val loss 3.9202\n",
      "iter 7800: loss 2.3963, time 82.00ms, mfu 0.01%\n",
      "iter 7900: loss 2.2625, time 266.28ms, mfu 0.01%\n",
      "step 8000: train loss 3.9314, val loss 3.9242\n",
      "iter 8000: loss 2.2979, time 980.22ms, mfu 0.01%\n",
      "done for softminus_c=5.0, margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_2.0_1.0_threshold_detached\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 1.0\n",
      "Overriding: detach_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8597, time 1903.75ms, mfu -100.00%\n",
      "iter 100: loss 2.6669, time 89.88ms, mfu 0.01%\n",
      "iter 200: loss 2.5527, time 51.29ms, mfu 0.02%\n",
      "step 250: train loss 3.2334, val loss 3.2420\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0_threshold_detached\n",
      "iter 300: loss 2.6198, time 76.02ms, mfu 0.02%\n",
      "iter 400: loss 2.5688, time 97.88ms, mfu 0.02%\n",
      "step 500: train loss 3.1554, val loss 3.1730\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0_threshold_detached\n",
      "iter 500: loss 2.4095, time 1440.25ms, mfu 0.01%\n",
      "iter 600: loss 2.4583, time 117.54ms, mfu 0.01%\n",
      "iter 700: loss 2.3795, time 50.86ms, mfu 0.01%\n",
      "step 750: train loss 3.1258, val loss 3.1103\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0_threshold_detached\n",
      "iter 800: loss 2.5566, time 57.56ms, mfu 0.02%\n",
      "iter 900: loss 2.4693, time 95.13ms, mfu 0.02%\n",
      "step 1000: train loss 3.0743, val loss 3.0720\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0_threshold_detached\n",
      "iter 1000: loss 2.4219, time 4629.22ms, mfu 0.01%\n",
      "iter 1100: loss 2.3809, time 96.54ms, mfu 0.01%\n",
      "iter 1200: loss 2.3915, time 102.67ms, mfu 0.01%\n",
      "step 1250: train loss 3.0873, val loss 3.1014\n",
      "iter 1300: loss 2.3616, time 95.61ms, mfu 0.01%\n",
      "iter 1400: loss 2.2768, time 78.71ms, mfu 0.01%\n",
      "step 1500: train loss 3.1194, val loss 3.1154\n",
      "iter 1500: loss 2.3570, time 1292.64ms, mfu 0.01%\n",
      "iter 1600: loss 2.4543, time 78.18ms, mfu 0.01%\n",
      "iter 1700: loss 2.2369, time 130.94ms, mfu 0.01%\n",
      "step 1750: train loss 3.0635, val loss 3.0777\n",
      "iter 1800: loss 2.2143, time 1019.07ms, mfu 0.01%\n",
      "iter 1900: loss 2.2730, time 66.33ms, mfu 0.01%\n",
      "step 2000: train loss 3.0395, val loss 3.0760\n",
      "iter 2000: loss 2.1725, time 953.21ms, mfu 0.01%\n",
      "iter 2100: loss 2.1872, time 126.88ms, mfu 0.01%\n",
      "iter 2200: loss 2.2547, time 103.74ms, mfu 0.01%\n",
      "step 2250: train loss 3.0914, val loss 3.1050\n",
      "iter 2300: loss 2.3195, time 50.66ms, mfu 0.01%\n",
      "iter 2400: loss 2.2547, time 48.81ms, mfu 0.01%\n",
      "step 2500: train loss 3.0603, val loss 3.0530\n",
      "saving checkpoint to out/softminus_margin_2.0_1.0_threshold_detached\n",
      "iter 2500: loss 2.2209, time 1113.13ms, mfu 0.01%\n",
      "iter 2600: loss 2.1874, time 83.34ms, mfu 0.01%\n",
      "iter 2700: loss 2.2453, time 52.84ms, mfu 0.01%\n",
      "step 2750: train loss 3.1481, val loss 3.1533\n",
      "iter 2800: loss 2.2177, time 88.64ms, mfu 0.01%\n",
      "iter 2900: loss 2.2887, time 113.31ms, mfu 0.01%\n",
      "step 3000: train loss 3.0843, val loss 3.0931\n",
      "iter 3000: loss 2.3161, time 1239.26ms, mfu 0.01%\n",
      "iter 3100: loss 2.3513, time 79.10ms, mfu 0.01%\n",
      "iter 3200: loss 2.2137, time 209.09ms, mfu 0.01%\n",
      "step 3250: train loss 3.0739, val loss 3.1038\n",
      "iter 3300: loss 2.3235, time 99.77ms, mfu 0.01%\n",
      "iter 3400: loss 2.2487, time 95.57ms, mfu 0.01%\n",
      "step 3500: train loss 3.1174, val loss 3.1399\n",
      "iter 3500: loss 2.3427, time 1049.67ms, mfu 0.01%\n",
      "iter 3600: loss 2.2363, time 47.73ms, mfu 0.01%\n",
      "iter 3700: loss 2.1940, time 124.07ms, mfu 0.01%\n",
      "step 3750: train loss 3.0804, val loss 3.0971\n",
      "iter 3800: loss 2.1999, time 57.14ms, mfu 0.01%\n",
      "iter 3900: loss 2.1855, time 109.23ms, mfu 0.01%\n",
      "step 4000: train loss 3.0879, val loss 3.1121\n",
      "iter 4000: loss 2.2303, time 3580.00ms, mfu 0.01%\n",
      "iter 4100: loss 2.3078, time 59.39ms, mfu 0.01%\n",
      "iter 4200: loss 2.1462, time 118.83ms, mfu 0.01%\n",
      "step 4250: train loss 3.0982, val loss 3.1458\n",
      "iter 4300: loss 2.2535, time 124.58ms, mfu 0.01%\n",
      "iter 4400: loss 2.1975, time 66.34ms, mfu 0.01%\n",
      "step 4500: train loss 3.0992, val loss 3.1240\n",
      "iter 4500: loss 2.2069, time 1813.99ms, mfu 0.01%\n",
      "iter 4600: loss 2.2420, time 100.47ms, mfu 0.01%\n",
      "iter 4700: loss 2.2119, time 97.01ms, mfu 0.01%\n",
      "step 4750: train loss 3.0832, val loss 3.1167\n",
      "iter 4800: loss 2.2259, time 95.19ms, mfu 0.01%\n",
      "iter 4900: loss 2.2601, time 119.55ms, mfu 0.01%\n",
      "step 5000: train loss 3.0898, val loss 3.1311\n",
      "iter 5000: loss 2.1929, time 1404.00ms, mfu 0.01%\n",
      "iter 5100: loss 2.1271, time 47.89ms, mfu 0.01%\n",
      "iter 5200: loss 2.1878, time 109.14ms, mfu 0.01%\n",
      "step 5250: train loss 3.0943, val loss 3.1468\n",
      "iter 5300: loss 2.1949, time 78.39ms, mfu 0.01%\n",
      "iter 5400: loss 2.0593, time 57.78ms, mfu 0.01%\n",
      "step 5500: train loss 3.0853, val loss 3.1264\n",
      "iter 5500: loss 2.1033, time 1168.54ms, mfu 0.01%\n",
      "iter 5600: loss 2.2745, time 50.53ms, mfu 0.01%\n",
      "iter 5700: loss 2.1735, time 50.27ms, mfu 0.02%\n",
      "step 5750: train loss 3.1028, val loss 3.1520\n",
      "iter 5800: loss 2.2338, time 96.56ms, mfu 0.01%\n",
      "iter 5900: loss 2.1983, time 59.73ms, mfu 0.02%\n",
      "step 6000: train loss 3.1189, val loss 3.1249\n",
      "iter 6000: loss 2.2377, time 1146.77ms, mfu 0.01%\n",
      "iter 6100: loss 2.2387, time 136.42ms, mfu 0.01%\n",
      "iter 6200: loss 2.1162, time 94.52ms, mfu 0.01%\n",
      "step 6250: train loss 3.1054, val loss 3.1431\n",
      "iter 6300: loss 2.1812, time 70.40ms, mfu 0.01%\n",
      "iter 6400: loss 2.2161, time 99.00ms, mfu 0.01%\n",
      "step 6500: train loss 3.7168, val loss 3.7641\n",
      "iter 6500: loss 3.3510, time 998.37ms, mfu 0.01%\n",
      "iter 6600: loss 3.0026, time 91.59ms, mfu 0.01%\n",
      "iter 6700: loss 2.3564, time 98.51ms, mfu 0.01%\n",
      "step 6750: train loss 3.0882, val loss 3.1206\n",
      "iter 6800: loss 2.1250, time 51.74ms, mfu 0.01%\n",
      "iter 6900: loss 2.2566, time 59.23ms, mfu 0.01%\n",
      "step 7000: train loss 3.1302, val loss 3.1682\n",
      "iter 7000: loss 2.2429, time 1199.91ms, mfu 0.01%\n",
      "iter 7100: loss 2.2847, time 59.91ms, mfu 0.01%\n",
      "iter 7200: loss 2.3058, time 106.93ms, mfu 0.01%\n",
      "step 7250: train loss 3.1593, val loss 3.1871\n",
      "iter 7300: loss 2.1568, time 94.21ms, mfu 0.01%\n",
      "iter 7400: loss 2.1753, time 138.33ms, mfu 0.01%\n",
      "step 7500: train loss 3.1312, val loss 3.1912\n",
      "iter 7500: loss 2.2960, time 1104.23ms, mfu 0.01%\n",
      "iter 7600: loss 2.2787, time 82.18ms, mfu 0.01%\n",
      "iter 7700: loss 2.2481, time 76.88ms, mfu 0.01%\n",
      "step 7750: train loss 3.1586, val loss 3.1977\n",
      "iter 7800: loss 2.1990, time 91.30ms, mfu 0.01%\n",
      "iter 7900: loss 2.1341, time 121.53ms, mfu 0.01%\n",
      "step 8000: train loss 3.1695, val loss 3.1987\n",
      "iter 8000: loss 2.1498, time 1139.84ms, mfu 0.01%\n",
      "done for softminus_c=1.0, margin=2.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/softminus_margin_2.0_5.0_threshold_detached\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: softminus = True\n",
      "Overriding: softminus_c = 5.0\n",
      "Overriding: detach_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 5.0013, time 1687.54ms, mfu -100.00%\n",
      "iter 100: loss 2.5633, time 97.33ms, mfu 0.01%\n",
      "iter 200: loss 2.3179, time 66.57ms, mfu 0.01%\n",
      "step 250: train loss 3.6007, val loss 3.6088\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 300: loss 2.2636, time 138.77ms, mfu 0.01%\n",
      "iter 400: loss 2.2520, time 61.02ms, mfu 0.01%\n",
      "step 500: train loss 3.5090, val loss 3.5247\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 500: loss 2.1327, time 1013.00ms, mfu 0.01%\n",
      "iter 600: loss 2.0582, time 95.17ms, mfu 0.01%\n",
      "iter 700: loss 2.5116, time 89.00ms, mfu 0.01%\n",
      "step 750: train loss 3.3361, val loss 3.3315\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 800: loss 2.9160, time 95.81ms, mfu 0.01%\n",
      "iter 900: loss 2.5690, time 50.34ms, mfu 0.01%\n",
      "step 1000: train loss 3.4174, val loss 3.4212\n",
      "iter 1000: loss 2.2182, time 1059.40ms, mfu 0.01%\n",
      "iter 1100: loss 2.0493, time 64.82ms, mfu 0.01%\n",
      "iter 1200: loss 1.9905, time 361.79ms, mfu 0.01%\n",
      "step 1250: train loss 3.0098, val loss 3.0416\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 1300: loss 2.2405, time 91.30ms, mfu 0.01%\n",
      "iter 1400: loss 2.4229, time 120.69ms, mfu 0.01%\n",
      "step 1500: train loss 3.1865, val loss 3.1928\n",
      "iter 1500: loss 2.3248, time 1076.05ms, mfu 0.01%\n",
      "iter 1600: loss 2.2010, time 125.29ms, mfu 0.01%\n",
      "iter 1700: loss 2.7375, time 116.84ms, mfu 0.01%\n",
      "step 1750: train loss 3.2051, val loss 3.2180\n",
      "iter 1800: loss 2.0644, time 82.73ms, mfu 0.01%\n",
      "iter 1900: loss 2.8994, time 68.26ms, mfu 0.01%\n",
      "step 2000: train loss 3.1685, val loss 3.1946\n",
      "iter 2000: loss 2.6485, time 972.22ms, mfu 0.01%\n",
      "iter 2100: loss 2.2854, time 180.18ms, mfu 0.01%\n",
      "iter 2200: loss 2.2452, time 54.96ms, mfu 0.01%\n",
      "step 2250: train loss 3.3217, val loss 3.3164\n",
      "iter 2300: loss 2.1911, time 116.81ms, mfu 0.01%\n",
      "iter 2400: loss 2.4793, time 75.74ms, mfu 0.01%\n",
      "step 2500: train loss 3.1699, val loss 3.1706\n",
      "iter 2500: loss 2.1925, time 899.03ms, mfu 0.01%\n",
      "iter 2600: loss 2.6250, time 52.05ms, mfu 0.01%\n",
      "iter 2700: loss 2.2949, time 84.56ms, mfu 0.01%\n",
      "step 2750: train loss 3.4795, val loss 3.4704\n",
      "iter 2800: loss 2.0825, time 92.15ms, mfu 0.01%\n",
      "iter 2900: loss 2.0516, time 102.94ms, mfu 0.01%\n",
      "step 3000: train loss 3.4250, val loss 3.4382\n",
      "iter 3000: loss 2.0727, time 914.42ms, mfu 0.01%\n",
      "iter 3100: loss 2.0367, time 106.54ms, mfu 0.01%\n",
      "iter 3200: loss 2.1360, time 128.90ms, mfu 0.01%\n",
      "step 3250: train loss 2.8968, val loss 2.9226\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 3300: loss 2.7316, time 52.63ms, mfu 0.01%\n",
      "iter 3400: loss 2.3507, time 92.13ms, mfu 0.01%\n",
      "step 3500: train loss 3.5017, val loss 3.5197\n",
      "iter 3500: loss 2.9645, time 1378.57ms, mfu 0.01%\n",
      "iter 3600: loss 2.2667, time 94.67ms, mfu 0.01%\n",
      "iter 3700: loss 3.0945, time 53.83ms, mfu 0.01%\n",
      "step 3750: train loss 3.0241, val loss 3.0136\n",
      "iter 3800: loss 2.0534, time 48.55ms, mfu 0.01%\n",
      "iter 3900: loss 2.3822, time 50.90ms, mfu 0.02%\n",
      "step 4000: train loss 2.8620, val loss 2.8723\n",
      "saving checkpoint to out/softminus_margin_2.0_5.0_threshold_detached\n",
      "iter 4000: loss 2.2256, time 1035.28ms, mfu 0.01%\n",
      "iter 4100: loss 2.2090, time 64.03ms, mfu 0.01%\n",
      "iter 4200: loss 2.1575, time 70.01ms, mfu 0.02%\n",
      "step 4250: train loss 3.0166, val loss 3.0588\n",
      "iter 4300: loss 3.0697, time 78.65ms, mfu 0.02%\n",
      "iter 4400: loss 2.1442, time 53.16ms, mfu 0.02%\n",
      "step 4500: train loss 3.2412, val loss 3.2019\n",
      "iter 4500: loss 2.3605, time 1254.10ms, mfu 0.01%\n",
      "iter 4600: loss 2.9920, time 90.94ms, mfu 0.01%\n",
      "iter 4700: loss 2.1138, time 72.01ms, mfu 0.01%\n",
      "step 4750: train loss 3.0879, val loss 3.0879\n",
      "iter 4800: loss 2.1441, time 48.75ms, mfu 0.02%\n",
      "iter 4900: loss 2.3341, time 63.12ms, mfu 0.02%\n",
      "step 5000: train loss 2.8604, val loss 2.8739\n",
      "iter 5000: loss 2.1156, time 1110.39ms, mfu 0.01%\n",
      "iter 5100: loss 2.2452, time 66.07ms, mfu 0.02%\n",
      "iter 5200: loss 2.3746, time 91.31ms, mfu 0.02%\n",
      "step 5250: train loss 3.1193, val loss 3.1649\n",
      "iter 5300: loss 2.2353, time 100.63ms, mfu 0.02%\n",
      "iter 5400: loss 2.1785, time 57.21ms, mfu 0.02%\n",
      "step 5500: train loss 3.4109, val loss 3.3962\n",
      "iter 5500: loss 2.2871, time 1383.23ms, mfu 0.01%\n",
      "iter 5600: loss 2.2006, time 59.53ms, mfu 0.02%\n",
      "iter 5700: loss 2.2718, time 93.35ms, mfu 0.01%\n",
      "step 5750: train loss 3.1149, val loss 3.1293\n",
      "iter 5800: loss 2.5708, time 75.08ms, mfu 0.02%\n",
      "iter 5900: loss 2.2572, time 48.77ms, mfu 0.02%\n",
      "step 6000: train loss 3.3061, val loss 3.2443\n",
      "iter 6000: loss 2.3104, time 1054.14ms, mfu 0.01%\n",
      "iter 6100: loss 2.2683, time 57.14ms, mfu 0.02%\n",
      "iter 6200: loss 2.1534, time 48.97ms, mfu 0.02%\n",
      "step 6250: train loss 3.3273, val loss 3.3220\n",
      "iter 6300: loss 2.0856, time 119.55ms, mfu 0.02%\n",
      "iter 6400: loss 2.1320, time 46.16ms, mfu 0.02%\n",
      "step 6500: train loss 3.3936, val loss 3.4062\n",
      "iter 6500: loss 2.0239, time 1064.11ms, mfu 0.02%\n",
      "iter 6600: loss 1.9715, time 46.96ms, mfu 0.02%\n",
      "iter 6700: loss 2.1284, time 92.47ms, mfu 0.02%\n",
      "step 6750: train loss 3.1826, val loss 3.1946\n",
      "iter 6800: loss 2.0579, time 114.35ms, mfu 0.02%\n",
      "iter 6900: loss 2.0967, time 122.96ms, mfu 0.02%\n",
      "step 7000: train loss 3.2871, val loss 3.2990\n",
      "iter 7000: loss 2.1009, time 927.23ms, mfu 0.01%\n",
      "iter 7100: loss 2.1823, time 121.32ms, mfu 0.01%\n",
      "iter 7200: loss 2.3064, time 49.79ms, mfu 0.01%\n",
      "step 7250: train loss 3.1109, val loss 3.1131\n",
      "iter 7300: loss 2.1205, time 73.50ms, mfu 0.02%\n",
      "iter 7400: loss 2.1668, time 84.86ms, mfu 0.02%\n",
      "step 7500: train loss 3.3402, val loss 3.3626\n",
      "iter 7500: loss 2.1738, time 886.67ms, mfu 0.01%\n",
      "iter 7600: loss 2.2033, time 101.28ms, mfu 0.01%\n",
      "iter 7700: loss 2.1994, time 59.70ms, mfu 0.01%\n",
      "step 7750: train loss 3.3408, val loss 3.3465\n",
      "iter 7800: loss 2.1561, time 78.24ms, mfu 0.01%\n",
      "iter 7900: loss 2.1389, time 81.03ms, mfu 0.01%\n",
      "step 8000: train loss 3.3416, val loss 3.3361\n",
      "iter 8000: loss 2.1352, time 729.62ms, mfu 0.01%\n",
      "done for softminus_c=5.0, margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.0, 2.0]:\n",
    "    for softminus_c in [1.0, 5.0]:\n",
    "        args = {\n",
    "            \"out_dir\": f\"out/softminus_margin_{margin}_{softminus_c}_threshold_detached\",\n",
    "            \"model_type\": \"proposed\",\n",
    "            \"margin\": margin,\n",
    "            \"softminus\": \"True\",\n",
    "            \"softminus_c\": softminus_c,\n",
    "            \"detach_threshold\": \"True\"\n",
    "        }\n",
    "        run_training(args)\n",
    "        print(f'done for softminus_c={softminus_c}, margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f037c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_detached_under_threshold\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: detach_logits_under_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1906.55ms, mfu -100.00%\n",
      "iter 100: loss 2.8585, time 87.68ms, mfu 0.01%\n",
      "iter 200: loss 2.8076, time 81.47ms, mfu 0.01%\n",
      "step 250: train loss 2.8000, val loss 2.7924\n",
      "saving checkpoint to out/proposed_margin_1.0_detached_under_threshold\n",
      "iter 300: loss 2.7697, time 78.77ms, mfu 0.01%\n",
      "iter 400: loss 3.0653, time 71.50ms, mfu 0.02%\n",
      "step 500: train loss 2.9860, val loss 3.0397\n",
      "iter 500: loss 3.0951, time 1986.96ms, mfu 0.01%\n",
      "iter 600: loss 3.2705, time 86.09ms, mfu 0.01%\n",
      "iter 700: loss 3.2978, time 108.97ms, mfu 0.01%\n",
      "step 750: train loss 3.4944, val loss 3.6187\n",
      "iter 800: loss 3.9549, time 89.99ms, mfu 0.01%\n",
      "iter 900: loss 4.3147, time 64.19ms, mfu 0.01%\n",
      "step 1000: train loss 4.0381, val loss 4.0759\n",
      "iter 1000: loss 3.7331, time 1249.17ms, mfu 0.01%\n",
      "iter 1100: loss 4.8194, time 87.58ms, mfu 0.01%\n",
      "iter 1200: loss 4.0131, time 94.72ms, mfu 0.01%\n",
      "step 1250: train loss 4.2851, val loss 4.9056\n",
      "iter 1300: loss 4.2904, time 63.45ms, mfu 0.01%\n",
      "iter 1400: loss 4.3300, time 63.90ms, mfu 0.01%\n",
      "step 1500: train loss 4.6413, val loss 4.5391\n",
      "iter 1500: loss 4.7792, time 1228.23ms, mfu 0.01%\n",
      "iter 1600: loss 3.9784, time 69.47ms, mfu 0.01%\n",
      "iter 1700: loss 5.4916, time 92.77ms, mfu 0.01%\n",
      "step 1750: train loss 4.8569, val loss 5.4388\n",
      "iter 1800: loss 4.8528, time 85.52ms, mfu 0.01%\n",
      "iter 1900: loss 6.6479, time 59.32ms, mfu 0.01%\n",
      "step 2000: train loss 5.6265, val loss 6.0379\n",
      "iter 2000: loss 6.6267, time 1071.77ms, mfu 0.01%\n",
      "iter 2100: loss 5.6177, time 93.53ms, mfu 0.01%\n",
      "iter 2200: loss 5.0320, time 116.05ms, mfu 0.01%\n",
      "step 2250: train loss 6.2749, val loss 6.8896\n",
      "iter 2300: loss 6.0379, time 72.69ms, mfu 0.01%\n",
      "iter 2400: loss 5.1622, time 79.55ms, mfu 0.01%\n",
      "step 2500: train loss 6.3777, val loss 6.3476\n",
      "iter 2500: loss 7.4402, time 936.25ms, mfu 0.01%\n",
      "iter 2600: loss 7.1176, time 145.87ms, mfu 0.01%\n",
      "iter 2700: loss 9.4577, time 158.78ms, mfu 0.01%\n",
      "step 2750: train loss 7.0629, val loss 7.2890\n",
      "iter 2800: loss 7.9119, time 220.92ms, mfu 0.01%\n",
      "iter 2900: loss 7.6624, time 65.66ms, mfu 0.01%\n",
      "step 3000: train loss 7.1213, val loss 7.4406\n",
      "iter 3000: loss 7.5263, time 1308.97ms, mfu 0.01%\n",
      "iter 3100: loss 8.2325, time 103.40ms, mfu 0.01%\n",
      "iter 3200: loss 9.2045, time 64.79ms, mfu 0.01%\n",
      "step 3250: train loss 8.0069, val loss 7.9626\n",
      "iter 3300: loss 7.7532, time 140.46ms, mfu 0.01%\n",
      "iter 3400: loss 7.0221, time 124.18ms, mfu 0.01%\n",
      "step 3500: train loss 9.2748, val loss 10.1142\n",
      "iter 3500: loss 11.8051, time 1037.41ms, mfu 0.01%\n",
      "iter 3600: loss 8.3438, time 93.78ms, mfu 0.01%\n",
      "iter 3700: loss 10.7888, time 68.16ms, mfu 0.01%\n",
      "step 3750: train loss 8.8454, val loss 8.5078\n",
      "iter 3800: loss 6.7874, time 68.63ms, mfu 0.01%\n",
      "iter 3900: loss 7.9365, time 115.67ms, mfu 0.01%\n",
      "step 4000: train loss 8.2206, val loss 8.4632\n",
      "iter 4000: loss 11.6324, time 1130.24ms, mfu 0.01%\n",
      "iter 4100: loss 13.6545, time 57.31ms, mfu 0.01%\n",
      "iter 4200: loss 10.7630, time 55.91ms, mfu 0.01%\n",
      "step 4250: train loss 9.7745, val loss 9.7495\n",
      "iter 4300: loss 4.4460, time 96.56ms, mfu 0.01%\n",
      "iter 4400: loss 9.9057, time 121.89ms, mfu 0.01%\n",
      "step 4500: train loss 9.0381, val loss 9.9988\n",
      "iter 4500: loss 12.8864, time 1252.45ms, mfu 0.01%\n",
      "iter 4600: loss 13.1617, time 88.10ms, mfu 0.01%\n",
      "iter 4700: loss 9.9910, time 106.24ms, mfu 0.01%\n",
      "step 4750: train loss 8.2466, val loss 9.6802\n",
      "iter 4800: loss 9.2424, time 61.92ms, mfu 0.01%\n",
      "iter 4900: loss 11.2988, time 77.99ms, mfu 0.01%\n",
      "step 5000: train loss 9.9225, val loss 10.2469\n",
      "iter 5000: loss 9.2733, time 1186.40ms, mfu 0.01%\n",
      "iter 5100: loss 10.9881, time 77.38ms, mfu 0.01%\n",
      "iter 5200: loss 8.7608, time 89.31ms, mfu 0.01%\n",
      "step 5250: train loss 10.6300, val loss 10.8730\n",
      "iter 5300: loss 13.2342, time 106.31ms, mfu 0.01%\n",
      "iter 5400: loss 7.6924, time 56.97ms, mfu 0.01%\n",
      "step 5500: train loss 10.3693, val loss 10.6098\n",
      "iter 5500: loss 11.5555, time 1057.09ms, mfu 0.01%\n",
      "iter 5600: loss 15.8834, time 61.39ms, mfu 0.01%\n",
      "iter 5700: loss 10.5924, time 78.78ms, mfu 0.01%\n",
      "step 5750: train loss 11.0741, val loss 10.4766\n",
      "iter 5800: loss 12.3148, time 99.97ms, mfu 0.01%\n",
      "iter 5900: loss 10.2596, time 113.55ms, mfu 0.01%\n",
      "step 6000: train loss 10.6046, val loss 10.3125\n",
      "iter 6000: loss 6.8113, time 907.63ms, mfu 0.01%\n",
      "iter 6100: loss 11.0558, time 105.62ms, mfu 0.01%\n",
      "iter 6200: loss 12.0560, time 89.60ms, mfu 0.01%\n",
      "step 6250: train loss 10.1427, val loss 11.4164\n",
      "iter 6300: loss 6.9638, time 107.83ms, mfu 0.01%\n",
      "iter 6400: loss 8.6964, time 53.39ms, mfu 0.01%\n",
      "step 6500: train loss 10.1546, val loss 10.5774\n",
      "iter 6500: loss 11.3942, time 1342.95ms, mfu 0.01%\n",
      "iter 6600: loss 9.0028, time 69.93ms, mfu 0.01%\n",
      "iter 6700: loss 9.4197, time 96.56ms, mfu 0.01%\n",
      "step 6750: train loss 10.4355, val loss 12.0394\n",
      "iter 6800: loss 8.7355, time 67.53ms, mfu 0.01%\n",
      "iter 6900: loss 10.7515, time 55.48ms, mfu 0.01%\n",
      "step 7000: train loss 9.9817, val loss 10.1327\n",
      "iter 7000: loss 10.5280, time 1168.50ms, mfu 0.01%\n",
      "iter 7100: loss 13.9820, time 92.01ms, mfu 0.01%\n",
      "iter 7200: loss 8.1287, time 83.66ms, mfu 0.01%\n",
      "step 7250: train loss 11.4332, val loss 12.2990\n",
      "iter 7300: loss 11.9714, time 87.91ms, mfu 0.01%\n",
      "iter 7400: loss 9.5744, time 71.13ms, mfu 0.01%\n",
      "step 7500: train loss 12.2009, val loss 11.3349\n",
      "iter 7500: loss 15.0074, time 1793.89ms, mfu 0.01%\n",
      "iter 7600: loss 12.3661, time 78.59ms, mfu 0.01%\n",
      "iter 7700: loss 10.3453, time 54.31ms, mfu 0.01%\n",
      "step 7750: train loss 11.5914, val loss 11.9456\n",
      "iter 7800: loss 9.6852, time 101.67ms, mfu 0.01%\n",
      "iter 7900: loss 10.0196, time 109.49ms, mfu 0.01%\n",
      "step 8000: train loss 10.4689, val loss 11.4131\n",
      "iter 8000: loss 11.9071, time 1246.54ms, mfu 0.01%\n",
      "done for margin=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_2.0_detached_under_threshold\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 2.0\n",
      "Overriding: detach_logits_under_threshold = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1221.80ms, mfu -100.00%\n",
      "iter 100: loss 2.8020, time 63.80ms, mfu 0.02%\n",
      "iter 200: loss 2.5653, time 102.25ms, mfu 0.02%\n",
      "step 250: train loss 2.5500, val loss 2.5394\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 300: loss 2.5226, time 94.83ms, mfu 0.02%\n",
      "iter 400: loss 2.5943, time 84.13ms, mfu 0.02%\n",
      "step 500: train loss 2.4496, val loss 2.4537\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 500: loss 2.3860, time 1060.29ms, mfu 0.02%\n",
      "iter 600: loss 2.4761, time 63.39ms, mfu 0.02%\n",
      "iter 700: loss 2.3933, time 66.40ms, mfu 0.02%\n",
      "step 750: train loss 2.4551, val loss 2.4765\n",
      "iter 800: loss 2.7053, time 82.23ms, mfu 0.02%\n",
      "iter 900: loss 2.4870, time 94.71ms, mfu 0.02%\n",
      "step 1000: train loss 2.4097, val loss 2.3686\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 1000: loss 2.3988, time 1206.12ms, mfu 0.02%\n",
      "iter 1100: loss 2.6081, time 116.96ms, mfu 0.01%\n",
      "iter 1200: loss 2.3357, time 111.84ms, mfu 0.01%\n",
      "step 1250: train loss 2.3308, val loss 2.3601\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 1300: loss 2.3590, time 105.41ms, mfu 0.01%\n",
      "iter 1400: loss 2.3164, time 56.80ms, mfu 0.02%\n",
      "step 1500: train loss 2.3620, val loss 2.3825\n",
      "iter 1500: loss 2.4337, time 1209.72ms, mfu 0.01%\n",
      "iter 1600: loss 2.4020, time 138.09ms, mfu 0.01%\n",
      "iter 1700: loss 2.3753, time 78.16ms, mfu 0.01%\n",
      "step 1750: train loss 2.2619, val loss 2.4132\n",
      "iter 1800: loss 2.2696, time 87.52ms, mfu 0.01%\n",
      "iter 1900: loss 2.3434, time 83.56ms, mfu 0.01%\n",
      "step 2000: train loss 2.2508, val loss 2.2947\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 2000: loss 2.2538, time 1063.75ms, mfu 0.01%\n",
      "iter 2100: loss 2.1555, time 65.22ms, mfu 0.01%\n",
      "iter 2200: loss 2.2891, time 97.76ms, mfu 0.01%\n",
      "step 2250: train loss 2.4115, val loss 2.5856\n",
      "iter 2300: loss 2.2302, time 188.97ms, mfu 0.01%\n",
      "iter 2400: loss 2.5674, time 85.12ms, mfu 0.01%\n",
      "step 2500: train loss 2.1955, val loss 2.2211\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 2500: loss 2.1337, time 1563.14ms, mfu 0.01%\n",
      "iter 2600: loss 2.1147, time 114.73ms, mfu 0.01%\n",
      "iter 2700: loss 2.1417, time 89.92ms, mfu 0.01%\n",
      "step 2750: train loss 2.2612, val loss 2.2065\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 2800: loss 2.3679, time 65.57ms, mfu 0.01%\n",
      "iter 2900: loss 2.1379, time 55.93ms, mfu 0.01%\n",
      "step 3000: train loss 2.3034, val loss 2.3336\n",
      "iter 3000: loss 2.3029, time 974.10ms, mfu 0.01%\n",
      "iter 3100: loss 2.2707, time 55.10ms, mfu 0.01%\n",
      "iter 3200: loss 2.1236, time 51.41ms, mfu 0.01%\n",
      "step 3250: train loss 2.2413, val loss 2.2020\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 3300: loss 2.3301, time 54.85ms, mfu 0.02%\n",
      "iter 3400: loss 2.3249, time 70.97ms, mfu 0.02%\n",
      "step 3500: train loss 2.2259, val loss 2.2466\n",
      "iter 3500: loss 2.5786, time 1066.13ms, mfu 0.01%\n",
      "iter 3600: loss 2.7989, time 53.77ms, mfu 0.02%\n",
      "iter 3700: loss 3.4370, time 51.76ms, mfu 0.02%\n",
      "step 3750: train loss 2.1867, val loss 2.2075\n",
      "iter 3800: loss 2.1451, time 110.72ms, mfu 0.02%\n",
      "iter 3900: loss 2.0796, time 56.90ms, mfu 0.02%\n",
      "step 4000: train loss 2.2683, val loss 2.2722\n",
      "iter 4000: loss 2.4416, time 1067.93ms, mfu 0.01%\n",
      "iter 4100: loss 3.0417, time 58.56ms, mfu 0.02%\n",
      "iter 4200: loss 3.0228, time 58.05ms, mfu 0.02%\n",
      "step 4250: train loss 2.1374, val loss 2.2208\n",
      "iter 4300: loss 2.4015, time 77.09ms, mfu 0.02%\n",
      "iter 4400: loss 3.2461, time 138.01ms, mfu 0.02%\n",
      "step 4500: train loss 2.3471, val loss 2.2615\n",
      "iter 4500: loss 2.3562, time 1062.49ms, mfu 0.01%\n",
      "iter 4600: loss 2.3202, time 104.44ms, mfu 0.01%\n",
      "iter 4700: loss 2.0669, time 261.54ms, mfu 0.01%\n",
      "step 4750: train loss 2.2264, val loss 2.2732\n",
      "iter 4800: loss 2.0745, time 204.80ms, mfu 0.01%\n",
      "iter 4900: loss 3.2324, time 205.82ms, mfu 0.01%\n",
      "step 5000: train loss 2.5330, val loss 2.5048\n",
      "iter 5000: loss 2.3311, time 2360.56ms, mfu 0.01%\n",
      "iter 5100: loss 2.3607, time 218.95ms, mfu 0.01%\n",
      "iter 5200: loss 2.1149, time 214.62ms, mfu 0.01%\n",
      "step 5250: train loss 2.5719, val loss 2.5624\n",
      "iter 5300: loss 2.5919, time 182.24ms, mfu 0.01%\n",
      "iter 5400: loss 2.1910, time 245.10ms, mfu 0.01%\n",
      "step 5500: train loss 2.0978, val loss 2.1169\n",
      "saving checkpoint to out/proposed_margin_2.0_detached_under_threshold\n",
      "iter 5500: loss 1.9889, time 2534.77ms, mfu 0.01%\n",
      "iter 5600: loss 2.4163, time 161.20ms, mfu 0.01%\n",
      "iter 5700: loss 2.4621, time 121.04ms, mfu 0.01%\n",
      "step 5750: train loss 2.3813, val loss 2.4835\n",
      "iter 5800: loss 2.8879, time 154.82ms, mfu 0.01%\n",
      "iter 5900: loss 3.1389, time 199.11ms, mfu 0.01%\n",
      "step 6000: train loss 2.8896, val loss 2.7841\n",
      "iter 6000: loss 2.1962, time 1695.23ms, mfu 0.01%\n",
      "iter 6100: loss 2.0380, time 135.71ms, mfu 0.01%\n",
      "iter 6200: loss 2.0476, time 78.77ms, mfu 0.01%\n",
      "step 6250: train loss 2.2176, val loss 2.2283\n",
      "iter 6300: loss 2.1745, time 47.36ms, mfu 0.01%\n",
      "iter 6400: loss 2.5801, time 201.14ms, mfu 0.01%\n",
      "step 6500: train loss 2.5349, val loss 2.5574\n",
      "iter 6500: loss 1.8943, time 910.54ms, mfu 0.01%\n",
      "iter 6600: loss 2.0628, time 46.23ms, mfu 0.01%\n",
      "iter 6700: loss 1.9770, time 548.53ms, mfu 0.01%\n",
      "step 6750: train loss 2.4797, val loss 2.6546\n",
      "iter 6800: loss 2.2075, time 70.79ms, mfu 0.01%\n",
      "iter 6900: loss 2.0133, time 171.40ms, mfu 0.01%\n",
      "step 7000: train loss 2.5893, val loss 2.6373\n",
      "iter 7000: loss 2.4706, time 866.16ms, mfu 0.01%\n",
      "iter 7100: loss 2.7597, time 45.66ms, mfu 0.01%\n",
      "iter 7200: loss 2.2338, time 76.38ms, mfu 0.01%\n",
      "step 7250: train loss 2.6234, val loss 2.4651\n",
      "iter 7300: loss 1.8600, time 95.04ms, mfu 0.01%\n",
      "iter 7400: loss 1.9153, time 71.51ms, mfu 0.01%\n",
      "step 7500: train loss 2.6518, val loss 2.4185\n",
      "iter 7500: loss 2.6031, time 1051.40ms, mfu 0.01%\n",
      "iter 7600: loss 2.3140, time 92.05ms, mfu 0.01%\n",
      "iter 7700: loss 2.2041, time 47.01ms, mfu 0.01%\n",
      "step 7750: train loss 2.4302, val loss 2.2727\n",
      "iter 7800: loss 2.5500, time 52.26ms, mfu 0.01%\n",
      "iter 7900: loss 1.9955, time 88.44ms, mfu 0.01%\n",
      "step 8000: train loss 2.5040, val loss 2.6821\n",
      "iter 8000: loss 2.4843, time 856.18ms, mfu 0.01%\n",
      "done for margin=2.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.0, 2.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_detached_under_threshold\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"detach_logits_under_threshold\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "901de2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_32.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 32.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1100.13ms, mfu -100.00%\n",
      "iter 100: loss 2.7908, time 75.86ms, mfu 0.02%\n",
      "iter 200: loss 2.5009, time 49.10ms, mfu 0.02%\n",
      "step 250: train loss 2.4772, val loss 2.4916\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 300: loss 2.4487, time 45.74ms, mfu 0.02%\n",
      "iter 400: loss 2.4665, time 86.23ms, mfu 0.02%\n",
      "step 500: train loss 2.2669, val loss 2.3109\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 500: loss 2.2378, time 1433.06ms, mfu 0.02%\n",
      "iter 600: loss 2.1920, time 125.92ms, mfu 0.02%\n",
      "iter 700: loss 2.1545, time 89.83ms, mfu 0.02%\n",
      "step 750: train loss 2.1503, val loss 2.1625\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 800: loss 2.2467, time 119.64ms, mfu 0.02%\n",
      "iter 900: loss 2.0326, time 162.55ms, mfu 0.01%\n",
      "step 1000: train loss 2.0337, val loss 2.0900\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 1000: loss 2.1145, time 2260.59ms, mfu 0.01%\n",
      "iter 1100: loss 2.0279, time 162.31ms, mfu 0.01%\n",
      "iter 1200: loss 1.9514, time 111.28ms, mfu 0.01%\n",
      "step 1250: train loss 1.9259, val loss 2.0529\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 1300: loss 1.9198, time 146.54ms, mfu 0.01%\n",
      "iter 1400: loss 1.8443, time 119.10ms, mfu 0.01%\n",
      "step 1500: train loss 1.9026, val loss 1.9910\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 1500: loss 1.9099, time 1549.01ms, mfu 0.01%\n",
      "iter 1600: loss 1.9199, time 149.89ms, mfu 0.01%\n",
      "iter 1700: loss 1.8352, time 133.61ms, mfu 0.01%\n",
      "step 1750: train loss 1.8322, val loss 1.9485\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 1800: loss 1.8018, time 114.70ms, mfu 0.01%\n",
      "iter 1900: loss 1.8322, time 111.58ms, mfu 0.01%\n",
      "step 2000: train loss 1.7869, val loss 1.9819\n",
      "iter 2000: loss 1.6818, time 1498.46ms, mfu 0.01%\n",
      "iter 2100: loss 1.7227, time 123.33ms, mfu 0.01%\n",
      "iter 2200: loss 1.8114, time 123.58ms, mfu 0.01%\n",
      "step 2250: train loss 1.7891, val loss 1.9237\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 2300: loss 1.8021, time 98.78ms, mfu 0.01%\n",
      "iter 2400: loss 1.7369, time 73.21ms, mfu 0.01%\n",
      "step 2500: train loss 1.7487, val loss 1.8560\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 2500: loss 1.6898, time 1712.75ms, mfu 0.01%\n",
      "iter 2600: loss 1.6720, time 145.22ms, mfu 0.01%\n",
      "iter 2700: loss 1.6428, time 193.24ms, mfu 0.01%\n",
      "step 2750: train loss 1.7344, val loss 1.8880\n",
      "iter 2800: loss 1.7888, time 128.46ms, mfu 0.01%\n",
      "iter 2900: loss 1.6998, time 122.45ms, mfu 0.01%\n",
      "step 3000: train loss 1.6968, val loss 1.8220\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 3000: loss 1.8184, time 1629.72ms, mfu 0.01%\n",
      "iter 3100: loss 1.8093, time 313.46ms, mfu 0.01%\n",
      "iter 3200: loss 1.7376, time 120.97ms, mfu 0.01%\n",
      "step 3250: train loss 1.6237, val loss 1.8156\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 3300: loss 1.7233, time 131.55ms, mfu 0.01%\n",
      "iter 3400: loss 1.7498, time 86.11ms, mfu 0.01%\n",
      "step 3500: train loss 1.6554, val loss 1.8254\n",
      "iter 3500: loss 1.8636, time 1535.64ms, mfu 0.01%\n",
      "iter 3600: loss 1.6789, time 288.39ms, mfu 0.01%\n",
      "iter 3700: loss 1.6544, time 84.75ms, mfu 0.01%\n",
      "step 3750: train loss 1.6191, val loss 1.7781\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 3800: loss 1.6352, time 127.59ms, mfu 0.01%\n",
      "iter 3900: loss 1.4672, time 104.04ms, mfu 0.01%\n",
      "step 4000: train loss 1.5904, val loss 1.7715\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 4000: loss 1.6164, time 1702.55ms, mfu 0.01%\n",
      "iter 4100: loss 1.7416, time 144.37ms, mfu 0.01%\n",
      "iter 4200: loss 1.5046, time 134.91ms, mfu 0.01%\n",
      "step 4250: train loss 1.5390, val loss 1.7327\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 4300: loss 1.6213, time 125.75ms, mfu 0.01%\n",
      "iter 4400: loss 1.6296, time 141.50ms, mfu 0.01%\n",
      "step 4500: train loss 1.5597, val loss 1.7433\n",
      "iter 4500: loss 1.5264, time 1804.10ms, mfu 0.01%\n",
      "iter 4600: loss 1.6227, time 113.55ms, mfu 0.01%\n",
      "iter 4700: loss 1.5195, time 103.34ms, mfu 0.01%\n",
      "step 4750: train loss 1.5504, val loss 1.7198\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 4800: loss 1.5362, time 148.65ms, mfu 0.01%\n",
      "iter 4900: loss 1.5825, time 156.29ms, mfu 0.01%\n",
      "step 5000: train loss 1.5166, val loss 1.7088\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 5000: loss 1.4665, time 1559.51ms, mfu 0.01%\n",
      "iter 5100: loss 1.4571, time 110.55ms, mfu 0.01%\n",
      "iter 5200: loss 1.4854, time 94.33ms, mfu 0.01%\n",
      "step 5250: train loss 1.4963, val loss 1.7164\n",
      "iter 5300: loss 1.5664, time 116.92ms, mfu 0.01%\n",
      "iter 5400: loss 1.4264, time 69.88ms, mfu 0.01%\n",
      "step 5500: train loss 1.5105, val loss 1.6693\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 5500: loss 1.4574, time 1413.05ms, mfu 0.01%\n",
      "iter 5600: loss 1.5100, time 107.20ms, mfu 0.01%\n",
      "iter 5700: loss 1.4645, time 188.65ms, mfu 0.01%\n",
      "step 5750: train loss 1.4783, val loss 1.6757\n",
      "iter 5800: loss 1.5725, time 140.12ms, mfu 0.01%\n",
      "iter 5900: loss 1.5255, time 72.09ms, mfu 0.01%\n",
      "step 6000: train loss 1.5161, val loss 1.6500\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 6000: loss 1.4916, time 1547.94ms, mfu 0.01%\n",
      "iter 6100: loss 1.5831, time 110.32ms, mfu 0.01%\n",
      "iter 6200: loss 1.3376, time 139.98ms, mfu 0.01%\n",
      "step 6250: train loss 1.4649, val loss 1.6433\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 6300: loss 1.3878, time 128.51ms, mfu 0.01%\n",
      "iter 6400: loss 1.5065, time 115.52ms, mfu 0.01%\n",
      "step 6500: train loss 1.4446, val loss 1.6321\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 6500: loss 1.4048, time 1568.20ms, mfu 0.01%\n",
      "iter 6600: loss 1.3880, time 85.98ms, mfu 0.01%\n",
      "iter 6700: loss 1.3989, time 105.28ms, mfu 0.01%\n",
      "step 6750: train loss 1.4583, val loss 1.6456\n",
      "iter 6800: loss 1.4383, time 118.65ms, mfu 0.01%\n",
      "iter 6900: loss 1.5448, time 97.03ms, mfu 0.01%\n",
      "step 7000: train loss 1.4253, val loss 1.6415\n",
      "iter 7000: loss 1.4400, time 1713.74ms, mfu 0.01%\n",
      "iter 7100: loss 1.5453, time 69.01ms, mfu 0.01%\n",
      "iter 7200: loss 1.4864, time 158.65ms, mfu 0.01%\n",
      "step 7250: train loss 1.4480, val loss 1.6445\n",
      "iter 7300: loss 1.4000, time 97.86ms, mfu 0.01%\n",
      "iter 7400: loss 1.3473, time 124.98ms, mfu 0.01%\n",
      "step 7500: train loss 1.4378, val loss 1.6154\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_32.0\n",
      "iter 7500: loss 1.4497, time 1497.02ms, mfu 0.01%\n",
      "iter 7600: loss 1.4446, time 87.11ms, mfu 0.01%\n",
      "iter 7700: loss 1.5329, time 129.90ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7750: train loss 1.4369, val loss 1.6754\n",
      "iter 7800: loss 1.4041, time 78.97ms, mfu 0.01%\n",
      "iter 7900: loss 1.3969, time 110.25ms, mfu 0.01%\n",
      "step 8000: train loss 1.4624, val loss 1.6347\n",
      "iter 8000: loss 1.3548, time 1462.78ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=32.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_16.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 16.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 2082.94ms, mfu -100.00%\n",
      "iter 100: loss 2.7908, time 112.23ms, mfu 0.01%\n",
      "iter 200: loss 2.5009, time 124.79ms, mfu 0.01%\n",
      "step 250: train loss 2.4786, val loss 2.4931\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 300: loss 2.4438, time 89.21ms, mfu 0.01%\n",
      "iter 400: loss 2.4817, time 66.78ms, mfu 0.01%\n",
      "step 500: train loss 2.2709, val loss 2.3144\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 500: loss 2.2410, time 1458.85ms, mfu 0.01%\n",
      "iter 600: loss 2.1834, time 89.49ms, mfu 0.01%\n",
      "iter 700: loss 2.1395, time 82.56ms, mfu 0.01%\n",
      "step 750: train loss 2.1487, val loss 2.1644\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 800: loss 2.2352, time 92.04ms, mfu 0.01%\n",
      "iter 900: loss 2.0098, time 117.73ms, mfu 0.01%\n",
      "step 1000: train loss 2.0246, val loss 2.0836\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 1000: loss 2.1005, time 1558.20ms, mfu 0.01%\n",
      "iter 1100: loss 2.0151, time 167.50ms, mfu 0.01%\n",
      "iter 1200: loss 1.9342, time 110.71ms, mfu 0.01%\n",
      "step 1250: train loss 1.9367, val loss 2.0506\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 1300: loss 1.9417, time 155.48ms, mfu 0.01%\n",
      "iter 1400: loss 1.8669, time 115.77ms, mfu 0.01%\n",
      "step 1500: train loss 1.9104, val loss 1.9911\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 1500: loss 1.9009, time 1537.04ms, mfu 0.01%\n",
      "iter 1600: loss 1.9241, time 84.50ms, mfu 0.01%\n",
      "iter 1700: loss 1.8295, time 122.81ms, mfu 0.01%\n",
      "step 1750: train loss 1.8281, val loss 1.9522\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 1800: loss 1.8154, time 157.80ms, mfu 0.01%\n",
      "iter 1900: loss 1.8299, time 165.35ms, mfu 0.01%\n",
      "step 2000: train loss 1.7934, val loss 1.9823\n",
      "iter 2000: loss 1.6774, time 1472.45ms, mfu 0.01%\n",
      "iter 2100: loss 1.7368, time 105.38ms, mfu 0.01%\n",
      "iter 2200: loss 1.8145, time 108.55ms, mfu 0.01%\n",
      "step 2250: train loss 1.7983, val loss 1.9245\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 2300: loss 1.7985, time 274.32ms, mfu 0.01%\n",
      "iter 2400: loss 1.7618, time 126.57ms, mfu 0.01%\n",
      "step 2500: train loss 1.7514, val loss 1.8667\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 2500: loss 1.7028, time 1540.41ms, mfu 0.01%\n",
      "iter 2600: loss 1.6789, time 118.53ms, mfu 0.01%\n",
      "iter 2700: loss 1.6208, time 84.07ms, mfu 0.01%\n",
      "step 2750: train loss 1.7328, val loss 1.8908\n",
      "iter 2800: loss 1.7713, time 167.30ms, mfu 0.01%\n",
      "iter 2900: loss 1.7033, time 123.07ms, mfu 0.01%\n",
      "step 3000: train loss 1.6943, val loss 1.8229\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 3000: loss 1.8178, time 1654.78ms, mfu 0.01%\n",
      "iter 3100: loss 1.8227, time 132.93ms, mfu 0.01%\n",
      "iter 3200: loss 1.7198, time 79.03ms, mfu 0.01%\n",
      "step 3250: train loss 1.6253, val loss 1.8246\n",
      "iter 3300: loss 1.7224, time 75.50ms, mfu 0.01%\n",
      "iter 3400: loss 1.7367, time 125.69ms, mfu 0.01%\n",
      "step 3500: train loss 1.6499, val loss 1.8185\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 3500: loss 1.8572, time 1411.98ms, mfu 0.01%\n",
      "iter 3600: loss 1.6500, time 138.89ms, mfu 0.01%\n",
      "iter 3700: loss 1.6450, time 143.50ms, mfu 0.01%\n",
      "step 3750: train loss 1.6125, val loss 1.7860\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 3800: loss 1.6521, time 105.04ms, mfu 0.01%\n",
      "iter 3900: loss 1.4617, time 101.16ms, mfu 0.01%\n",
      "step 4000: train loss 1.5848, val loss 1.7756\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 4000: loss 1.6091, time 1548.85ms, mfu 0.01%\n",
      "iter 4100: loss 1.7316, time 79.14ms, mfu 0.01%\n",
      "iter 4200: loss 1.4847, time 120.02ms, mfu 0.01%\n",
      "step 4250: train loss 1.5359, val loss 1.7420\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 4300: loss 1.6255, time 344.63ms, mfu 0.01%\n",
      "iter 4400: loss 1.6044, time 71.10ms, mfu 0.01%\n",
      "step 4500: train loss 1.5498, val loss 1.7471\n",
      "iter 4500: loss 1.5387, time 1290.16ms, mfu 0.01%\n",
      "iter 4600: loss 1.6352, time 167.34ms, mfu 0.01%\n",
      "iter 4700: loss 1.5174, time 191.26ms, mfu 0.01%\n",
      "step 4750: train loss 1.5535, val loss 1.7158\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 4800: loss 1.5530, time 93.36ms, mfu 0.01%\n",
      "iter 4900: loss 1.6105, time 102.62ms, mfu 0.01%\n",
      "step 5000: train loss 1.5215, val loss 1.7081\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 5000: loss 1.4666, time 1377.29ms, mfu 0.01%\n",
      "iter 5100: loss 1.4681, time 106.38ms, mfu 0.01%\n",
      "iter 5200: loss 1.4978, time 133.66ms, mfu 0.01%\n",
      "step 5250: train loss 1.4920, val loss 1.7152\n",
      "iter 5300: loss 1.5541, time 84.67ms, mfu 0.01%\n",
      "iter 5400: loss 1.4450, time 89.89ms, mfu 0.01%\n",
      "step 5500: train loss 1.5114, val loss 1.6720\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 5500: loss 1.4458, time 1669.41ms, mfu 0.01%\n",
      "iter 5600: loss 1.4945, time 127.67ms, mfu 0.01%\n",
      "iter 5700: loss 1.4665, time 103.29ms, mfu 0.01%\n",
      "step 5750: train loss 1.4795, val loss 1.6708\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 5800: loss 1.5908, time 96.33ms, mfu 0.01%\n",
      "iter 5900: loss 1.5292, time 110.50ms, mfu 0.01%\n",
      "step 6000: train loss 1.5169, val loss 1.6418\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 6000: loss 1.4862, time 1865.34ms, mfu 0.01%\n",
      "iter 6100: loss 1.5901, time 81.50ms, mfu 0.01%\n",
      "iter 6200: loss 1.3255, time 99.44ms, mfu 0.01%\n",
      "step 6250: train loss 1.4697, val loss 1.6455\n",
      "iter 6300: loss 1.4142, time 102.25ms, mfu 0.01%\n",
      "iter 6400: loss 1.4963, time 120.20ms, mfu 0.01%\n",
      "step 6500: train loss 1.4442, val loss 1.6321\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 6500: loss 1.3969, time 1575.76ms, mfu 0.01%\n",
      "iter 6600: loss 1.3777, time 74.04ms, mfu 0.01%\n",
      "iter 6700: loss 1.4004, time 114.51ms, mfu 0.01%\n",
      "step 6750: train loss 1.4512, val loss 1.6437\n",
      "iter 6800: loss 1.4445, time 78.56ms, mfu 0.01%\n",
      "iter 6900: loss 1.5533, time 109.21ms, mfu 0.01%\n",
      "step 7000: train loss 1.4227, val loss 1.6369\n",
      "iter 7000: loss 1.4411, time 1509.13ms, mfu 0.01%\n",
      "iter 7100: loss 1.5283, time 73.39ms, mfu 0.01%\n",
      "iter 7200: loss 1.5003, time 73.05ms, mfu 0.01%\n",
      "step 7250: train loss 1.4446, val loss 1.6369\n",
      "iter 7300: loss 1.3957, time 132.53ms, mfu 0.01%\n",
      "iter 7400: loss 1.3567, time 89.67ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: train loss 1.4384, val loss 1.6075\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_16.0\n",
      "iter 7500: loss 1.4619, time 1822.87ms, mfu 0.01%\n",
      "iter 7600: loss 1.4752, time 124.01ms, mfu 0.01%\n",
      "iter 7700: loss 1.5124, time 114.71ms, mfu 0.01%\n",
      "step 7750: train loss 1.4347, val loss 1.6648\n",
      "iter 7800: loss 1.3820, time 129.72ms, mfu 0.01%\n",
      "iter 7900: loss 1.4163, time 121.78ms, mfu 0.01%\n",
      "step 8000: train loss 1.4521, val loss 1.6292\n",
      "iter 8000: loss 1.3490, time 1543.89ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=16.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_8.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 8.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 2171.23ms, mfu -100.00%\n",
      "iter 100: loss 2.7584, time 187.49ms, mfu 0.01%\n",
      "iter 200: loss 2.4628, time 79.75ms, mfu 0.01%\n",
      "step 250: train loss 2.5167, val loss 2.5274\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 300: loss 2.4404, time 83.68ms, mfu 0.01%\n",
      "iter 400: loss 2.5050, time 84.74ms, mfu 0.01%\n",
      "step 500: train loss 2.2854, val loss 2.3239\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 500: loss 2.2229, time 1703.20ms, mfu 0.01%\n",
      "iter 600: loss 2.1868, time 112.88ms, mfu 0.01%\n",
      "iter 700: loss 2.1295, time 159.19ms, mfu 0.01%\n",
      "step 750: train loss 2.1540, val loss 2.1734\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 800: loss 2.2329, time 134.46ms, mfu 0.01%\n",
      "iter 900: loss 2.0248, time 88.22ms, mfu 0.01%\n",
      "step 1000: train loss 2.0359, val loss 2.0921\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 1000: loss 2.0888, time 1398.91ms, mfu 0.01%\n",
      "iter 1100: loss 2.0248, time 124.33ms, mfu 0.01%\n",
      "iter 1200: loss 1.9441, time 147.64ms, mfu 0.01%\n",
      "step 1250: train loss 1.9344, val loss 2.0621\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 1300: loss 1.9338, time 77.68ms, mfu 0.01%\n",
      "iter 1400: loss 1.8279, time 123.69ms, mfu 0.01%\n",
      "step 1500: train loss 1.9106, val loss 1.9999\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 1500: loss 1.8722, time 1468.32ms, mfu 0.01%\n",
      "iter 1600: loss 1.9069, time 101.47ms, mfu 0.01%\n",
      "iter 1700: loss 1.8153, time 101.93ms, mfu 0.01%\n",
      "step 1750: train loss 1.8282, val loss 1.9590\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 1800: loss 1.7935, time 125.93ms, mfu 0.01%\n",
      "iter 1900: loss 1.8173, time 88.22ms, mfu 0.01%\n",
      "step 2000: train loss 1.7877, val loss 1.9723\n",
      "iter 2000: loss 1.6628, time 1473.19ms, mfu 0.01%\n",
      "iter 2100: loss 1.7519, time 119.61ms, mfu 0.01%\n",
      "iter 2200: loss 1.8183, time 158.02ms, mfu 0.01%\n",
      "step 2250: train loss 1.8023, val loss 1.9261\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 2300: loss 1.7822, time 130.13ms, mfu 0.01%\n",
      "iter 2400: loss 1.7235, time 141.37ms, mfu 0.01%\n",
      "step 2500: train loss 1.7415, val loss 1.8586\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 2500: loss 1.6890, time 1556.77ms, mfu 0.01%\n",
      "iter 2600: loss 1.6698, time 123.48ms, mfu 0.01%\n",
      "iter 2700: loss 1.6208, time 92.39ms, mfu 0.01%\n",
      "step 2750: train loss 1.7374, val loss 1.8821\n",
      "iter 2800: loss 1.7609, time 111.75ms, mfu 0.01%\n",
      "iter 2900: loss 1.6878, time 116.56ms, mfu 0.01%\n",
      "step 3000: train loss 1.6935, val loss 1.8287\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 3000: loss 1.8094, time 1552.69ms, mfu 0.01%\n",
      "iter 3100: loss 1.7963, time 130.65ms, mfu 0.01%\n",
      "iter 3200: loss 1.7478, time 91.73ms, mfu 0.01%\n",
      "step 3250: train loss 1.6168, val loss 1.8199\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 3300: loss 1.7045, time 113.22ms, mfu 0.01%\n",
      "iter 3400: loss 1.7463, time 117.47ms, mfu 0.01%\n",
      "step 3500: train loss 1.6485, val loss 1.8052\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 3500: loss 1.8189, time 1971.43ms, mfu 0.01%\n",
      "iter 3600: loss 1.6543, time 78.82ms, mfu 0.01%\n",
      "iter 3700: loss 1.6277, time 107.04ms, mfu 0.01%\n",
      "step 3750: train loss 1.6125, val loss 1.7739\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 3800: loss 1.5972, time 133.77ms, mfu 0.01%\n",
      "iter 3900: loss 1.4557, time 163.76ms, mfu 0.01%\n",
      "step 4000: train loss 1.5867, val loss 1.7681\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 4000: loss 1.5887, time 1594.60ms, mfu 0.01%\n",
      "iter 4100: loss 1.6844, time 130.57ms, mfu 0.01%\n",
      "iter 4200: loss 1.4666, time 132.81ms, mfu 0.01%\n",
      "step 4250: train loss 1.5294, val loss 1.7239\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 4300: loss 1.5923, time 117.94ms, mfu 0.01%\n",
      "iter 4400: loss 1.6338, time 138.60ms, mfu 0.01%\n",
      "step 4500: train loss 1.5444, val loss 1.7404\n",
      "iter 4500: loss 1.5131, time 1470.39ms, mfu 0.01%\n",
      "iter 4600: loss 1.6315, time 86.98ms, mfu 0.01%\n",
      "iter 4700: loss 1.5080, time 101.18ms, mfu 0.01%\n",
      "step 4750: train loss 1.5371, val loss 1.7014\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 4800: loss 1.5519, time 126.24ms, mfu 0.01%\n",
      "iter 4900: loss 1.5833, time 102.41ms, mfu 0.01%\n",
      "step 5000: train loss 1.5071, val loss 1.6971\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 5000: loss 1.4455, time 1439.12ms, mfu 0.01%\n",
      "iter 5100: loss 1.4677, time 145.36ms, mfu 0.01%\n",
      "iter 5200: loss 1.4905, time 81.05ms, mfu 0.01%\n",
      "step 5250: train loss 1.4922, val loss 1.6989\n",
      "iter 5300: loss 1.5369, time 120.89ms, mfu 0.01%\n",
      "iter 5400: loss 1.4272, time 123.47ms, mfu 0.01%\n",
      "step 5500: train loss 1.4987, val loss 1.6472\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 5500: loss 1.4637, time 2120.84ms, mfu 0.01%\n",
      "iter 5600: loss 1.4545, time 84.83ms, mfu 0.01%\n",
      "iter 5700: loss 1.4347, time 173.71ms, mfu 0.01%\n",
      "step 5750: train loss 1.4748, val loss 1.6607\n",
      "iter 5800: loss 1.5548, time 100.94ms, mfu 0.01%\n",
      "iter 5900: loss 1.5057, time 131.27ms, mfu 0.01%\n",
      "step 6000: train loss 1.5017, val loss 1.6332\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 6000: loss 1.4758, time 1621.79ms, mfu 0.01%\n",
      "iter 6100: loss 1.5808, time 169.50ms, mfu 0.01%\n",
      "iter 6200: loss 1.3226, time 97.87ms, mfu 0.01%\n",
      "step 6250: train loss 1.4635, val loss 1.6331\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 6300: loss 1.3771, time 140.14ms, mfu 0.01%\n",
      "iter 6400: loss 1.4957, time 132.11ms, mfu 0.01%\n",
      "step 6500: train loss 1.4327, val loss 1.6177\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 6500: loss 1.4021, time 1808.98ms, mfu 0.01%\n",
      "iter 6600: loss 1.3672, time 153.34ms, mfu 0.01%\n",
      "iter 6700: loss 1.4083, time 91.67ms, mfu 0.01%\n",
      "step 6750: train loss 1.4440, val loss 1.6404\n",
      "iter 6800: loss 1.4561, time 210.15ms, mfu 0.01%\n",
      "iter 6900: loss 1.5437, time 101.19ms, mfu 0.01%\n",
      "step 7000: train loss 1.4142, val loss 1.6293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000: loss 1.4334, time 1298.76ms, mfu 0.01%\n",
      "iter 7100: loss 1.5385, time 86.76ms, mfu 0.01%\n",
      "iter 7200: loss 1.4509, time 158.14ms, mfu 0.01%\n",
      "step 7250: train loss 1.4350, val loss 1.6329\n",
      "iter 7300: loss 1.3745, time 146.12ms, mfu 0.01%\n",
      "iter 7400: loss 1.3481, time 135.82ms, mfu 0.01%\n",
      "step 7500: train loss 1.4318, val loss 1.6068\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_8.0\n",
      "iter 7500: loss 1.4349, time 1657.96ms, mfu 0.01%\n",
      "iter 7600: loss 1.4438, time 117.08ms, mfu 0.01%\n",
      "iter 7700: loss 1.4993, time 107.02ms, mfu 0.01%\n",
      "step 7750: train loss 1.4256, val loss 1.6549\n",
      "iter 7800: loss 1.3709, time 101.55ms, mfu 0.01%\n",
      "iter 7900: loss 1.4011, time 109.33ms, mfu 0.01%\n",
      "step 8000: train loss 1.4457, val loss 1.6204\n",
      "iter 8000: loss 1.3396, time 1444.56ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=8.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_4.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 4.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8331, time 1683.73ms, mfu -100.00%\n",
      "iter 100: loss 2.5463, time 107.74ms, mfu 0.01%\n",
      "iter 200: loss 2.2896, time 108.23ms, mfu 0.01%\n",
      "step 250: train loss 2.8103, val loss 2.8215\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 300: loss 2.2682, time 126.40ms, mfu 0.01%\n",
      "iter 400: loss 2.2475, time 96.92ms, mfu 0.01%\n",
      "step 500: train loss 2.5277, val loss 2.5655\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 500: loss 2.0954, time 1432.15ms, mfu 0.01%\n",
      "iter 600: loss 2.0339, time 78.09ms, mfu 0.01%\n",
      "iter 700: loss 1.9690, time 98.91ms, mfu 0.01%\n",
      "step 750: train loss 2.3144, val loss 2.3396\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 800: loss 2.0414, time 108.08ms, mfu 0.01%\n",
      "iter 900: loss 1.8635, time 119.52ms, mfu 0.01%\n",
      "step 1000: train loss 2.1655, val loss 2.2169\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 1000: loss 1.9155, time 1498.99ms, mfu 0.01%\n",
      "iter 1100: loss 1.8512, time 96.26ms, mfu 0.01%\n",
      "iter 1200: loss 1.7602, time 115.79ms, mfu 0.01%\n",
      "step 1250: train loss 2.0346, val loss 2.1721\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 1300: loss 1.7925, time 95.59ms, mfu 0.01%\n",
      "iter 1400: loss 1.7115, time 97.65ms, mfu 0.01%\n",
      "step 1500: train loss 1.9678, val loss 2.0623\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 1500: loss 1.6874, time 1580.62ms, mfu 0.01%\n",
      "iter 1600: loss 1.7421, time 111.50ms, mfu 0.01%\n",
      "iter 1700: loss 1.6492, time 109.39ms, mfu 0.01%\n",
      "step 1750: train loss 1.8977, val loss 2.0240\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 1800: loss 1.6772, time 126.73ms, mfu 0.01%\n",
      "iter 1900: loss 1.6957, time 118.32ms, mfu 0.01%\n",
      "step 2000: train loss 1.8503, val loss 2.0172\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 2000: loss 1.5605, time 1569.12ms, mfu 0.01%\n",
      "iter 2100: loss 1.6400, time 107.82ms, mfu 0.01%\n",
      "iter 2200: loss 1.6891, time 83.43ms, mfu 0.01%\n",
      "step 2250: train loss 1.8514, val loss 1.9669\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 2300: loss 1.6659, time 119.01ms, mfu 0.01%\n",
      "iter 2400: loss 1.5962, time 140.01ms, mfu 0.01%\n",
      "step 2500: train loss 1.7942, val loss 1.8889\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 2500: loss 1.5892, time 1514.83ms, mfu 0.01%\n",
      "iter 2600: loss 1.5850, time 91.54ms, mfu 0.01%\n",
      "iter 2700: loss 1.5289, time 77.94ms, mfu 0.01%\n",
      "step 2750: train loss 1.7773, val loss 1.9034\n",
      "iter 2800: loss 1.6412, time 115.93ms, mfu 0.01%\n",
      "iter 2900: loss 1.5623, time 118.54ms, mfu 0.01%\n",
      "step 3000: train loss 1.7358, val loss 1.8420\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 3000: loss 1.7022, time 1523.04ms, mfu 0.01%\n",
      "iter 3100: loss 1.6826, time 77.44ms, mfu 0.01%\n",
      "iter 3200: loss 1.6261, time 150.52ms, mfu 0.01%\n",
      "step 3250: train loss 1.6619, val loss 1.8386\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 3300: loss 1.6108, time 190.95ms, mfu 0.01%\n",
      "iter 3400: loss 1.6388, time 77.06ms, mfu 0.01%\n",
      "step 3500: train loss 1.6845, val loss 1.8265\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 3500: loss 1.6991, time 1710.08ms, mfu 0.01%\n",
      "iter 3600: loss 1.5790, time 174.02ms, mfu 0.01%\n",
      "iter 3700: loss 1.5428, time 141.43ms, mfu 0.01%\n",
      "step 3750: train loss 1.6453, val loss 1.7944\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 3800: loss 1.5261, time 79.52ms, mfu 0.01%\n",
      "iter 3900: loss 1.3661, time 126.22ms, mfu 0.01%\n",
      "step 4000: train loss 1.6324, val loss 1.8011\n",
      "iter 4000: loss 1.5093, time 1653.37ms, mfu 0.01%\n",
      "iter 4100: loss 1.5898, time 122.04ms, mfu 0.01%\n",
      "iter 4200: loss 1.4298, time 135.88ms, mfu 0.01%\n",
      "step 4250: train loss 1.5761, val loss 1.7458\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 4300: loss 1.5025, time 76.68ms, mfu 0.01%\n",
      "iter 4400: loss 1.5498, time 110.35ms, mfu 0.01%\n",
      "step 4500: train loss 1.5859, val loss 1.7566\n",
      "iter 4500: loss 1.4451, time 1629.79ms, mfu 0.01%\n",
      "iter 4600: loss 1.5508, time 99.00ms, mfu 0.01%\n",
      "iter 4700: loss 1.4331, time 94.06ms, mfu 0.01%\n",
      "step 4750: train loss 1.5872, val loss 1.7277\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 4800: loss 1.4840, time 104.22ms, mfu 0.01%\n",
      "iter 4900: loss 1.5129, time 67.82ms, mfu 0.01%\n",
      "step 5000: train loss 1.5550, val loss 1.7194\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 5000: loss 1.3864, time 1585.88ms, mfu 0.01%\n",
      "iter 5100: loss 1.4014, time 124.04ms, mfu 0.01%\n",
      "iter 5200: loss 1.4073, time 187.73ms, mfu 0.01%\n",
      "step 5250: train loss 1.5343, val loss 1.7221\n",
      "iter 5300: loss 1.4653, time 106.83ms, mfu 0.01%\n",
      "iter 5400: loss 1.3584, time 83.24ms, mfu 0.01%\n",
      "step 5500: train loss 1.5447, val loss 1.6813\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 5500: loss 1.3722, time 3032.08ms, mfu 0.01%\n",
      "iter 5600: loss 1.3807, time 108.71ms, mfu 0.01%\n",
      "iter 5700: loss 1.3790, time 104.08ms, mfu 0.01%\n",
      "step 5750: train loss 1.5170, val loss 1.6840\n",
      "iter 5800: loss 1.5114, time 95.62ms, mfu 0.01%\n",
      "iter 5900: loss 1.4436, time 119.54ms, mfu 0.01%\n",
      "step 6000: train loss 1.5470, val loss 1.6567\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 6000: loss 1.4096, time 1489.08ms, mfu 0.01%\n",
      "iter 6100: loss 1.5013, time 110.85ms, mfu 0.01%\n",
      "iter 6200: loss 1.2854, time 98.09ms, mfu 0.01%\n",
      "step 6250: train loss 1.5155, val loss 1.6627\n",
      "iter 6300: loss 1.3504, time 110.18ms, mfu 0.01%\n",
      "iter 6400: loss 1.4472, time 116.08ms, mfu 0.01%\n",
      "step 6500: train loss 1.4842, val loss 1.6527\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 6500: loss 1.3571, time 1463.15ms, mfu 0.01%\n",
      "iter 6600: loss 1.2997, time 70.30ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6700: loss 1.3350, time 95.64ms, mfu 0.01%\n",
      "step 6750: train loss 1.4986, val loss 1.6684\n",
      "iter 6800: loss 1.4039, time 96.93ms, mfu 0.01%\n",
      "iter 6900: loss 1.4793, time 108.15ms, mfu 0.01%\n",
      "step 7000: train loss 1.4702, val loss 1.6589\n",
      "iter 7000: loss 1.3843, time 1473.63ms, mfu 0.01%\n",
      "iter 7100: loss 1.4649, time 125.56ms, mfu 0.01%\n",
      "iter 7200: loss 1.3746, time 140.46ms, mfu 0.01%\n",
      "step 7250: train loss 1.4911, val loss 1.6523\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 7300: loss 1.3102, time 178.61ms, mfu 0.01%\n",
      "iter 7400: loss 1.3010, time 124.14ms, mfu 0.01%\n",
      "step 7500: train loss 1.4904, val loss 1.6373\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_4.0\n",
      "iter 7500: loss 1.3829, time 1488.58ms, mfu 0.01%\n",
      "iter 7600: loss 1.3751, time 84.93ms, mfu 0.01%\n",
      "iter 7700: loss 1.4511, time 93.62ms, mfu 0.01%\n",
      "step 7750: train loss 1.4843, val loss 1.6812\n",
      "iter 7800: loss 1.3277, time 165.13ms, mfu 0.01%\n",
      "iter 7900: loss 1.3554, time 130.49ms, mfu 0.01%\n",
      "step 8000: train loss 1.4923, val loss 1.6509\n",
      "iter 8000: loss 1.2837, time 1660.89ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=4.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_2.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 2.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.7182, time 2065.30ms, mfu -100.00%\n",
      "iter 100: loss 2.3086, time 161.28ms, mfu 0.01%\n",
      "iter 200: loss 2.0530, time 105.58ms, mfu 0.01%\n",
      "step 250: train loss 3.4179, val loss 3.4279\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 300: loss 2.0513, time 72.97ms, mfu 0.01%\n",
      "iter 400: loss 2.0216, time 109.81ms, mfu 0.01%\n",
      "step 500: train loss 3.2083, val loss 3.2355\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 500: loss 1.8404, time 1348.78ms, mfu 0.01%\n",
      "iter 600: loss 1.8095, time 73.93ms, mfu 0.01%\n",
      "iter 700: loss 1.7702, time 147.11ms, mfu 0.01%\n",
      "step 750: train loss 2.9522, val loss 2.9696\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 800: loss 1.8339, time 136.70ms, mfu 0.01%\n",
      "iter 900: loss 1.6160, time 124.44ms, mfu 0.01%\n",
      "step 1000: train loss 2.7600, val loss 2.8010\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 1000: loss 1.7085, time 1530.89ms, mfu 0.01%\n",
      "iter 1100: loss 1.6476, time 152.60ms, mfu 0.01%\n",
      "iter 1200: loss 1.5920, time 258.91ms, mfu 0.01%\n",
      "step 1250: train loss 2.6166, val loss 2.7042\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 1300: loss 1.5859, time 96.26ms, mfu 0.01%\n",
      "iter 1400: loss 1.5559, time 129.69ms, mfu 0.01%\n",
      "step 1500: train loss 2.5157, val loss 2.5757\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 1500: loss 1.5244, time 1479.84ms, mfu 0.01%\n",
      "iter 1600: loss 1.5753, time 122.41ms, mfu 0.01%\n",
      "iter 1700: loss 1.5085, time 106.15ms, mfu 0.01%\n",
      "step 1750: train loss 2.4331, val loss 2.5110\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 1800: loss 1.5130, time 244.88ms, mfu 0.01%\n",
      "iter 1900: loss 1.5325, time 99.30ms, mfu 0.01%\n",
      "step 2000: train loss 2.3502, val loss 2.4694\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 2000: loss 1.4036, time 1518.02ms, mfu 0.01%\n",
      "iter 2100: loss 1.4828, time 91.69ms, mfu 0.01%\n",
      "iter 2200: loss 1.5201, time 109.34ms, mfu 0.01%\n",
      "step 2250: train loss 2.3146, val loss 2.3920\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 2300: loss 1.5071, time 82.16ms, mfu 0.01%\n",
      "iter 2400: loss 1.4502, time 95.07ms, mfu 0.01%\n",
      "step 2500: train loss 2.2494, val loss 2.3230\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 2500: loss 1.4467, time 1659.12ms, mfu 0.01%\n",
      "iter 2600: loss 1.4610, time 124.70ms, mfu 0.01%\n",
      "iter 2700: loss 1.3728, time 81.47ms, mfu 0.01%\n",
      "step 2750: train loss 2.2267, val loss 2.3215\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 2800: loss 1.5068, time 75.52ms, mfu 0.01%\n",
      "iter 2900: loss 1.4427, time 119.76ms, mfu 0.01%\n",
      "step 3000: train loss 2.1643, val loss 2.2384\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 3000: loss 1.5376, time 1432.05ms, mfu 0.01%\n",
      "iter 3100: loss 1.5494, time 198.32ms, mfu 0.01%\n",
      "iter 3200: loss 1.4642, time 113.92ms, mfu 0.01%\n",
      "step 3250: train loss 2.1044, val loss 2.2322\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 3300: loss 1.4872, time 109.12ms, mfu 0.01%\n",
      "iter 3400: loss 1.4891, time 104.76ms, mfu 0.01%\n",
      "step 3500: train loss 2.1062, val loss 2.1988\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 3500: loss 1.5893, time 1684.00ms, mfu 0.01%\n",
      "iter 3600: loss 1.4433, time 175.32ms, mfu 0.01%\n",
      "iter 3700: loss 1.4241, time 89.93ms, mfu 0.01%\n",
      "step 3750: train loss 2.0677, val loss 2.1723\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 3800: loss 1.3983, time 113.47ms, mfu 0.01%\n",
      "iter 3900: loss 1.2646, time 115.38ms, mfu 0.01%\n",
      "step 4000: train loss 2.0627, val loss 2.1747\n",
      "iter 4000: loss 1.3989, time 1373.00ms, mfu 0.01%\n",
      "iter 4100: loss 1.4552, time 109.93ms, mfu 0.01%\n",
      "iter 4200: loss 1.3218, time 136.01ms, mfu 0.01%\n",
      "step 4250: train loss 2.0062, val loss 2.1252\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 4300: loss 1.3798, time 127.61ms, mfu 0.01%\n",
      "iter 4400: loss 1.4121, time 144.71ms, mfu 0.01%\n",
      "step 4500: train loss 2.0112, val loss 2.1237\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 4500: loss 1.3548, time 1568.06ms, mfu 0.01%\n",
      "iter 4600: loss 1.4279, time 75.63ms, mfu 0.01%\n",
      "iter 4700: loss 1.3348, time 115.11ms, mfu 0.01%\n",
      "step 4750: train loss 2.0144, val loss 2.1127\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 4800: loss 1.3631, time 111.01ms, mfu 0.01%\n",
      "iter 4900: loss 1.4131, time 123.10ms, mfu 0.01%\n",
      "step 5000: train loss 1.9807, val loss 2.0925\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 5000: loss 1.2891, time 1527.20ms, mfu 0.01%\n",
      "iter 5100: loss 1.2926, time 96.60ms, mfu 0.01%\n",
      "iter 5200: loss 1.3120, time 122.54ms, mfu 0.01%\n",
      "step 5250: train loss 1.9554, val loss 2.0889\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 5300: loss 1.3639, time 86.49ms, mfu 0.01%\n",
      "iter 5400: loss 1.2688, time 169.37ms, mfu 0.01%\n",
      "step 5500: train loss 1.9657, val loss 2.0634\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 5500: loss 1.2601, time 1361.50ms, mfu 0.01%\n",
      "iter 5600: loss 1.2955, time 79.95ms, mfu 0.01%\n",
      "iter 5700: loss 1.2701, time 143.32ms, mfu 0.01%\n",
      "step 5750: train loss 1.9471, val loss 2.0695\n",
      "iter 5800: loss 1.3888, time 110.75ms, mfu 0.01%\n",
      "iter 5900: loss 1.3627, time 80.65ms, mfu 0.01%\n",
      "step 6000: train loss 1.9771, val loss 2.0486\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6000: loss 1.3153, time 1680.12ms, mfu 0.01%\n",
      "iter 6100: loss 1.3747, time 128.19ms, mfu 0.01%\n",
      "iter 6200: loss 1.1916, time 143.30ms, mfu 0.01%\n",
      "step 6250: train loss 1.9509, val loss 2.0609\n",
      "iter 6300: loss 1.2587, time 70.93ms, mfu 0.01%\n",
      "iter 6400: loss 1.3309, time 115.63ms, mfu 0.01%\n",
      "step 6500: train loss 1.9183, val loss 2.0399\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 6500: loss 1.2317, time 1529.20ms, mfu 0.01%\n",
      "iter 6600: loss 1.2122, time 120.34ms, mfu 0.01%\n",
      "iter 6700: loss 1.2193, time 119.76ms, mfu 0.01%\n",
      "step 6750: train loss 1.9357, val loss 2.0451\n",
      "iter 6800: loss 1.2943, time 126.84ms, mfu 0.01%\n",
      "iter 6900: loss 1.3816, time 86.20ms, mfu 0.01%\n",
      "step 7000: train loss 1.9187, val loss 2.0471\n",
      "iter 7000: loss 1.2781, time 1399.22ms, mfu 0.01%\n",
      "iter 7100: loss 1.3630, time 131.22ms, mfu 0.01%\n",
      "iter 7200: loss 1.2612, time 85.33ms, mfu 0.01%\n",
      "step 7250: train loss 1.9293, val loss 2.0412\n",
      "iter 7300: loss 1.2287, time 89.02ms, mfu 0.01%\n",
      "iter 7400: loss 1.2212, time 77.48ms, mfu 0.01%\n",
      "step 7500: train loss 1.9222, val loss 2.0347\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_2.0\n",
      "iter 7500: loss 1.2962, time 2040.21ms, mfu 0.01%\n",
      "iter 7600: loss 1.2579, time 102.46ms, mfu 0.01%\n",
      "iter 7700: loss 1.3379, time 229.00ms, mfu 0.01%\n",
      "step 7750: train loss 1.9200, val loss 2.0583\n",
      "iter 7800: loss 1.2275, time 115.38ms, mfu 0.01%\n",
      "iter 7900: loss 1.2495, time 108.54ms, mfu 0.01%\n",
      "step 8000: train loss 1.9308, val loss 2.0357\n",
      "iter 8000: loss 1.1974, time 1580.46ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=2.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_1.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 1.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.5010, time 1946.40ms, mfu -100.00%\n",
      "iter 100: loss 2.1406, time 102.97ms, mfu 0.01%\n",
      "iter 200: loss 1.9224, time 106.47ms, mfu 0.01%\n",
      "step 250: train loss 3.9394, val loss 3.9438\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 300: loss 1.9194, time 118.17ms, mfu 0.01%\n",
      "iter 400: loss 1.9052, time 119.93ms, mfu 0.01%\n",
      "step 500: train loss 3.8232, val loss 3.8381\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 500: loss 1.7469, time 1538.67ms, mfu 0.01%\n",
      "iter 600: loss 1.6876, time 136.03ms, mfu 0.01%\n",
      "iter 700: loss 1.6814, time 120.95ms, mfu 0.01%\n",
      "step 750: train loss 3.6705, val loss 3.6762\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 800: loss 1.7081, time 201.64ms, mfu 0.01%\n",
      "iter 900: loss 1.5036, time 122.15ms, mfu 0.01%\n",
      "step 1000: train loss 3.5154, val loss 3.5368\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 1000: loss 1.6103, time 1513.93ms, mfu 0.01%\n",
      "iter 1100: loss 1.5615, time 135.22ms, mfu 0.01%\n",
      "iter 1200: loss 1.5202, time 125.21ms, mfu 0.01%\n",
      "step 1250: train loss 3.3837, val loss 3.4306\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 1300: loss 1.4662, time 107.23ms, mfu 0.01%\n",
      "iter 1400: loss 1.4442, time 77.78ms, mfu 0.01%\n",
      "step 1500: train loss 3.2939, val loss 3.3243\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 1500: loss 1.4583, time 1644.48ms, mfu 0.01%\n",
      "iter 1600: loss 1.4653, time 135.06ms, mfu 0.01%\n",
      "iter 1700: loss 1.3795, time 92.67ms, mfu 0.01%\n",
      "step 1750: train loss 3.1918, val loss 3.2348\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 1800: loss 1.4242, time 89.27ms, mfu 0.01%\n",
      "iter 1900: loss 1.4241, time 66.67ms, mfu 0.01%\n",
      "step 2000: train loss 3.1350, val loss 3.1960\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 2000: loss 1.3235, time 1400.39ms, mfu 0.01%\n",
      "iter 2100: loss 1.4036, time 82.97ms, mfu 0.01%\n",
      "iter 2200: loss 1.4161, time 145.36ms, mfu 0.01%\n",
      "step 2250: train loss 3.0719, val loss 3.1126\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 2300: loss 1.4420, time 147.06ms, mfu 0.01%\n",
      "iter 2400: loss 1.3549, time 95.24ms, mfu 0.01%\n",
      "step 2500: train loss 3.0217, val loss 3.0578\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 2500: loss 1.3719, time 1811.97ms, mfu 0.01%\n",
      "iter 2600: loss 1.3501, time 127.06ms, mfu 0.01%\n",
      "iter 2700: loss 1.2814, time 100.52ms, mfu 0.01%\n",
      "step 2750: train loss 2.9888, val loss 3.0429\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 2800: loss 1.4078, time 102.91ms, mfu 0.01%\n",
      "iter 2900: loss 1.3313, time 118.78ms, mfu 0.01%\n",
      "step 3000: train loss 2.8954, val loss 2.9402\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 3000: loss 1.4080, time 1636.69ms, mfu 0.01%\n",
      "iter 3100: loss 1.4431, time 119.89ms, mfu 0.01%\n",
      "iter 3200: loss 1.3514, time 135.95ms, mfu 0.01%\n",
      "step 3250: train loss 2.8725, val loss 2.9533\n",
      "iter 3300: loss 1.3618, time 94.95ms, mfu 0.01%\n",
      "iter 3400: loss 1.3768, time 94.51ms, mfu 0.01%\n",
      "step 3500: train loss 2.8556, val loss 2.9098\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 3500: loss 1.4687, time 1479.09ms, mfu 0.01%\n",
      "iter 3600: loss 1.3581, time 89.01ms, mfu 0.01%\n",
      "iter 3700: loss 1.3314, time 109.77ms, mfu 0.01%\n",
      "step 3750: train loss 2.8153, val loss 2.8750\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 3800: loss 1.3013, time 110.23ms, mfu 0.01%\n",
      "iter 3900: loss 1.2092, time 105.10ms, mfu 0.01%\n",
      "step 4000: train loss 2.8097, val loss 2.8814\n",
      "iter 4000: loss 1.2814, time 1442.56ms, mfu 0.01%\n",
      "iter 4100: loss 1.3506, time 97.42ms, mfu 0.01%\n",
      "iter 4200: loss 1.2389, time 86.32ms, mfu 0.01%\n",
      "step 4250: train loss 2.7628, val loss 2.8444\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 4300: loss 1.2968, time 146.60ms, mfu 0.01%\n",
      "iter 4400: loss 1.3229, time 139.77ms, mfu 0.01%\n",
      "step 4500: train loss 2.7648, val loss 2.8352\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 4500: loss 1.2609, time 1413.30ms, mfu 0.01%\n",
      "iter 4600: loss 1.3287, time 107.49ms, mfu 0.01%\n",
      "iter 4700: loss 1.2192, time 111.46ms, mfu 0.01%\n",
      "step 4750: train loss 2.7567, val loss 2.8217\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 4800: loss 1.2691, time 74.53ms, mfu 0.01%\n",
      "iter 4900: loss 1.3177, time 89.14ms, mfu 0.01%\n",
      "step 5000: train loss 2.7518, val loss 2.8249\n",
      "iter 5000: loss 1.2099, time 1430.90ms, mfu 0.01%\n",
      "iter 5100: loss 1.1924, time 79.39ms, mfu 0.01%\n",
      "iter 5200: loss 1.1981, time 104.33ms, mfu 0.01%\n",
      "step 5250: train loss 2.7187, val loss 2.8127\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 5300: loss 1.2605, time 119.09ms, mfu 0.01%\n",
      "iter 5400: loss 1.1717, time 103.63ms, mfu 0.01%\n",
      "step 5500: train loss 2.7321, val loss 2.7916\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 5500: loss 1.1960, time 1594.87ms, mfu 0.01%\n",
      "iter 5600: loss 1.2077, time 116.68ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5700: loss 1.1580, time 81.68ms, mfu 0.01%\n",
      "step 5750: train loss 2.7112, val loss 2.7860\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 5800: loss 1.2983, time 132.83ms, mfu 0.01%\n",
      "iter 5900: loss 1.2423, time 114.30ms, mfu 0.01%\n",
      "step 6000: train loss 2.7246, val loss 2.7664\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_1.0\n",
      "iter 6000: loss 1.2163, time 1649.95ms, mfu 0.01%\n",
      "iter 6100: loss 1.2694, time 81.19ms, mfu 0.01%\n",
      "iter 6200: loss 1.1145, time 128.15ms, mfu 0.01%\n",
      "step 6250: train loss 2.7115, val loss 2.7786\n",
      "iter 6300: loss 1.1369, time 82.07ms, mfu 0.01%\n",
      "iter 6400: loss 1.2324, time 117.68ms, mfu 0.01%\n",
      "step 6500: train loss 2.6923, val loss 2.7675\n",
      "iter 6500: loss 1.1493, time 1553.85ms, mfu 0.01%\n",
      "iter 6600: loss 1.1074, time 86.57ms, mfu 0.01%\n",
      "iter 6700: loss 1.1369, time 95.23ms, mfu 0.01%\n",
      "step 6750: train loss 2.7078, val loss 2.7758\n",
      "iter 6800: loss 1.2099, time 104.58ms, mfu 0.01%\n",
      "iter 6900: loss 1.2719, time 88.05ms, mfu 0.01%\n",
      "step 7000: train loss 2.6921, val loss 2.7736\n",
      "iter 7000: loss 1.1983, time 1748.19ms, mfu 0.01%\n",
      "iter 7100: loss 1.2678, time 131.36ms, mfu 0.01%\n",
      "iter 7200: loss 1.1765, time 121.95ms, mfu 0.01%\n",
      "step 7250: train loss 2.7019, val loss 2.7693\n",
      "iter 7300: loss 1.1276, time 92.59ms, mfu 0.01%\n",
      "iter 7400: loss 1.1073, time 102.87ms, mfu 0.01%\n",
      "step 7500: train loss 2.6989, val loss 2.7670\n",
      "iter 7500: loss 1.2011, time 1340.87ms, mfu 0.01%\n",
      "iter 7600: loss 1.1512, time 92.66ms, mfu 0.01%\n",
      "iter 7700: loss 1.2522, time 276.59ms, mfu 0.01%\n",
      "step 7750: train loss 2.7043, val loss 2.7897\n",
      "iter 7800: loss 1.1375, time 122.27ms, mfu 0.01%\n",
      "iter 7900: loss 1.1382, time 114.45ms, mfu 0.01%\n",
      "step 8000: train loss 2.7032, val loss 2.7672\n",
      "iter 8000: loss 1.0862, time 1398.69ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_alpha_0.5\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = True\n",
      "Overriding: margin_by_weight_alpha = 0.5\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.2980, time 2361.55ms, mfu -100.00%\n",
      "iter 100: loss 2.0774, time 142.31ms, mfu 0.01%\n",
      "iter 200: loss 1.8810, time 86.08ms, mfu 0.01%\n",
      "step 250: train loss 4.2999, val loss 4.3042\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 300: loss 1.8921, time 98.46ms, mfu 0.01%\n",
      "iter 400: loss 1.8513, time 116.44ms, mfu 0.01%\n",
      "step 500: train loss 4.2544, val loss 4.2629\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 500: loss 1.7111, time 1523.74ms, mfu 0.01%\n",
      "iter 600: loss 1.6658, time 174.84ms, mfu 0.01%\n",
      "iter 700: loss 1.6265, time 114.20ms, mfu 0.01%\n",
      "step 750: train loss 4.1704, val loss 4.1729\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 800: loss 1.6398, time 116.91ms, mfu 0.01%\n",
      "iter 900: loss 1.5015, time 105.45ms, mfu 0.01%\n",
      "step 1000: train loss 4.0853, val loss 4.0964\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 1000: loss 1.5400, time 1551.58ms, mfu 0.01%\n",
      "iter 1100: loss 1.5169, time 104.20ms, mfu 0.01%\n",
      "iter 1200: loss 1.4386, time 84.96ms, mfu 0.01%\n",
      "step 1250: train loss 3.9960, val loss 4.0234\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 1300: loss 1.3902, time 134.12ms, mfu 0.01%\n",
      "iter 1400: loss 1.3698, time 84.79ms, mfu 0.01%\n",
      "step 1500: train loss 3.9327, val loss 3.9488\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 1500: loss 1.4250, time 1580.61ms, mfu 0.01%\n",
      "iter 1600: loss 1.4799, time 100.92ms, mfu 0.01%\n",
      "iter 1700: loss 1.3263, time 113.16ms, mfu 0.01%\n",
      "step 1750: train loss 3.8662, val loss 3.8922\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 1800: loss 1.3839, time 121.18ms, mfu 0.01%\n",
      "iter 1900: loss 1.3693, time 208.24ms, mfu 0.01%\n",
      "step 2000: train loss 3.8017, val loss 3.8378\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 2000: loss 1.2593, time 1506.51ms, mfu 0.01%\n",
      "iter 2100: loss 1.3383, time 220.69ms, mfu 0.01%\n",
      "iter 2200: loss 1.3682, time 126.36ms, mfu 0.01%\n",
      "step 2250: train loss 3.8027, val loss 3.8238\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 2300: loss 1.3678, time 114.15ms, mfu 0.01%\n",
      "iter 2400: loss 1.3063, time 111.62ms, mfu 0.01%\n",
      "step 2500: train loss 3.7113, val loss 3.7290\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 2500: loss 1.3024, time 1455.60ms, mfu 0.01%\n",
      "iter 2600: loss 1.3304, time 123.01ms, mfu 0.01%\n",
      "iter 2700: loss 1.1893, time 159.69ms, mfu 0.01%\n",
      "step 2750: train loss 3.6858, val loss 3.7170\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 2800: loss 1.3092, time 116.87ms, mfu 0.01%\n",
      "iter 2900: loss 1.2735, time 128.00ms, mfu 0.01%\n",
      "step 3000: train loss 3.6359, val loss 3.6587\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 3000: loss 1.3489, time 2052.02ms, mfu 0.01%\n",
      "iter 3100: loss 1.3531, time 73.14ms, mfu 0.01%\n",
      "iter 3200: loss 1.3065, time 95.87ms, mfu 0.01%\n",
      "step 3250: train loss 3.6186, val loss 3.6685\n",
      "iter 3300: loss 1.2731, time 100.92ms, mfu 0.01%\n",
      "iter 3400: loss 1.3114, time 119.55ms, mfu 0.01%\n",
      "step 3500: train loss 3.5782, val loss 3.6060\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 3500: loss 1.3946, time 1724.49ms, mfu 0.01%\n",
      "iter 3600: loss 1.2764, time 94.22ms, mfu 0.01%\n",
      "iter 3700: loss 1.2592, time 228.52ms, mfu 0.01%\n",
      "step 3750: train loss 3.5587, val loss 3.5957\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 3800: loss 1.2443, time 122.43ms, mfu 0.01%\n",
      "iter 3900: loss 1.1479, time 117.99ms, mfu 0.01%\n",
      "step 4000: train loss 3.5255, val loss 3.5785\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 4000: loss 1.2333, time 1644.82ms, mfu 0.01%\n",
      "iter 4100: loss 1.2946, time 85.62ms, mfu 0.01%\n",
      "iter 4200: loss 1.1938, time 83.57ms, mfu 0.01%\n",
      "step 4250: train loss 3.5272, val loss 3.5737\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 4300: loss 1.2154, time 119.87ms, mfu 0.01%\n",
      "iter 4400: loss 1.2626, time 93.03ms, mfu 0.01%\n",
      "step 4500: train loss 3.5292, val loss 3.5702\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 4500: loss 1.2088, time 1724.68ms, mfu 0.01%\n",
      "iter 4600: loss 1.2753, time 108.87ms, mfu 0.01%\n",
      "iter 4700: loss 1.1724, time 99.90ms, mfu 0.01%\n",
      "step 4750: train loss 3.5232, val loss 3.5596\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 4800: loss 1.1828, time 102.04ms, mfu 0.01%\n",
      "iter 4900: loss 1.2347, time 113.25ms, mfu 0.01%\n",
      "step 5000: train loss 3.4940, val loss 3.5309\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 5000: loss 1.1424, time 1474.58ms, mfu 0.01%\n",
      "iter 5100: loss 1.1523, time 156.03ms, mfu 0.01%\n",
      "iter 5200: loss 1.1565, time 94.85ms, mfu 0.01%\n",
      "step 5250: train loss 3.4857, val loss 3.5442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5300: loss 1.1899, time 122.23ms, mfu 0.01%\n",
      "iter 5400: loss 1.1261, time 132.51ms, mfu 0.01%\n",
      "step 5500: train loss 3.4951, val loss 3.5275\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 5500: loss 1.1314, time 1423.34ms, mfu 0.01%\n",
      "iter 5600: loss 1.1522, time 87.91ms, mfu 0.01%\n",
      "iter 5700: loss 1.1560, time 83.18ms, mfu 0.01%\n",
      "step 5750: train loss 3.4839, val loss 3.5279\n",
      "iter 5800: loss 1.2380, time 96.05ms, mfu 0.01%\n",
      "iter 5900: loss 1.1803, time 135.72ms, mfu 0.01%\n",
      "step 6000: train loss 3.4929, val loss 3.5138\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 6000: loss 1.1549, time 1653.49ms, mfu 0.01%\n",
      "iter 6100: loss 1.2211, time 122.92ms, mfu 0.01%\n",
      "iter 6200: loss 1.0608, time 111.46ms, mfu 0.01%\n",
      "step 6250: train loss 3.4777, val loss 3.5180\n",
      "iter 6300: loss 1.1133, time 143.09ms, mfu 0.01%\n",
      "iter 6400: loss 1.1645, time 79.62ms, mfu 0.01%\n",
      "step 6500: train loss 3.4636, val loss 3.5142\n",
      "iter 6500: loss 1.0896, time 1422.91ms, mfu 0.01%\n",
      "iter 6600: loss 1.0747, time 109.97ms, mfu 0.01%\n",
      "iter 6700: loss 1.0930, time 94.37ms, mfu 0.01%\n",
      "step 6750: train loss 3.4616, val loss 3.5029\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 6800: loss 1.1467, time 120.79ms, mfu 0.01%\n",
      "iter 6900: loss 1.2313, time 132.05ms, mfu 0.01%\n",
      "step 7000: train loss 3.4467, val loss 3.5004\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 7000: loss 1.1405, time 1346.71ms, mfu 0.01%\n",
      "iter 7100: loss 1.2030, time 108.74ms, mfu 0.01%\n",
      "iter 7200: loss 1.1218, time 139.12ms, mfu 0.01%\n",
      "step 7250: train loss 3.4652, val loss 3.5062\n",
      "iter 7300: loss 1.0941, time 88.34ms, mfu 0.01%\n",
      "iter 7400: loss 1.0786, time 76.29ms, mfu 0.01%\n",
      "step 7500: train loss 3.4579, val loss 3.4952\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 7500: loss 1.1446, time 1895.99ms, mfu 0.01%\n",
      "iter 7600: loss 1.0816, time 132.03ms, mfu 0.01%\n",
      "iter 7700: loss 1.1639, time 95.25ms, mfu 0.01%\n",
      "step 7750: train loss 3.4446, val loss 3.5002\n",
      "iter 7800: loss 1.0653, time 124.06ms, mfu 0.01%\n",
      "iter 7900: loss 1.0977, time 109.58ms, mfu 0.01%\n",
      "step 8000: train loss 3.4550, val loss 3.4912\n",
      "saving checkpoint to out/proposed_margin_by_weight_alpha_0.5\n",
      "iter 8000: loss 1.0600, time 1514.70ms, mfu 0.01%\n",
      "done for margin_by_weight_alpha=0.5\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [32.0, 16.0, 8.0, 4.0, 2.0, 1.0, 0.5]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_alpha_{margin_by_weight_alpha}\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"1\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin_by_weight_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19591789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.44\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8600, time 1516.47ms, mfu -100.00%\n",
      "iter 100: loss 2.5359, time 96.64ms, mfu 0.01%\n",
      "iter 200: loss 2.2621, time 58.48ms, mfu 0.01%\n",
      "step 250: train loss 2.5666, val loss 2.5952\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 300: loss 2.0637, time 200.39ms, mfu 0.01%\n",
      "iter 400: loss 2.0489, time 86.56ms, mfu 0.01%\n",
      "step 500: train loss 2.3778, val loss 2.3964\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 500: loss 2.1437, time 1300.18ms, mfu 0.01%\n",
      "iter 600: loss 1.9667, time 57.49ms, mfu 0.01%\n",
      "iter 700: loss 1.8583, time 59.08ms, mfu 0.01%\n",
      "step 750: train loss 2.2378, val loss 2.2576\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 800: loss 1.9584, time 54.77ms, mfu 0.02%\n",
      "iter 900: loss 1.8252, time 605.72ms, mfu 0.01%\n",
      "step 1000: train loss 2.1036, val loss 2.1878\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 1000: loss 1.8716, time 1041.58ms, mfu 0.01%\n",
      "iter 1100: loss 1.6682, time 49.69ms, mfu 0.01%\n",
      "iter 1200: loss 1.7026, time 95.78ms, mfu 0.01%\n",
      "step 1250: train loss 2.0332, val loss 2.1033\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 1300: loss 1.6675, time 53.00ms, mfu 0.01%\n",
      "iter 1400: loss 1.6906, time 49.51ms, mfu 0.02%\n",
      "step 1500: train loss 1.9445, val loss 2.0669\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 1500: loss 1.6145, time 835.57ms, mfu 0.01%\n",
      "iter 1600: loss 1.7225, time 51.30ms, mfu 0.02%\n",
      "iter 1700: loss 1.5251, time 345.55ms, mfu 0.01%\n",
      "step 1750: train loss 1.9137, val loss 2.0024\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 1800: loss 1.6277, time 359.17ms, mfu 0.01%\n",
      "iter 1900: loss 1.5052, time 71.26ms, mfu 0.01%\n",
      "step 2000: train loss 1.8553, val loss 1.9870\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 2000: loss 1.6504, time 679.13ms, mfu 0.01%\n",
      "iter 2100: loss 1.5578, time 101.63ms, mfu 0.01%\n",
      "iter 2200: loss 1.5646, time 103.47ms, mfu 0.01%\n",
      "step 2250: train loss 1.8188, val loss 1.9850\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 2300: loss 1.5481, time 89.00ms, mfu 0.01%\n",
      "iter 2400: loss 1.4877, time 60.36ms, mfu 0.01%\n",
      "step 2500: train loss 1.8158, val loss 1.9304\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 2500: loss 1.5647, time 1930.33ms, mfu 0.01%\n",
      "iter 2600: loss 1.4497, time 57.11ms, mfu 0.01%\n",
      "iter 2700: loss 1.4412, time 68.30ms, mfu 0.01%\n",
      "step 2750: train loss 1.7813, val loss 1.8997\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 2800: loss 1.6133, time 221.10ms, mfu 0.01%\n",
      "iter 2900: loss 1.4173, time 158.72ms, mfu 0.01%\n",
      "step 3000: train loss 1.7874, val loss 1.9186\n",
      "iter 3000: loss 1.4269, time 1742.71ms, mfu 0.01%\n",
      "iter 3100: loss 1.3627, time 92.36ms, mfu 0.01%\n",
      "iter 3200: loss 1.3996, time 132.34ms, mfu 0.01%\n",
      "step 3250: train loss 1.7015, val loss 1.8487\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 3300: loss 1.4924, time 128.25ms, mfu 0.01%\n",
      "iter 3400: loss 1.4180, time 80.30ms, mfu 0.01%\n",
      "step 3500: train loss 1.6790, val loss 1.8619\n",
      "iter 3500: loss 1.3152, time 1975.36ms, mfu 0.01%\n",
      "iter 3600: loss 1.3503, time 65.80ms, mfu 0.01%\n",
      "iter 3700: loss 1.3181, time 317.37ms, mfu 0.01%\n",
      "step 3750: train loss 1.6609, val loss 1.8089\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 3800: loss 1.3625, time 76.37ms, mfu 0.01%\n",
      "iter 3900: loss 1.2845, time 71.94ms, mfu 0.01%\n",
      "step 4000: train loss 1.6216, val loss 1.8367\n",
      "iter 4000: loss 1.4523, time 963.34ms, mfu 0.01%\n",
      "iter 4100: loss 1.2702, time 254.69ms, mfu 0.01%\n",
      "iter 4200: loss 1.2557, time 122.26ms, mfu 0.01%\n",
      "step 4250: train loss 1.6154, val loss 1.8041\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 4300: loss 1.2713, time 66.33ms, mfu 0.01%\n",
      "iter 4400: loss 1.4366, time 153.81ms, mfu 0.01%\n",
      "step 4500: train loss 1.6156, val loss 1.8058\n",
      "iter 4500: loss 1.4003, time 947.16ms, mfu 0.01%\n",
      "iter 4600: loss 1.3858, time 110.94ms, mfu 0.01%\n",
      "iter 4700: loss 1.3240, time 96.92ms, mfu 0.01%\n",
      "step 4750: train loss 1.5997, val loss 1.8104\n",
      "iter 4800: loss 1.3686, time 110.77ms, mfu 0.01%\n",
      "iter 4900: loss 1.2191, time 240.39ms, mfu 0.01%\n",
      "step 5000: train loss 1.5522, val loss 1.7689\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 5000: loss 1.4365, time 3191.21ms, mfu 0.01%\n",
      "iter 5100: loss 1.3068, time 433.46ms, mfu 0.01%\n",
      "iter 5200: loss 1.2976, time 112.36ms, mfu 0.01%\n",
      "step 5250: train loss 1.5695, val loss 1.7355\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 5300: loss 1.1553, time 85.66ms, mfu 0.01%\n",
      "iter 5400: loss 1.2932, time 100.48ms, mfu 0.01%\n",
      "step 5500: train loss 1.5944, val loss 1.7103\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 5500: loss 1.3611, time 2652.91ms, mfu 0.01%\n",
      "iter 5600: loss 1.3238, time 113.81ms, mfu 0.01%\n",
      "iter 5700: loss 1.2389, time 354.41ms, mfu 0.01%\n",
      "step 5750: train loss 1.5226, val loss 1.6965\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 5800: loss 1.2459, time 432.43ms, mfu 0.01%\n",
      "iter 5900: loss 1.2919, time 130.30ms, mfu 0.01%\n",
      "step 6000: train loss 1.5085, val loss 1.7085\n",
      "iter 6000: loss 1.3372, time 1556.09ms, mfu 0.01%\n",
      "iter 6100: loss 1.3588, time 980.58ms, mfu 0.01%\n",
      "iter 6200: loss 1.2540, time 393.97ms, mfu 0.01%\n",
      "step 6250: train loss 1.5293, val loss 1.6818\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 6300: loss 1.3524, time 87.71ms, mfu 0.01%\n",
      "iter 6400: loss 1.1644, time 329.41ms, mfu 0.01%\n",
      "step 6500: train loss 1.5230, val loss 1.6970\n",
      "iter 6500: loss 1.2632, time 2579.48ms, mfu 0.01%\n",
      "iter 6600: loss 1.3322, time 157.49ms, mfu 0.01%\n",
      "iter 6700: loss 1.2763, time 100.75ms, mfu 0.01%\n",
      "step 6750: train loss 1.5011, val loss 1.6778\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 6800: loss 1.2560, time 139.41ms, mfu 0.01%\n",
      "iter 6900: loss 1.2344, time 117.52ms, mfu 0.01%\n",
      "step 7000: train loss 1.4760, val loss 1.6970\n",
      "iter 7000: loss 1.3805, time 1973.97ms, mfu 0.01%\n",
      "iter 7100: loss 1.3137, time 202.63ms, mfu 0.01%\n",
      "iter 7200: loss 1.1795, time 190.85ms, mfu 0.01%\n",
      "step 7250: train loss 1.4764, val loss 1.6836\n",
      "iter 7300: loss 1.2875, time 90.95ms, mfu 0.01%\n",
      "iter 7400: loss 1.3321, time 329.08ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: train loss 1.4546, val loss 1.6745\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.44_separated_embeddings\n",
      "iter 7500: loss 1.2560, time 3502.12ms, mfu 0.01%\n",
      "iter 7600: loss 1.2327, time 627.48ms, mfu 0.01%\n",
      "iter 7700: loss 1.1853, time 138.19ms, mfu 0.01%\n",
      "step 7750: train loss 1.5003, val loss 1.6872\n",
      "iter 7800: loss 1.3103, time 112.11ms, mfu 0.01%\n",
      "iter 7900: loss 1.2317, time 177.54ms, mfu 0.01%\n",
      "step 8000: train loss 1.4703, val loss 1.6856\n",
      "iter 8000: loss 1.2805, time 2647.52ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.44\n"
     ]
    }
   ],
   "source": [
    "margin = 1.0\n",
    "temperature = 0.44\n",
    "args = {\n",
    "    \"out_dir\": f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings\",\n",
    "    \"model_type\": \"proposed\",\n",
    "    \"margin\": margin,\n",
    "    \"temperature\": temperature,\n",
    "    \"separated_embeddings\": \"True\"\n",
    "}\n",
    "run_training(args)\n",
    "print(f'done for margin={margin}, T={temperature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3319cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.0_T_0.44_separated_embeddings/ckpt_fixed.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28/1374527990.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "\n",
    "temperature = 0.44\n",
    "for margin in [1.0]:\n",
    "    # Path to your original checkpoint\n",
    "    ckpt_path = f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings/ckpt_fixed.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9309fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv out/proposed_margin_1.0_T_0.44_separated_embeddings/ckpt.pt out/proposed_margin_1.0_T_0.44_separated_embeddings/ckpt_original.pt\n",
    "!mv out/proposed_margin_1.0_T_0.44_separated_embeddings/ckpt_fixed.pt out/proposed_margin_1.0_T_0.44_separated_embeddings/ckpt.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf1cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 2.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 1396.83ms, mfu -100.00%\n",
      "iter 100: loss 2.7908, time 43.43ms, mfu 0.03%\n",
      "iter 200: loss 2.5009, time 29.19ms, mfu 0.03%\n",
      "step 250: train loss 2.4772, val loss 2.4916\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 300: loss 2.4487, time 37.54ms, mfu 0.03%\n",
      "iter 400: loss 2.4665, time 30.17ms, mfu 0.03%\n",
      "step 500: train loss 2.2669, val loss 2.3109\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 500: loss 2.2378, time 588.76ms, mfu 0.03%\n",
      "iter 600: loss 2.1920, time 71.95ms, mfu 0.03%\n",
      "iter 700: loss 2.1545, time 94.14ms, mfu 0.03%\n",
      "step 750: train loss 2.1503, val loss 2.1625\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 800: loss 2.2467, time 51.96ms, mfu 0.03%\n",
      "iter 900: loss 2.0326, time 122.63ms, mfu 0.02%\n",
      "step 1000: train loss 2.0337, val loss 2.0900\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 1000: loss 2.1145, time 759.24ms, mfu 0.02%\n",
      "iter 1100: loss 2.0279, time 61.22ms, mfu 0.02%\n",
      "iter 1200: loss 1.9514, time 44.14ms, mfu 0.02%\n",
      "step 1250: train loss 1.9259, val loss 2.0529\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 1300: loss 1.9198, time 81.05ms, mfu 0.02%\n",
      "iter 1400: loss 1.8443, time 39.59ms, mfu 0.02%\n",
      "step 1500: train loss 1.9026, val loss 1.9910\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 1500: loss 1.9099, time 2742.52ms, mfu 0.02%\n",
      "iter 1600: loss 1.9199, time 40.41ms, mfu 0.02%\n",
      "iter 1700: loss 1.8352, time 41.64ms, mfu 0.02%\n",
      "step 1750: train loss 1.8322, val loss 1.9485\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 1800: loss 1.8018, time 274.96ms, mfu 0.02%\n",
      "iter 1900: loss 1.8322, time 48.36ms, mfu 0.02%\n",
      "step 2000: train loss 1.7869, val loss 1.9819\n",
      "iter 2000: loss 1.6818, time 1478.38ms, mfu 0.02%\n",
      "iter 2100: loss 1.7227, time 147.52ms, mfu 0.02%\n",
      "iter 2200: loss 1.8114, time 50.14ms, mfu 0.02%\n",
      "step 2250: train loss 1.7891, val loss 1.9237\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 2300: loss 1.8021, time 83.51ms, mfu 0.02%\n",
      "iter 2400: loss 1.7369, time 140.02ms, mfu 0.02%\n",
      "step 2500: train loss 1.7487, val loss 1.8560\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 2500: loss 1.6898, time 1201.56ms, mfu 0.02%\n",
      "iter 2600: loss 1.6720, time 98.64ms, mfu 0.02%\n",
      "iter 2700: loss 1.6428, time 85.69ms, mfu 0.02%\n",
      "step 2750: train loss 1.7344, val loss 1.8880\n",
      "iter 2800: loss 1.7888, time 43.22ms, mfu 0.02%\n",
      "iter 2900: loss 1.6998, time 64.83ms, mfu 0.02%\n",
      "step 3000: train loss 1.6968, val loss 1.8220\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 3000: loss 1.8184, time 1836.02ms, mfu 0.02%\n",
      "iter 3100: loss 1.8093, time 206.98ms, mfu 0.01%\n",
      "iter 3200: loss 1.7376, time 55.86ms, mfu 0.02%\n",
      "step 3250: train loss 1.6237, val loss 1.8156\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 3300: loss 1.7233, time 57.46ms, mfu 0.02%\n",
      "iter 3400: loss 1.7498, time 80.29ms, mfu 0.02%\n",
      "step 3500: train loss 1.6554, val loss 1.8254\n",
      "iter 3500: loss 1.8636, time 1506.55ms, mfu 0.01%\n",
      "iter 3600: loss 1.6789, time 109.51ms, mfu 0.01%\n",
      "iter 3700: loss 1.6544, time 48.91ms, mfu 0.02%\n",
      "step 3750: train loss 1.6191, val loss 1.7781\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 3800: loss 1.6352, time 81.13ms, mfu 0.02%\n",
      "iter 3900: loss 1.4672, time 170.01ms, mfu 0.01%\n",
      "step 4000: train loss 1.5904, val loss 1.7715\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 4000: loss 1.6164, time 1667.32ms, mfu 0.01%\n",
      "iter 4100: loss 1.7416, time 189.83ms, mfu 0.01%\n",
      "iter 4200: loss 1.5046, time 117.12ms, mfu 0.01%\n",
      "step 4250: train loss 1.5390, val loss 1.7327\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 4300: loss 1.6213, time 89.34ms, mfu 0.01%\n",
      "iter 4400: loss 1.6296, time 311.92ms, mfu 0.01%\n",
      "step 4500: train loss 1.5597, val loss 1.7433\n",
      "iter 4500: loss 1.5264, time 1125.30ms, mfu 0.01%\n",
      "iter 4600: loss 1.6227, time 84.90ms, mfu 0.01%\n",
      "iter 4700: loss 1.5195, time 82.14ms, mfu 0.01%\n",
      "step 4750: train loss 1.5504, val loss 1.7198\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 4800: loss 1.5362, time 142.46ms, mfu 0.01%\n",
      "iter 4900: loss 1.5825, time 78.41ms, mfu 0.01%\n",
      "step 5000: train loss 1.5166, val loss 1.7088\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 5000: loss 1.4665, time 1408.49ms, mfu 0.01%\n",
      "iter 5100: loss 1.4571, time 81.91ms, mfu 0.01%\n",
      "iter 5200: loss 1.4854, time 59.30ms, mfu 0.01%\n",
      "step 5250: train loss 1.4963, val loss 1.7164\n",
      "iter 5300: loss 1.5664, time 342.82ms, mfu 0.01%\n",
      "iter 5400: loss 1.4264, time 89.04ms, mfu 0.01%\n",
      "step 5500: train loss 1.5105, val loss 1.6693\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 5500: loss 1.4574, time 2290.48ms, mfu 0.01%\n",
      "iter 5600: loss 1.5100, time 69.28ms, mfu 0.01%\n",
      "iter 5700: loss 1.4645, time 167.84ms, mfu 0.01%\n",
      "step 5750: train loss 1.4783, val loss 1.6757\n",
      "iter 5800: loss 1.5725, time 171.52ms, mfu 0.01%\n",
      "iter 5900: loss 1.5255, time 117.67ms, mfu 0.01%\n",
      "step 6000: train loss 1.5161, val loss 1.6500\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 6000: loss 1.4916, time 1154.66ms, mfu 0.01%\n",
      "iter 6100: loss 1.5831, time 82.87ms, mfu 0.01%\n",
      "iter 6200: loss 1.3376, time 146.60ms, mfu 0.01%\n",
      "step 6250: train loss 1.4649, val loss 1.6433\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 6300: loss 1.3878, time 98.15ms, mfu 0.01%\n",
      "iter 6400: loss 1.5065, time 302.94ms, mfu 0.01%\n",
      "step 6500: train loss 1.4446, val loss 1.6321\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 6500: loss 1.4048, time 1083.60ms, mfu 0.01%\n",
      "iter 6600: loss 1.3880, time 94.12ms, mfu 0.01%\n",
      "iter 6700: loss 1.3989, time 201.88ms, mfu 0.01%\n",
      "step 6750: train loss 1.4583, val loss 1.6456\n",
      "iter 6800: loss 1.4383, time 1087.58ms, mfu 0.01%\n",
      "iter 6900: loss 1.5448, time 619.65ms, mfu 0.01%\n",
      "step 7000: train loss 1.4253, val loss 1.6415\n",
      "iter 7000: loss 1.4400, time 1963.63ms, mfu 0.01%\n",
      "iter 7100: loss 1.5453, time 132.31ms, mfu 0.01%\n",
      "iter 7200: loss 1.4864, time 103.72ms, mfu 0.01%\n",
      "step 7250: train loss 1.4480, val loss 1.6445\n",
      "iter 7300: loss 1.4000, time 49.54ms, mfu 0.01%\n",
      "iter 7400: loss 1.3473, time 224.37ms, mfu 0.01%\n",
      "step 7500: train loss 1.4378, val loss 1.6154\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_2.0\n",
      "iter 7500: loss 1.4497, time 3223.21ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7600: loss 1.4446, time 84.47ms, mfu 0.01%\n",
      "iter 7700: loss 1.5329, time 55.50ms, mfu 0.01%\n",
      "step 7750: train loss 1.4369, val loss 1.6754\n",
      "iter 7800: loss 1.4041, time 65.15ms, mfu 0.01%\n",
      "iter 7900: loss 1.3969, time 79.27ms, mfu 0.01%\n",
      "step 8000: train loss 1.4624, val loss 1.6347\n",
      "iter 8000: loss 1.3548, time 1891.46ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=2.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 1.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8688, time 3529.29ms, mfu -100.00%\n",
      "iter 100: loss 2.8027, time 193.41ms, mfu 0.01%\n",
      "iter 200: loss 2.4962, time 75.54ms, mfu 0.01%\n",
      "step 250: train loss 2.4841, val loss 2.4990\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 300: loss 2.4424, time 96.14ms, mfu 0.01%\n",
      "iter 400: loss 2.4558, time 71.46ms, mfu 0.01%\n",
      "step 500: train loss 2.2742, val loss 2.3155\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 500: loss 2.2416, time 1165.98ms, mfu 0.01%\n",
      "iter 600: loss 2.1961, time 237.06ms, mfu 0.01%\n",
      "iter 700: loss 2.1446, time 90.81ms, mfu 0.01%\n",
      "step 750: train loss 2.1444, val loss 2.1636\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 800: loss 2.2297, time 63.66ms, mfu 0.01%\n",
      "iter 900: loss 2.0246, time 134.14ms, mfu 0.01%\n",
      "step 1000: train loss 2.0247, val loss 2.0775\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 1000: loss 2.0827, time 1202.50ms, mfu 0.01%\n",
      "iter 1100: loss 2.0165, time 72.46ms, mfu 0.01%\n",
      "iter 1200: loss 1.9535, time 73.59ms, mfu 0.01%\n",
      "step 1250: train loss 1.9354, val loss 2.0636\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 1300: loss 1.9356, time 108.22ms, mfu 0.01%\n",
      "iter 1400: loss 1.8726, time 104.32ms, mfu 0.01%\n",
      "step 1500: train loss 1.9026, val loss 1.9842\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 1500: loss 1.8907, time 1333.70ms, mfu 0.01%\n",
      "iter 1600: loss 1.9179, time 92.29ms, mfu 0.01%\n",
      "iter 1700: loss 1.8246, time 67.08ms, mfu 0.01%\n",
      "step 1750: train loss 1.8287, val loss 1.9426\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 1800: loss 1.7960, time 81.67ms, mfu 0.01%\n",
      "iter 1900: loss 1.8156, time 144.84ms, mfu 0.01%\n",
      "step 2000: train loss 1.7886, val loss 1.9889\n",
      "iter 2000: loss 1.6538, time 2263.90ms, mfu 0.01%\n",
      "iter 2100: loss 1.7331, time 53.25ms, mfu 0.01%\n",
      "iter 2200: loss 1.8317, time 89.53ms, mfu 0.01%\n",
      "step 2250: train loss 1.8092, val loss 1.9391\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 2300: loss 1.8169, time 76.60ms, mfu 0.01%\n",
      "iter 2400: loss 1.7447, time 122.41ms, mfu 0.01%\n",
      "step 2500: train loss 1.7591, val loss 1.8759\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 2500: loss 1.7239, time 1694.93ms, mfu 0.01%\n",
      "iter 2600: loss 1.6903, time 481.69ms, mfu 0.01%\n",
      "iter 2700: loss 1.6383, time 273.74ms, mfu 0.01%\n",
      "step 2750: train loss 1.7239, val loss 1.8831\n",
      "iter 2800: loss 1.7711, time 49.05ms, mfu 0.01%\n",
      "iter 2900: loss 1.6957, time 362.92ms, mfu 0.01%\n",
      "step 3000: train loss 1.6985, val loss 1.8158\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 3000: loss 1.8131, time 1218.43ms, mfu 0.01%\n",
      "iter 3100: loss 1.8153, time 53.63ms, mfu 0.01%\n",
      "iter 3200: loss 1.7438, time 118.19ms, mfu 0.01%\n",
      "step 3250: train loss 1.6204, val loss 1.8137\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 3300: loss 1.7401, time 65.36ms, mfu 0.01%\n",
      "iter 3400: loss 1.7402, time 58.21ms, mfu 0.01%\n",
      "step 3500: train loss 1.6579, val loss 1.8195\n",
      "iter 3500: loss 1.8273, time 1466.49ms, mfu 0.01%\n",
      "iter 3600: loss 1.6621, time 277.33ms, mfu 0.01%\n",
      "iter 3700: loss 1.6361, time 97.20ms, mfu 0.01%\n",
      "step 3750: train loss 1.6195, val loss 1.7834\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 3800: loss 1.6542, time 73.50ms, mfu 0.01%\n",
      "iter 3900: loss 1.4798, time 177.59ms, mfu 0.01%\n",
      "step 4000: train loss 1.5943, val loss 1.7900\n",
      "iter 4000: loss 1.6255, time 780.83ms, mfu 0.01%\n",
      "iter 4100: loss 1.7069, time 68.47ms, mfu 0.01%\n",
      "iter 4200: loss 1.4972, time 100.86ms, mfu 0.01%\n",
      "step 4250: train loss 1.5389, val loss 1.7306\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 4300: loss 1.5865, time 103.31ms, mfu 0.01%\n",
      "iter 4400: loss 1.6302, time 58.57ms, mfu 0.01%\n",
      "step 4500: train loss 1.5548, val loss 1.7378\n",
      "iter 4500: loss 1.5179, time 1047.20ms, mfu 0.01%\n",
      "iter 4600: loss 1.6314, time 104.65ms, mfu 0.01%\n",
      "iter 4700: loss 1.5331, time 128.56ms, mfu 0.01%\n",
      "step 4750: train loss 1.5471, val loss 1.7159\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 4800: loss 1.5523, time 45.76ms, mfu 0.01%\n",
      "iter 4900: loss 1.5998, time 2769.58ms, mfu 0.01%\n",
      "step 5000: train loss 1.5194, val loss 1.7102\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 5000: loss 1.4805, time 54192.44ms, mfu 0.01%\n",
      "iter 5100: loss 1.4654, time 278.38ms, mfu 0.01%\n",
      "iter 5200: loss 1.4943, time 92.66ms, mfu 0.01%\n",
      "step 5250: train loss 1.5007, val loss 1.7151\n",
      "iter 5300: loss 1.5453, time 152.87ms, mfu 0.01%\n",
      "iter 5400: loss 1.4312, time 414.54ms, mfu 0.01%\n",
      "step 5500: train loss 1.5028, val loss 1.6640\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 5500: loss 1.4362, time 846.71ms, mfu 0.01%\n",
      "iter 5600: loss 1.4912, time 89.77ms, mfu 0.01%\n",
      "iter 5700: loss 1.4718, time 70.06ms, mfu 0.01%\n",
      "step 5750: train loss 1.4812, val loss 1.6765\n",
      "iter 5800: loss 1.5528, time 78.26ms, mfu 0.01%\n",
      "iter 5900: loss 1.5319, time 46.99ms, mfu 0.01%\n",
      "step 6000: train loss 1.5073, val loss 1.6443\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 6000: loss 1.4856, time 1831.22ms, mfu 0.01%\n",
      "iter 6100: loss 1.5695, time 90.84ms, mfu 0.01%\n",
      "iter 6200: loss 1.3224, time 63.96ms, mfu 0.01%\n",
      "step 6250: train loss 1.4722, val loss 1.6489\n",
      "iter 6300: loss 1.3815, time 83.45ms, mfu 0.01%\n",
      "iter 6400: loss 1.5063, time 104.20ms, mfu 0.01%\n",
      "step 6500: train loss 1.4421, val loss 1.6281\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 6500: loss 1.4050, time 1219.06ms, mfu 0.01%\n",
      "iter 6600: loss 1.3863, time 68.44ms, mfu 0.01%\n",
      "iter 6700: loss 1.3823, time 60.39ms, mfu 0.01%\n",
      "step 6750: train loss 1.4563, val loss 1.6473\n",
      "iter 6800: loss 1.4453, time 374.58ms, mfu 0.01%\n",
      "iter 6900: loss 1.5796, time 171.38ms, mfu 0.01%\n",
      "step 7000: train loss 1.4251, val loss 1.6369\n",
      "iter 7000: loss 1.4486, time 1219.74ms, mfu 0.01%\n",
      "iter 7100: loss 1.5391, time 82.78ms, mfu 0.01%\n",
      "iter 7200: loss 1.4640, time 113.56ms, mfu 0.01%\n",
      "step 7250: train loss 1.4442, val loss 1.6394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7300: loss 1.3929, time 690.04ms, mfu 0.01%\n",
      "iter 7400: loss 1.3515, time 43.29ms, mfu 0.01%\n",
      "step 7500: train loss 1.4380, val loss 1.6134\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_1.0\n",
      "iter 7500: loss 1.4704, time 1301.43ms, mfu 0.01%\n",
      "iter 7600: loss 1.4412, time 251.26ms, mfu 0.01%\n",
      "iter 7700: loss 1.5256, time 70.84ms, mfu 0.01%\n",
      "step 7750: train loss 1.4389, val loss 1.6731\n",
      "iter 7800: loss 1.3794, time 67.63ms, mfu 0.01%\n",
      "iter 7900: loss 1.4425, time 60.84ms, mfu 0.01%\n",
      "step 8000: train loss 1.4541, val loss 1.6297\n",
      "iter 8000: loss 1.3412, time 795.52ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=1.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.5\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.8625, time 2232.04ms, mfu -100.00%\n",
      "iter 100: loss 2.6850, time 88.48ms, mfu 0.01%\n",
      "iter 200: loss 2.4259, time 116.16ms, mfu 0.01%\n",
      "step 250: train loss 2.5864, val loss 2.5977\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 300: loss 2.4059, time 64.27ms, mfu 0.01%\n",
      "iter 400: loss 2.3822, time 73.63ms, mfu 0.02%\n",
      "step 500: train loss 2.3292, val loss 2.3670\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 500: loss 2.1900, time 2538.67ms, mfu 0.01%\n",
      "iter 600: loss 2.1552, time 48.65ms, mfu 0.01%\n",
      "iter 700: loss 2.1102, time 90.44ms, mfu 0.01%\n",
      "step 750: train loss 2.1585, val loss 2.1873\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 800: loss 2.1955, time 60.50ms, mfu 0.02%\n",
      "iter 900: loss 1.9842, time 55.31ms, mfu 0.02%\n",
      "step 1000: train loss 2.0273, val loss 2.0845\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 1000: loss 2.0402, time 936.54ms, mfu 0.01%\n",
      "iter 1100: loss 1.9830, time 50.65ms, mfu 0.02%\n",
      "iter 1200: loss 1.8957, time 47.57ms, mfu 0.02%\n",
      "step 1250: train loss 1.9156, val loss 2.0594\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 1300: loss 1.8978, time 48.18ms, mfu 0.02%\n",
      "iter 1400: loss 1.8141, time 71.42ms, mfu 0.02%\n",
      "step 1500: train loss 1.8846, val loss 1.9854\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 1500: loss 1.8198, time 1316.41ms, mfu 0.02%\n",
      "iter 1600: loss 1.8814, time 174.79ms, mfu 0.02%\n",
      "iter 1700: loss 1.7688, time 149.20ms, mfu 0.01%\n",
      "step 1750: train loss 1.8184, val loss 1.9462\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 1800: loss 1.7538, time 79.13ms, mfu 0.01%\n",
      "iter 1900: loss 1.7687, time 39.62ms, mfu 0.02%\n",
      "step 2000: train loss 1.7713, val loss 1.9701\n",
      "iter 2000: loss 1.6478, time 1016.07ms, mfu 0.02%\n",
      "iter 2100: loss 1.7323, time 216.21ms, mfu 0.01%\n",
      "iter 2200: loss 1.7850, time 144.40ms, mfu 0.01%\n",
      "step 2250: train loss 1.7770, val loss 1.9159\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 2300: loss 1.7812, time 45.44ms, mfu 0.02%\n",
      "iter 2400: loss 1.7208, time 77.49ms, mfu 0.02%\n",
      "step 2500: train loss 1.7312, val loss 1.8418\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 2500: loss 1.6722, time 1299.07ms, mfu 0.01%\n",
      "iter 2600: loss 1.6709, time 103.04ms, mfu 0.01%\n",
      "iter 2700: loss 1.5858, time 69.17ms, mfu 0.01%\n",
      "step 2750: train loss 1.7088, val loss 1.8528\n",
      "iter 2800: loss 1.7278, time 46.90ms, mfu 0.02%\n",
      "iter 2900: loss 1.6637, time 65.59ms, mfu 0.02%\n",
      "step 3000: train loss 1.6805, val loss 1.7919\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 3000: loss 1.8175, time 891.41ms, mfu 0.01%\n",
      "iter 3100: loss 1.7820, time 56.95ms, mfu 0.02%\n",
      "iter 3200: loss 1.7302, time 337.49ms, mfu 0.01%\n",
      "step 3250: train loss 1.6018, val loss 1.7940\n",
      "iter 3300: loss 1.6658, time 43.55ms, mfu 0.02%\n",
      "iter 3400: loss 1.7477, time 73.42ms, mfu 0.02%\n",
      "step 3500: train loss 1.6259, val loss 1.7911\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 3500: loss 1.7971, time 1447.21ms, mfu 0.01%\n",
      "iter 3600: loss 1.6455, time 46.02ms, mfu 0.02%\n",
      "iter 3700: loss 1.6452, time 46.69ms, mfu 0.02%\n",
      "step 3750: train loss 1.5917, val loss 1.7474\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 3800: loss 1.6003, time 135.42ms, mfu 0.02%\n",
      "iter 3900: loss 1.4434, time 52.05ms, mfu 0.02%\n",
      "step 4000: train loss 1.5718, val loss 1.7566\n",
      "iter 4000: loss 1.5855, time 904.04ms, mfu 0.02%\n",
      "iter 4100: loss 1.6867, time 85.16ms, mfu 0.02%\n",
      "iter 4200: loss 1.4721, time 69.94ms, mfu 0.02%\n",
      "step 4250: train loss 1.5195, val loss 1.7066\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 4300: loss 1.5829, time 50.55ms, mfu 0.02%\n",
      "iter 4400: loss 1.6142, time 51.08ms, mfu 0.02%\n",
      "step 4500: train loss 1.5360, val loss 1.7379\n",
      "iter 4500: loss 1.5115, time 947.76ms, mfu 0.02%\n",
      "iter 4600: loss 1.6086, time 65.31ms, mfu 0.02%\n",
      "iter 4700: loss 1.5019, time 46.56ms, mfu 0.02%\n",
      "step 4750: train loss 1.5209, val loss 1.7068\n",
      "iter 4800: loss 1.5239, time 67.85ms, mfu 0.02%\n",
      "iter 4900: loss 1.5956, time 95.76ms, mfu 0.02%\n",
      "step 5000: train loss 1.5040, val loss 1.6919\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 5000: loss 1.4300, time 902.93ms, mfu 0.02%\n",
      "iter 5100: loss 1.4615, time 113.94ms, mfu 0.02%\n",
      "iter 5200: loss 1.4737, time 41.69ms, mfu 0.02%\n",
      "step 5250: train loss 1.4841, val loss 1.7054\n",
      "iter 5300: loss 1.5420, time 46.81ms, mfu 0.02%\n",
      "iter 5400: loss 1.4033, time 43.12ms, mfu 0.02%\n",
      "step 5500: train loss 1.4899, val loss 1.6455\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 5500: loss 1.4194, time 858.87ms, mfu 0.02%\n",
      "iter 5600: loss 1.4571, time 107.72ms, mfu 0.02%\n",
      "iter 5700: loss 1.4581, time 77.25ms, mfu 0.02%\n",
      "step 5750: train loss 1.4631, val loss 1.6513\n",
      "iter 5800: loss 1.5487, time 43.67ms, mfu 0.02%\n",
      "iter 5900: loss 1.4962, time 79.21ms, mfu 0.02%\n",
      "step 6000: train loss 1.4911, val loss 1.6272\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 6000: loss 1.4492, time 999.58ms, mfu 0.02%\n",
      "iter 6100: loss 1.5940, time 106.07ms, mfu 0.02%\n",
      "iter 6200: loss 1.3391, time 83.41ms, mfu 0.02%\n",
      "step 6250: train loss 1.4536, val loss 1.6337\n",
      "iter 6300: loss 1.3848, time 136.42ms, mfu 0.02%\n",
      "iter 6400: loss 1.4765, time 51.84ms, mfu 0.02%\n",
      "step 6500: train loss 1.4283, val loss 1.6134\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 6500: loss 1.3737, time 1447.76ms, mfu 0.01%\n",
      "iter 6600: loss 1.3473, time 68.13ms, mfu 0.01%\n",
      "iter 6700: loss 1.3896, time 100.54ms, mfu 0.01%\n",
      "step 6750: train loss 1.4390, val loss 1.6338\n",
      "iter 6800: loss 1.4400, time 78.76ms, mfu 0.01%\n",
      "iter 6900: loss 1.5589, time 48.31ms, mfu 0.02%\n",
      "step 7000: train loss 1.4085, val loss 1.6339\n",
      "iter 7000: loss 1.4314, time 952.03ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7100: loss 1.5333, time 65.94ms, mfu 0.02%\n",
      "iter 7200: loss 1.4371, time 68.19ms, mfu 0.02%\n",
      "step 7250: train loss 1.4330, val loss 1.6269\n",
      "iter 7300: loss 1.3741, time 162.83ms, mfu 0.01%\n",
      "iter 7400: loss 1.3303, time 322.65ms, mfu 0.01%\n",
      "step 7500: train loss 1.4305, val loss 1.5990\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5\n",
      "iter 7500: loss 1.4361, time 1139.56ms, mfu 0.01%\n",
      "iter 7600: loss 1.4294, time 403.35ms, mfu 0.01%\n",
      "iter 7700: loss 1.5109, time 48.83ms, mfu 0.01%\n",
      "step 7750: train loss 1.4220, val loss 1.6532\n",
      "iter 7800: loss 1.3736, time 114.25ms, mfu 0.01%\n",
      "iter 7900: loss 1.3766, time 674.36ms, mfu 0.01%\n",
      "step 8000: train loss 1.4338, val loss 1.6134\n",
      "iter 8000: loss 1.3222, time 2464.55ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.5\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.25\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.7888, time 2025.33ms, mfu -100.00%\n",
      "iter 100: loss 2.4303, time 79.90ms, mfu 0.02%\n",
      "iter 200: loss 2.1619, time 79.30ms, mfu 0.02%\n",
      "step 250: train loss 3.0783, val loss 3.0910\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 300: loss 2.1492, time 71.82ms, mfu 0.02%\n",
      "iter 400: loss 2.1250, time 83.57ms, mfu 0.02%\n",
      "step 500: train loss 2.7811, val loss 2.8166\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 500: loss 1.9790, time 1223.06ms, mfu 0.01%\n",
      "iter 600: loss 1.9304, time 100.46ms, mfu 0.01%\n",
      "iter 700: loss 1.8710, time 61.13ms, mfu 0.02%\n",
      "step 750: train loss 2.5190, val loss 2.5399\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 800: loss 1.9312, time 80.60ms, mfu 0.02%\n",
      "iter 900: loss 1.7394, time 123.72ms, mfu 0.01%\n",
      "step 1000: train loss 2.3214, val loss 2.3743\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 1000: loss 1.8283, time 1546.22ms, mfu 0.01%\n",
      "iter 1100: loss 1.7611, time 305.05ms, mfu 0.01%\n",
      "iter 1200: loss 1.7014, time 36.40ms, mfu 0.01%\n",
      "step 1250: train loss 2.1668, val loss 2.2898\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 1300: loss 1.7106, time 63.18ms, mfu 0.02%\n",
      "iter 1400: loss 1.6227, time 111.78ms, mfu 0.01%\n",
      "step 1500: train loss 2.0887, val loss 2.1684\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 1500: loss 1.6164, time 1608.90ms, mfu 0.01%\n",
      "iter 1600: loss 1.6663, time 63.08ms, mfu 0.01%\n",
      "iter 1700: loss 1.6248, time 116.60ms, mfu 0.01%\n",
      "step 1750: train loss 2.0041, val loss 2.1039\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 1800: loss 1.6257, time 78.03ms, mfu 0.01%\n",
      "iter 1900: loss 1.6407, time 73.43ms, mfu 0.01%\n",
      "step 2000: train loss 1.9436, val loss 2.0875\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 2000: loss 1.4983, time 2087.92ms, mfu 0.01%\n",
      "iter 2100: loss 1.5952, time 120.23ms, mfu 0.01%\n",
      "iter 2200: loss 1.6463, time 184.70ms, mfu 0.01%\n",
      "step 2250: train loss 1.9350, val loss 2.0275\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 2300: loss 1.6233, time 315.44ms, mfu 0.01%\n",
      "iter 2400: loss 1.5633, time 92.42ms, mfu 0.01%\n",
      "step 2500: train loss 1.8777, val loss 1.9657\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 2500: loss 1.5324, time 1550.36ms, mfu 0.01%\n",
      "iter 2600: loss 1.5390, time 86.37ms, mfu 0.01%\n",
      "iter 2700: loss 1.4607, time 88.78ms, mfu 0.01%\n",
      "step 2750: train loss 1.8537, val loss 1.9752\n",
      "iter 2800: loss 1.6159, time 69.82ms, mfu 0.01%\n",
      "iter 2900: loss 1.5283, time 44.25ms, mfu 0.01%\n",
      "step 3000: train loss 1.8128, val loss 1.9049\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 3000: loss 1.6530, time 759.76ms, mfu 0.01%\n",
      "iter 3100: loss 1.6407, time 46.96ms, mfu 0.01%\n",
      "iter 3200: loss 1.5776, time 42.97ms, mfu 0.02%\n",
      "step 3250: train loss 1.7494, val loss 1.8990\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 3300: loss 1.5646, time 59.42ms, mfu 0.02%\n",
      "iter 3400: loss 1.5792, time 55.98ms, mfu 0.02%\n",
      "step 3500: train loss 1.7603, val loss 1.8778\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 3500: loss 1.6871, time 864.89ms, mfu 0.02%\n",
      "iter 3600: loss 1.5357, time 80.84ms, mfu 0.02%\n",
      "iter 3700: loss 1.5201, time 57.22ms, mfu 0.02%\n",
      "step 3750: train loss 1.7173, val loss 1.8449\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 3800: loss 1.4956, time 71.76ms, mfu 0.02%\n",
      "iter 3900: loss 1.3594, time 48.51ms, mfu 0.02%\n",
      "step 4000: train loss 1.6941, val loss 1.8498\n",
      "iter 4000: loss 1.4697, time 941.38ms, mfu 0.02%\n",
      "iter 4100: loss 1.5468, time 105.78ms, mfu 0.02%\n",
      "iter 4200: loss 1.3972, time 45.14ms, mfu 0.02%\n",
      "step 4250: train loss 1.6529, val loss 1.7961\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 4300: loss 1.4673, time 111.67ms, mfu 0.02%\n",
      "iter 4400: loss 1.5213, time 172.21ms, mfu 0.02%\n",
      "step 4500: train loss 1.6564, val loss 1.8062\n",
      "iter 4500: loss 1.4353, time 842.95ms, mfu 0.01%\n",
      "iter 4600: loss 1.5409, time 82.23ms, mfu 0.01%\n",
      "iter 4700: loss 1.4059, time 92.78ms, mfu 0.01%\n",
      "step 4750: train loss 1.6559, val loss 1.7813\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 4800: loss 1.4286, time 162.43ms, mfu 0.01%\n",
      "iter 4900: loss 1.4891, time 114.65ms, mfu 0.01%\n",
      "step 5000: train loss 1.6250, val loss 1.7693\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 5000: loss 1.3614, time 2104.16ms, mfu 0.01%\n",
      "iter 5100: loss 1.3567, time 133.62ms, mfu 0.01%\n",
      "iter 5200: loss 1.3787, time 114.97ms, mfu 0.01%\n",
      "step 5250: train loss 1.6022, val loss 1.7648\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 5300: loss 1.4549, time 134.83ms, mfu 0.01%\n",
      "iter 5400: loss 1.3413, time 66.25ms, mfu 0.01%\n",
      "step 5500: train loss 1.6094, val loss 1.7282\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 5500: loss 1.3423, time 1192.58ms, mfu 0.01%\n",
      "iter 5600: loss 1.3634, time 427.77ms, mfu 0.01%\n",
      "iter 5700: loss 1.3787, time 74.16ms, mfu 0.01%\n",
      "step 5750: train loss 1.5823, val loss 1.7370\n",
      "iter 5800: loss 1.4850, time 51.22ms, mfu 0.01%\n",
      "iter 5900: loss 1.4377, time 43.96ms, mfu 0.01%\n",
      "step 6000: train loss 1.6197, val loss 1.7111\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 6000: loss 1.3862, time 1218.28ms, mfu 0.01%\n",
      "iter 6100: loss 1.4944, time 77.54ms, mfu 0.01%\n",
      "iter 6200: loss 1.2607, time 44.12ms, mfu 0.01%\n",
      "step 6250: train loss 1.5845, val loss 1.7179\n",
      "iter 6300: loss 1.3223, time 249.31ms, mfu 0.01%\n",
      "iter 6400: loss 1.4241, time 80.67ms, mfu 0.01%\n",
      "step 6500: train loss 1.5476, val loss 1.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 6500: loss 1.3153, time 882.91ms, mfu 0.01%\n",
      "iter 6600: loss 1.2928, time 72.08ms, mfu 0.01%\n",
      "iter 6700: loss 1.3153, time 42.18ms, mfu 0.01%\n",
      "step 6750: train loss 1.5612, val loss 1.7063\n",
      "iter 6800: loss 1.3784, time 46.42ms, mfu 0.02%\n",
      "iter 6900: loss 1.4563, time 66.56ms, mfu 0.02%\n",
      "step 7000: train loss 1.5364, val loss 1.7068\n",
      "iter 7000: loss 1.3637, time 665.17ms, mfu 0.02%\n",
      "iter 7100: loss 1.4462, time 46.71ms, mfu 0.02%\n",
      "iter 7200: loss 1.3505, time 68.09ms, mfu 0.02%\n",
      "step 7250: train loss 1.5551, val loss 1.6948\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 7300: loss 1.3002, time 59.48ms, mfu 0.02%\n",
      "iter 7400: loss 1.2641, time 92.64ms, mfu 0.02%\n",
      "step 7500: train loss 1.5496, val loss 1.6871\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25\n",
      "iter 7500: loss 1.3763, time 786.28ms, mfu 0.02%\n",
      "iter 7600: loss 1.3398, time 51.91ms, mfu 0.02%\n",
      "iter 7700: loss 1.4218, time 43.47ms, mfu 0.02%\n",
      "step 7750: train loss 1.5425, val loss 1.7259\n",
      "iter 7800: loss 1.3035, time 64.86ms, mfu 0.02%\n",
      "iter 7900: loss 1.3470, time 197.93ms, mfu 0.02%\n",
      "step 8000: train loss 1.5534, val loss 1.6981\n",
      "iter 8000: loss 1.2738, time 2303.54ms, mfu 0.02%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.25\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [2.0, 1.0, 0.5, 0.25]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"3\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for proposed_margin_by_weight_type_3_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4189c45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.125\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.6145, time 1911.30ms, mfu -100.00%\n",
      "iter 100: loss 2.2095, time 45.81ms, mfu 0.03%\n",
      "iter 200: loss 1.9764, time 77.27ms, mfu 0.03%\n",
      "step 250: train loss 3.7080, val loss 3.7165\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 300: loss 1.9499, time 64.31ms, mfu 0.03%\n",
      "iter 400: loss 1.9269, time 81.32ms, mfu 0.03%\n",
      "step 500: train loss 3.5553, val loss 3.5771\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 500: loss 1.7699, time 1145.93ms, mfu 0.02%\n",
      "iter 600: loss 1.7477, time 67.12ms, mfu 0.02%\n",
      "iter 700: loss 1.7135, time 44.17ms, mfu 0.02%\n",
      "step 750: train loss 3.3653, val loss 3.3762\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 800: loss 1.7508, time 48.86ms, mfu 0.02%\n",
      "iter 900: loss 1.5623, time 91.45ms, mfu 0.02%\n",
      "step 1000: train loss 3.2029, val loss 3.2331\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 1000: loss 1.6308, time 645.46ms, mfu 0.02%\n",
      "iter 1100: loss 1.5972, time 62.98ms, mfu 0.02%\n",
      "iter 1200: loss 1.5464, time 55.67ms, mfu 0.02%\n",
      "step 1250: train loss 3.0542, val loss 3.1136\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 1300: loss 1.4847, time 73.19ms, mfu 0.02%\n",
      "iter 1400: loss 1.4665, time 71.61ms, mfu 0.02%\n",
      "step 1500: train loss 2.9657, val loss 3.0073\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 1500: loss 1.4436, time 820.61ms, mfu 0.02%\n",
      "iter 1600: loss 1.4653, time 41.43ms, mfu 0.02%\n",
      "iter 1700: loss 1.4089, time 70.84ms, mfu 0.02%\n",
      "step 1750: train loss 2.8873, val loss 2.9388\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 1800: loss 1.4472, time 75.31ms, mfu 0.02%\n",
      "iter 1900: loss 1.4483, time 38.96ms, mfu 0.02%\n",
      "step 2000: train loss 2.8193, val loss 2.8926\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 2000: loss 1.3446, time 732.24ms, mfu 0.02%\n",
      "iter 2100: loss 1.3959, time 35.18ms, mfu 0.02%\n",
      "iter 2200: loss 1.4739, time 40.05ms, mfu 0.02%\n",
      "step 2250: train loss 2.7788, val loss 2.8274\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 2300: loss 1.4306, time 74.74ms, mfu 0.02%\n",
      "iter 2400: loss 1.4031, time 34.77ms, mfu 0.02%\n",
      "step 2500: train loss 2.7337, val loss 2.7750\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 2500: loss 1.3966, time 575.77ms, mfu 0.02%\n",
      "iter 2600: loss 1.3628, time 102.84ms, mfu 0.02%\n",
      "iter 2700: loss 1.2862, time 95.89ms, mfu 0.02%\n",
      "step 2750: train loss 2.6881, val loss 2.7544\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 2800: loss 1.4305, time 61.68ms, mfu 0.02%\n",
      "iter 2900: loss 1.3642, time 71.33ms, mfu 0.02%\n",
      "step 3000: train loss 2.6508, val loss 2.6984\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 3000: loss 1.4693, time 725.95ms, mfu 0.02%\n",
      "iter 3100: loss 1.4428, time 77.19ms, mfu 0.02%\n",
      "iter 3200: loss 1.3764, time 64.25ms, mfu 0.02%\n",
      "step 3250: train loss 2.5927, val loss 2.6857\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 3300: loss 1.3923, time 49.02ms, mfu 0.02%\n",
      "iter 3400: loss 1.4078, time 41.05ms, mfu 0.02%\n",
      "step 3500: train loss 2.5802, val loss 2.6450\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 3500: loss 1.4883, time 881.82ms, mfu 0.02%\n",
      "iter 3600: loss 1.3526, time 101.08ms, mfu 0.02%\n",
      "iter 3700: loss 1.3697, time 65.82ms, mfu 0.02%\n",
      "step 3750: train loss 2.5572, val loss 2.6285\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 3800: loss 1.3180, time 54.90ms, mfu 0.02%\n",
      "iter 3900: loss 1.2217, time 41.55ms, mfu 0.02%\n",
      "step 4000: train loss 2.5264, val loss 2.6148\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 4000: loss 1.3101, time 683.54ms, mfu 0.02%\n",
      "iter 4100: loss 1.3834, time 42.97ms, mfu 0.02%\n",
      "iter 4200: loss 1.2537, time 163.40ms, mfu 0.02%\n",
      "step 4250: train loss 2.4976, val loss 2.5880\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 4300: loss 1.2897, time 111.19ms, mfu 0.02%\n",
      "iter 4400: loss 1.3251, time 69.92ms, mfu 0.02%\n",
      "step 4500: train loss 2.4984, val loss 2.5825\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 4500: loss 1.2820, time 1367.41ms, mfu 0.02%\n",
      "iter 4600: loss 1.3483, time 42.91ms, mfu 0.02%\n",
      "iter 4700: loss 1.2456, time 62.02ms, mfu 0.02%\n",
      "step 4750: train loss 2.4965, val loss 2.5695\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 4800: loss 1.2700, time 41.81ms, mfu 0.02%\n",
      "iter 4900: loss 1.3357, time 77.32ms, mfu 0.02%\n",
      "step 5000: train loss 2.4760, val loss 2.5662\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 5000: loss 1.2340, time 1042.30ms, mfu 0.02%\n",
      "iter 5100: loss 1.2301, time 90.56ms, mfu 0.02%\n",
      "iter 5200: loss 1.2469, time 69.81ms, mfu 0.02%\n",
      "step 5250: train loss 2.4609, val loss 2.5660\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 5300: loss 1.2887, time 43.42ms, mfu 0.02%\n",
      "iter 5400: loss 1.1974, time 89.63ms, mfu 0.02%\n",
      "step 5500: train loss 2.4665, val loss 2.5405\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 5500: loss 1.2046, time 809.97ms, mfu 0.02%\n",
      "iter 5600: loss 1.2289, time 98.63ms, mfu 0.02%\n",
      "iter 5700: loss 1.1937, time 257.12ms, mfu 0.01%\n",
      "step 5750: train loss 2.4492, val loss 2.5391\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 5800: loss 1.3128, time 96.27ms, mfu 0.01%\n",
      "iter 5900: loss 1.2615, time 68.45ms, mfu 0.01%\n",
      "step 6000: train loss 2.4659, val loss 2.5195\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 6000: loss 1.2445, time 994.86ms, mfu 0.01%\n",
      "iter 6100: loss 1.2941, time 97.35ms, mfu 0.01%\n",
      "iter 6200: loss 1.1281, time 81.48ms, mfu 0.01%\n",
      "step 6250: train loss 2.4464, val loss 2.5277\n",
      "iter 6300: loss 1.1839, time 73.90ms, mfu 0.01%\n",
      "iter 6400: loss 1.2638, time 134.20ms, mfu 0.01%\n",
      "step 6500: train loss 2.4284, val loss 2.5211\n",
      "iter 6500: loss 1.1944, time 1464.73ms, mfu 0.01%\n",
      "iter 6600: loss 1.1427, time 133.98ms, mfu 0.01%\n",
      "iter 6700: loss 1.1638, time 89.82ms, mfu 0.01%\n",
      "step 6750: train loss 2.4335, val loss 2.5180\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 6800: loss 1.2306, time 103.22ms, mfu 0.01%\n",
      "iter 6900: loss 1.3085, time 111.60ms, mfu 0.01%\n",
      "step 7000: train loss 2.4185, val loss 2.5193\n",
      "iter 7000: loss 1.2238, time 1100.61ms, mfu 0.01%\n",
      "iter 7100: loss 1.2908, time 45.63ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7200: loss 1.2143, time 57.72ms, mfu 0.01%\n",
      "step 7250: train loss 2.4267, val loss 2.5098\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 7300: loss 1.1712, time 306.43ms, mfu 0.01%\n",
      "iter 7400: loss 1.1366, time 70.51ms, mfu 0.01%\n",
      "step 7500: train loss 2.4230, val loss 2.5058\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125\n",
      "iter 7500: loss 1.2368, time 962.28ms, mfu 0.01%\n",
      "iter 7600: loss 1.1871, time 52.80ms, mfu 0.01%\n",
      "iter 7700: loss 1.2545, time 87.30ms, mfu 0.01%\n",
      "step 7750: train loss 2.4317, val loss 2.5321\n",
      "iter 7800: loss 1.1486, time 41.98ms, mfu 0.02%\n",
      "iter 7900: loss 1.1915, time 83.11ms, mfu 0.02%\n",
      "step 8000: train loss 2.4314, val loss 2.5104\n",
      "iter 8000: loss 1.1416, time 926.91ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.125\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.0625\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.3934, time 1354.76ms, mfu -100.00%\n",
      "iter 100: loss 2.1280, time 41.89ms, mfu 0.03%\n",
      "iter 200: loss 1.9502, time 44.34ms, mfu 0.03%\n",
      "step 250: train loss 4.1341, val loss 4.1369\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 300: loss 1.8851, time 75.04ms, mfu 0.03%\n",
      "iter 400: loss 1.8851, time 74.74ms, mfu 0.03%\n",
      "step 500: train loss 4.0851, val loss 4.0975\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 500: loss 1.7076, time 1020.88ms, mfu 0.03%\n",
      "iter 600: loss 1.6689, time 64.16ms, mfu 0.02%\n",
      "iter 700: loss 1.6633, time 60.94ms, mfu 0.02%\n",
      "step 750: train loss 4.0063, val loss 4.0050\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 800: loss 1.6891, time 44.29ms, mfu 0.02%\n",
      "iter 900: loss 1.5103, time 101.35ms, mfu 0.02%\n",
      "step 1000: train loss 3.9197, val loss 3.9335\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 1000: loss 1.5732, time 845.57ms, mfu 0.02%\n",
      "iter 1100: loss 1.5456, time 102.13ms, mfu 0.02%\n",
      "iter 1200: loss 1.4670, time 430.92ms, mfu 0.02%\n",
      "step 1250: train loss 3.8380, val loss 3.8687\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 1300: loss 1.4469, time 152.70ms, mfu 0.02%\n",
      "iter 1400: loss 1.3949, time 88.60ms, mfu 0.02%\n",
      "step 1500: train loss 3.7922, val loss 3.8148\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 1500: loss 1.4081, time 1274.64ms, mfu 0.02%\n",
      "iter 1600: loss 1.4186, time 116.75ms, mfu 0.02%\n",
      "iter 1700: loss 1.3155, time 152.80ms, mfu 0.01%\n",
      "step 1750: train loss 3.7243, val loss 3.7581\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 1800: loss 1.3764, time 165.09ms, mfu 0.01%\n",
      "iter 1900: loss 1.4091, time 130.75ms, mfu 0.01%\n",
      "step 2000: train loss 3.7002, val loss 3.7410\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 2000: loss 1.2830, time 1518.94ms, mfu 0.01%\n",
      "iter 2100: loss 1.3812, time 252.71ms, mfu 0.01%\n",
      "iter 2200: loss 1.3715, time 117.48ms, mfu 0.01%\n",
      "step 2250: train loss 3.6868, val loss 3.7152\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 2300: loss 1.3626, time 46.14ms, mfu 0.01%\n",
      "iter 2400: loss 1.3312, time 113.81ms, mfu 0.01%\n",
      "step 2500: train loss 3.6336, val loss 3.6561\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 2500: loss 1.3030, time 2421.71ms, mfu 0.01%\n",
      "iter 2600: loss 1.3117, time 253.73ms, mfu 0.01%\n",
      "iter 2700: loss 1.1859, time 147.66ms, mfu 0.01%\n",
      "step 2750: train loss 3.6320, val loss 3.6740\n",
      "iter 2800: loss 1.3588, time 184.52ms, mfu 0.01%\n",
      "iter 2900: loss 1.2867, time 86.91ms, mfu 0.01%\n",
      "step 3000: train loss 3.5989, val loss 3.6262\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 3000: loss 1.3589, time 2722.36ms, mfu 0.01%\n",
      "iter 3100: loss 1.3457, time 73.77ms, mfu 0.01%\n",
      "iter 3200: loss 1.2991, time 164.84ms, mfu 0.01%\n",
      "step 3250: train loss 3.5948, val loss 3.6563\n",
      "iter 3300: loss 1.2818, time 90.30ms, mfu 0.01%\n",
      "iter 3400: loss 1.3173, time 55.58ms, mfu 0.01%\n",
      "step 3500: train loss 3.5766, val loss 3.6076\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 3500: loss 1.3731, time 1423.95ms, mfu 0.01%\n",
      "iter 3600: loss 1.2681, time 128.78ms, mfu 0.01%\n",
      "iter 3700: loss 1.2711, time 314.72ms, mfu 0.01%\n",
      "step 3750: train loss 3.5447, val loss 3.5805\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 3800: loss 1.2224, time 74.42ms, mfu 0.01%\n",
      "iter 3900: loss 1.1594, time 102.17ms, mfu 0.01%\n",
      "step 4000: train loss 3.5462, val loss 3.6008\n",
      "iter 4000: loss 1.2035, time 1443.66ms, mfu 0.01%\n",
      "iter 4100: loss 1.2746, time 106.08ms, mfu 0.01%\n",
      "iter 4200: loss 1.1752, time 75.27ms, mfu 0.01%\n",
      "step 4250: train loss 3.4997, val loss 3.5583\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 4300: loss 1.2162, time 94.11ms, mfu 0.01%\n",
      "iter 4400: loss 1.2267, time 84.40ms, mfu 0.01%\n",
      "step 4500: train loss 3.5381, val loss 3.5904\n",
      "iter 4500: loss 1.1799, time 4406.06ms, mfu 0.01%\n",
      "iter 4600: loss 1.2647, time 52.95ms, mfu 0.01%\n",
      "iter 4700: loss 1.1518, time 246.28ms, mfu 0.01%\n",
      "step 4750: train loss 3.5434, val loss 3.5804\n",
      "iter 4800: loss 1.1807, time 100.04ms, mfu 0.01%\n",
      "iter 4900: loss 1.2323, time 143.82ms, mfu 0.01%\n",
      "step 5000: train loss 3.5285, val loss 3.5689\n",
      "iter 5000: loss 1.1307, time 7520.47ms, mfu 0.01%\n",
      "iter 5100: loss 1.1557, time 112.71ms, mfu 0.01%\n",
      "iter 5200: loss 1.1438, time 603.98ms, mfu 0.01%\n",
      "step 5250: train loss 3.5102, val loss 3.5750\n",
      "iter 5300: loss 1.1903, time 216.75ms, mfu 0.01%\n",
      "iter 5400: loss 1.1268, time 65.01ms, mfu 0.01%\n",
      "step 5500: train loss 3.5143, val loss 3.5490\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 5500: loss 1.1193, time 1329.85ms, mfu 0.01%\n",
      "iter 5600: loss 1.1243, time 49.45ms, mfu 0.01%\n",
      "iter 5700: loss 1.1349, time 53.83ms, mfu 0.01%\n",
      "step 5750: train loss 3.5153, val loss 3.5653\n",
      "iter 5800: loss 1.2385, time 75.14ms, mfu 0.01%\n",
      "iter 5900: loss 1.1777, time 55.19ms, mfu 0.01%\n",
      "step 6000: train loss 3.5009, val loss 3.5244\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 6000: loss 1.1570, time 1595.64ms, mfu 0.01%\n",
      "iter 6100: loss 1.2079, time 61.33ms, mfu 0.01%\n",
      "iter 6200: loss 1.0538, time 69.86ms, mfu 0.01%\n",
      "step 6250: train loss 3.4963, val loss 3.5348\n",
      "iter 6300: loss 1.0939, time 1451.84ms, mfu 0.01%\n",
      "iter 6400: loss 1.1784, time 48.21ms, mfu 0.01%\n",
      "step 6500: train loss 3.4828, val loss 3.5355\n",
      "iter 6500: loss 1.0836, time 1043.28ms, mfu 0.01%\n",
      "iter 6600: loss 1.0782, time 152.59ms, mfu 0.01%\n",
      "iter 6700: loss 1.0978, time 50.72ms, mfu 0.01%\n",
      "step 6750: train loss 3.4729, val loss 3.5172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 6800: loss 1.1363, time 322.18ms, mfu 0.01%\n",
      "iter 6900: loss 1.2149, time 75.26ms, mfu 0.01%\n",
      "step 7000: train loss 3.4407, val loss 3.5035\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 7000: loss 1.1370, time 974.25ms, mfu 0.01%\n",
      "iter 7100: loss 1.2080, time 54.64ms, mfu 0.01%\n",
      "iter 7200: loss 1.1006, time 109.92ms, mfu 0.01%\n",
      "step 7250: train loss 3.4688, val loss 3.5150\n",
      "iter 7300: loss 1.0981, time 48.37ms, mfu 0.01%\n",
      "iter 7400: loss 1.0709, time 112.69ms, mfu 0.01%\n",
      "step 7500: train loss 3.4653, val loss 3.5088\n",
      "iter 7500: loss 1.1568, time 1355.23ms, mfu 0.01%\n",
      "iter 7600: loss 1.1063, time 47.41ms, mfu 0.01%\n",
      "iter 7700: loss 1.1636, time 45.97ms, mfu 0.02%\n",
      "step 7750: train loss 3.4646, val loss 3.5221\n",
      "iter 7800: loss 1.0677, time 116.66ms, mfu 0.02%\n",
      "iter 7900: loss 1.1014, time 46.23ms, mfu 0.02%\n",
      "step 8000: train loss 3.4565, val loss 3.4952\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625\n",
      "iter 8000: loss 1.0357, time 1182.84ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.0625\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.04\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.2732, time 3836.84ms, mfu -100.00%\n",
      "iter 100: loss 2.0749, time 44.77ms, mfu 0.03%\n",
      "iter 200: loss 1.8805, time 77.80ms, mfu 0.03%\n",
      "step 250: train loss 4.3147, val loss 4.3174\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 300: loss 1.8799, time 77.01ms, mfu 0.03%\n",
      "iter 400: loss 1.8393, time 100.94ms, mfu 0.03%\n",
      "step 500: train loss 4.3262, val loss 4.3302\n",
      "iter 500: loss 1.6612, time 887.13ms, mfu 0.02%\n",
      "iter 600: loss 1.6494, time 72.65ms, mfu 0.02%\n",
      "iter 700: loss 1.6057, time 42.24ms, mfu 0.02%\n",
      "step 750: train loss 4.2831, val loss 4.2834\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 800: loss 1.6440, time 48.17ms, mfu 0.02%\n",
      "iter 900: loss 1.4951, time 46.04ms, mfu 0.02%\n",
      "step 1000: train loss 4.2577, val loss 4.2644\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 1000: loss 1.5556, time 945.87ms, mfu 0.02%\n",
      "iter 1100: loss 1.5072, time 58.06ms, mfu 0.02%\n",
      "iter 1200: loss 1.4848, time 226.20ms, mfu 0.02%\n",
      "step 1250: train loss 4.1994, val loss 4.2184\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 1300: loss 1.4022, time 103.15ms, mfu 0.02%\n",
      "iter 1400: loss 1.3611, time 49.04ms, mfu 0.02%\n",
      "step 1500: train loss 4.1702, val loss 4.1833\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 1500: loss 1.4035, time 912.21ms, mfu 0.02%\n",
      "iter 1600: loss 1.3951, time 101.20ms, mfu 0.02%\n",
      "iter 1700: loss 1.3088, time 79.64ms, mfu 0.02%\n",
      "step 1750: train loss 4.1582, val loss 4.1792\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 1800: loss 1.3593, time 69.62ms, mfu 0.02%\n",
      "iter 1900: loss 1.3377, time 87.74ms, mfu 0.02%\n",
      "step 2000: train loss 4.1671, val loss 4.1926\n",
      "iter 2000: loss 1.2495, time 1003.42ms, mfu 0.02%\n",
      "iter 2100: loss 1.3163, time 126.34ms, mfu 0.02%\n",
      "iter 2200: loss 1.3446, time 44.78ms, mfu 0.02%\n",
      "step 2250: train loss 4.1708, val loss 4.1852\n",
      "iter 2300: loss 1.2933, time 73.56ms, mfu 0.02%\n",
      "iter 2400: loss 1.2852, time 104.86ms, mfu 0.02%\n",
      "step 2500: train loss 4.1454, val loss 4.1567\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04\n",
      "iter 2500: loss 1.2712, time 850.48ms, mfu 0.01%\n",
      "iter 2600: loss 1.2627, time 54.80ms, mfu 0.02%\n",
      "iter 2700: loss 1.1841, time 152.07ms, mfu 0.01%\n",
      "step 2750: train loss 4.1874, val loss 4.2053\n",
      "iter 2800: loss 1.2995, time 85.28ms, mfu 0.01%\n",
      "iter 2900: loss 1.2874, time 65.91ms, mfu 0.02%\n",
      "step 3000: train loss 4.1875, val loss 4.2023\n",
      "iter 3000: loss 1.2846, time 1299.53ms, mfu 0.01%\n",
      "iter 3100: loss 1.3030, time 47.67ms, mfu 0.02%\n",
      "iter 3200: loss 1.2639, time 60.07ms, mfu 0.02%\n",
      "step 3250: train loss 4.2370, val loss 4.2641\n",
      "iter 3300: loss 1.2380, time 41.99ms, mfu 0.02%\n",
      "iter 3400: loss 1.2477, time 85.62ms, mfu 0.02%\n",
      "step 3500: train loss 4.2348, val loss 4.2476\n",
      "iter 3500: loss 1.3517, time 884.89ms, mfu 0.02%\n",
      "iter 3600: loss 1.2488, time 71.98ms, mfu 0.02%\n",
      "iter 3700: loss 1.2396, time 45.92ms, mfu 0.02%\n",
      "step 3750: train loss 4.2456, val loss 4.2645\n",
      "iter 3800: loss 1.1918, time 63.62ms, mfu 0.02%\n",
      "iter 3900: loss 1.1137, time 56.63ms, mfu 0.02%\n",
      "step 4000: train loss 4.2647, val loss 4.2869\n",
      "iter 4000: loss 1.1569, time 828.89ms, mfu 0.02%\n",
      "iter 4100: loss 1.2375, time 43.23ms, mfu 0.02%\n",
      "iter 4200: loss 1.1329, time 79.60ms, mfu 0.02%\n",
      "step 4250: train loss 4.2814, val loss 4.3105\n",
      "iter 4300: loss 1.1720, time 108.63ms, mfu 0.02%\n",
      "iter 4400: loss 1.1828, time 61.92ms, mfu 0.02%\n",
      "step 4500: train loss 4.3026, val loss 4.3229\n",
      "iter 4500: loss 1.1478, time 979.72ms, mfu 0.02%\n",
      "iter 4600: loss 1.1982, time 110.86ms, mfu 0.02%\n",
      "iter 4700: loss 1.1246, time 81.68ms, mfu 0.02%\n",
      "step 4750: train loss 4.3116, val loss 4.3255\n",
      "iter 4800: loss 1.1542, time 60.07ms, mfu 0.02%\n",
      "iter 4900: loss 1.1914, time 86.79ms, mfu 0.02%\n",
      "step 5000: train loss 4.3104, val loss 4.3224\n",
      "iter 5000: loss 1.0786, time 743.71ms, mfu 0.01%\n",
      "iter 5100: loss 1.1054, time 44.15ms, mfu 0.02%\n",
      "iter 5200: loss 1.1065, time 40.39ms, mfu 0.02%\n",
      "step 5250: train loss 4.3117, val loss 4.3380\n",
      "iter 5300: loss 1.1559, time 778.17ms, mfu 0.02%\n",
      "iter 5400: loss 1.0436, time 80.12ms, mfu 0.02%\n",
      "step 5500: train loss 4.3350, val loss 4.3454\n",
      "iter 5500: loss 1.1028, time 930.68ms, mfu 0.01%\n",
      "iter 5600: loss 1.0998, time 44.87ms, mfu 0.02%\n",
      "iter 5700: loss 1.0834, time 49.53ms, mfu 0.02%\n",
      "step 5750: train loss 4.3353, val loss 4.3536\n",
      "iter 5800: loss 1.1798, time 62.58ms, mfu 0.02%\n",
      "iter 5900: loss 1.1246, time 45.75ms, mfu 0.02%\n",
      "step 6000: train loss 4.3362, val loss 4.3394\n",
      "iter 6000: loss 1.0972, time 886.03ms, mfu 0.02%\n",
      "iter 6100: loss 1.1564, time 42.92ms, mfu 0.02%\n",
      "iter 6200: loss 1.0130, time 42.65ms, mfu 0.02%\n",
      "step 6250: train loss 4.3266, val loss 4.3378\n",
      "iter 6300: loss 1.0672, time 44.41ms, mfu 0.02%\n",
      "iter 6400: loss 1.1205, time 163.34ms, mfu 0.02%\n",
      "step 6500: train loss 4.3214, val loss 4.3378\n",
      "iter 6500: loss 1.0351, time 853.54ms, mfu 0.02%\n",
      "iter 6600: loss 1.0224, time 91.19ms, mfu 0.02%\n",
      "iter 6700: loss 1.0482, time 102.55ms, mfu 0.02%\n",
      "step 6750: train loss 4.3298, val loss 4.3430\n",
      "iter 6800: loss 1.0985, time 42.40ms, mfu 0.02%\n",
      "iter 6900: loss 1.1408, time 47.74ms, mfu 0.02%\n",
      "step 7000: train loss 4.3138, val loss 4.3394\n",
      "iter 7000: loss 1.0836, time 1100.31ms, mfu 0.02%\n",
      "iter 7100: loss 1.1557, time 80.98ms, mfu 0.02%\n",
      "iter 7200: loss 1.0613, time 78.09ms, mfu 0.02%\n",
      "step 7250: train loss 4.3307, val loss 4.3372\n",
      "iter 7300: loss 1.0222, time 153.84ms, mfu 0.02%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7400: loss 1.0080, time 83.77ms, mfu 0.02%\n",
      "step 7500: train loss 4.3314, val loss 4.3388\n",
      "iter 7500: loss 1.1231, time 716.09ms, mfu 0.01%\n",
      "iter 7600: loss 1.0545, time 77.41ms, mfu 0.01%\n",
      "iter 7700: loss 1.1282, time 63.15ms, mfu 0.02%\n",
      "step 7750: train loss 4.3090, val loss 4.3332\n",
      "iter 7800: loss 1.0165, time 44.13ms, mfu 0.02%\n",
      "iter 7900: loss 1.0453, time 61.42ms, mfu 0.02%\n",
      "step 8000: train loss 4.3132, val loss 4.3188\n",
      "iter 8000: loss 0.9926, time 1113.71ms, mfu 0.02%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.04\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.02\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.02\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.1492, time 1123.78ms, mfu -100.00%\n",
      "iter 100: loss 1.9746, time 107.36ms, mfu 0.01%\n",
      "iter 200: loss 1.8374, time 83.00ms, mfu 0.01%\n",
      "step 250: train loss 4.5117, val loss 4.5137\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.02\n",
      "iter 300: loss 1.9339, time 75.68ms, mfu 0.01%\n",
      "iter 400: loss 1.8483, time 51.63ms, mfu 0.01%\n",
      "step 500: train loss 4.5492, val loss 4.5514\n",
      "iter 500: loss 1.7146, time 768.22ms, mfu 0.01%\n",
      "iter 600: loss 1.6229, time 44.00ms, mfu 0.01%\n",
      "iter 700: loss 1.6137, time 45.58ms, mfu 0.02%\n",
      "step 750: train loss 4.5547, val loss 4.5567\n",
      "iter 800: loss 1.6694, time 83.80ms, mfu 0.02%\n",
      "iter 900: loss 1.5126, time 44.79ms, mfu 0.02%\n",
      "step 1000: train loss 4.5591, val loss 4.5620\n",
      "iter 1000: loss 1.5440, time 1184.07ms, mfu 0.02%\n",
      "iter 1100: loss 1.5011, time 61.52ms, mfu 0.02%\n",
      "iter 1200: loss 1.4758, time 48.27ms, mfu 0.02%\n",
      "step 1250: train loss 4.5507, val loss 4.5600\n",
      "iter 1300: loss 1.3892, time 58.15ms, mfu 0.02%\n",
      "iter 1400: loss 1.3317, time 58.28ms, mfu 0.02%\n",
      "step 1500: train loss 4.5490, val loss 4.5543\n",
      "iter 1500: loss 1.3547, time 815.38ms, mfu 0.02%\n",
      "iter 1600: loss 1.3915, time 94.96ms, mfu 0.02%\n",
      "iter 1700: loss 1.2941, time 50.10ms, mfu 0.02%\n",
      "step 1750: train loss 4.5552, val loss 4.5631\n",
      "iter 1800: loss 1.3081, time 86.80ms, mfu 0.02%\n",
      "iter 1900: loss 1.3580, time 93.26ms, mfu 0.02%\n",
      "step 2000: train loss 4.5596, val loss 4.5700\n",
      "iter 2000: loss 1.2625, time 1124.21ms, mfu 0.01%\n",
      "iter 2100: loss 1.2669, time 47.73ms, mfu 0.02%\n",
      "iter 2200: loss 1.3290, time 46.19ms, mfu 0.02%\n",
      "step 2250: train loss 4.5782, val loss 4.5819\n",
      "iter 2300: loss 1.2794, time 43.79ms, mfu 0.02%\n",
      "iter 2400: loss 1.2908, time 53.46ms, mfu 0.02%\n",
      "step 2500: train loss 4.5982, val loss 4.6022\n",
      "iter 2500: loss 1.2808, time 1321.24ms, mfu 0.02%\n",
      "iter 2600: loss 1.2639, time 84.90ms, mfu 0.02%\n",
      "iter 2700: loss 1.1498, time 46.09ms, mfu 0.02%\n",
      "step 2750: train loss 4.6305, val loss 4.6372\n",
      "iter 2800: loss 1.2605, time 109.76ms, mfu 0.02%\n",
      "iter 2900: loss 1.2678, time 69.34ms, mfu 0.02%\n",
      "step 3000: train loss 4.6468, val loss 4.6511\n",
      "iter 3000: loss 1.3444, time 1237.16ms, mfu 0.02%\n",
      "iter 3100: loss 1.2951, time 63.00ms, mfu 0.02%\n",
      "iter 3200: loss 1.2864, time 102.09ms, mfu 0.02%\n",
      "step 3250: train loss 4.6727, val loss 4.6788\n",
      "iter 3300: loss 1.2564, time 47.09ms, mfu 0.02%\n",
      "iter 3400: loss 1.2701, time 101.90ms, mfu 0.02%\n",
      "step 3500: train loss 4.6934, val loss 4.6970\n",
      "iter 3500: loss 1.3073, time 982.38ms, mfu 0.02%\n",
      "iter 3600: loss 1.2328, time 87.33ms, mfu 0.02%\n",
      "iter 3700: loss 1.3369, time 56.08ms, mfu 0.02%\n",
      "step 3750: train loss 4.7304, val loss 4.7334\n",
      "iter 3800: loss 1.2339, time 50.07ms, mfu 0.02%\n",
      "iter 3900: loss 1.1642, time 108.70ms, mfu 0.02%\n",
      "step 4000: train loss 4.7625, val loss 4.7646\n",
      "iter 4000: loss 1.6951, time 1555.10ms, mfu 0.01%\n",
      "iter 4100: loss 1.6920, time 97.47ms, mfu 0.01%\n",
      "iter 4200: loss 1.3593, time 46.89ms, mfu 0.02%\n",
      "step 4250: train loss 4.7554, val loss 4.7576\n",
      "iter 4300: loss 1.2965, time 220.68ms, mfu 0.01%\n",
      "iter 4400: loss 1.2361, time 121.31ms, mfu 0.01%\n",
      "step 4500: train loss 4.7586, val loss 4.7610\n",
      "iter 4500: loss 1.2104, time 993.31ms, mfu 0.01%\n",
      "iter 4600: loss 1.2612, time 85.60ms, mfu 0.01%\n",
      "iter 4700: loss 1.1329, time 85.89ms, mfu 0.01%\n",
      "step 4750: train loss 4.7708, val loss 4.7712\n",
      "iter 4800: loss 1.1699, time 45.05ms, mfu 0.02%\n",
      "iter 4900: loss 1.2276, time 60.90ms, mfu 0.02%\n",
      "step 5000: train loss 4.7790, val loss 4.7791\n",
      "iter 5000: loss 1.1134, time 1052.25ms, mfu 0.01%\n",
      "iter 5100: loss 1.1462, time 59.18ms, mfu 0.01%\n",
      "iter 5200: loss 1.1318, time 44.40ms, mfu 0.02%\n",
      "step 5250: train loss 4.7859, val loss 4.7881\n",
      "iter 5300: loss 1.1529, time 119.26ms, mfu 0.02%\n",
      "iter 5400: loss 1.0817, time 64.13ms, mfu 0.02%\n",
      "step 5500: train loss 4.7902, val loss 4.7902\n",
      "iter 5500: loss 1.1159, time 1155.85ms, mfu 0.01%\n",
      "iter 5600: loss 1.1232, time 107.98ms, mfu 0.01%\n",
      "iter 5700: loss 1.0993, time 48.29ms, mfu 0.02%\n",
      "step 5750: train loss 4.7943, val loss 4.7951\n",
      "iter 5800: loss 1.2016, time 270.44ms, mfu 0.01%\n",
      "iter 5900: loss 1.1530, time 282.08ms, mfu 0.01%\n",
      "step 6000: train loss 4.7950, val loss 4.7933\n",
      "iter 6000: loss 1.1248, time 2709.20ms, mfu 0.01%\n",
      "iter 6100: loss 1.1643, time 56.93ms, mfu 0.01%\n",
      "iter 6200: loss 1.0634, time 73.07ms, mfu 0.01%\n",
      "step 6250: train loss 4.7984, val loss 4.7988\n",
      "iter 6300: loss 1.0819, time 82.20ms, mfu 0.01%\n",
      "iter 6400: loss 1.1185, time 269.10ms, mfu 0.01%\n",
      "step 6500: train loss 4.7951, val loss 4.7956\n",
      "iter 6500: loss 1.0385, time 995.20ms, mfu 0.01%\n",
      "iter 6600: loss 1.0209, time 52.59ms, mfu 0.01%\n",
      "iter 6700: loss 1.0678, time 153.02ms, mfu 0.01%\n",
      "step 6750: train loss 4.7992, val loss 4.7993\n",
      "iter 6800: loss 1.1241, time 116.06ms, mfu 0.01%\n",
      "iter 6900: loss 1.1719, time 84.85ms, mfu 0.01%\n",
      "step 7000: train loss 4.7976, val loss 4.7993\n",
      "iter 7000: loss 1.0919, time 1581.13ms, mfu 0.01%\n",
      "iter 7100: loss 1.1628, time 122.63ms, mfu 0.01%\n",
      "iter 7200: loss 1.0871, time 126.89ms, mfu 0.01%\n",
      "step 7250: train loss 4.8012, val loss 4.8003\n",
      "iter 7300: loss 1.0813, time 109.13ms, mfu 0.01%\n",
      "iter 7400: loss 1.0599, time 112.06ms, mfu 0.01%\n",
      "step 7500: train loss 4.8017, val loss 4.8007\n",
      "iter 7500: loss 1.1177, time 986.27ms, mfu 0.01%\n",
      "iter 7600: loss 1.0869, time 199.62ms, mfu 0.01%\n",
      "iter 7700: loss 1.1661, time 235.33ms, mfu 0.01%\n",
      "step 7750: train loss 4.7997, val loss 4.8001\n",
      "iter 7800: loss 1.0531, time 538.15ms, mfu 0.01%\n",
      "iter 7900: loss 1.1074, time 128.98ms, mfu 0.01%\n",
      "step 8000: train loss 4.8021, val loss 4.8027\n",
      "iter 8000: loss 1.0110, time 1935.67ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.02\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.01\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.01\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8628, val loss 4.8586\n",
      "iter 0: loss 4.0765, time 2879.18ms, mfu -100.00%\n",
      "iter 100: loss 1.9926, time 114.19ms, mfu 0.01%\n",
      "iter 200: loss 1.7851, time 107.02ms, mfu 0.01%\n",
      "step 250: train loss 4.6095, val loss 4.6114\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.01\n",
      "iter 300: loss 1.8206, time 115.41ms, mfu 0.01%\n",
      "iter 400: loss 1.8338, time 88.52ms, mfu 0.01%\n",
      "step 500: train loss 4.6917, val loss 4.6929\n",
      "iter 500: loss 1.8017, time 1276.40ms, mfu 0.01%\n",
      "iter 600: loss 1.7091, time 74.55ms, mfu 0.01%\n",
      "iter 700: loss 1.6217, time 132.79ms, mfu 0.01%\n",
      "step 750: train loss 4.6874, val loss 4.6889\n",
      "iter 800: loss 1.7205, time 354.22ms, mfu 0.01%\n",
      "iter 900: loss 1.5156, time 104.62ms, mfu 0.01%\n",
      "step 1000: train loss 4.6995, val loss 4.7008\n",
      "iter 1000: loss 1.5499, time 1251.06ms, mfu 0.01%\n",
      "iter 1100: loss 1.5218, time 501.07ms, mfu 0.01%\n",
      "iter 1200: loss 1.4696, time 513.65ms, mfu 0.01%\n",
      "step 1250: train loss 4.7070, val loss 4.7095\n",
      "iter 1300: loss 1.4402, time 67.42ms, mfu 0.01%\n",
      "iter 1400: loss 1.3669, time 528.29ms, mfu 0.01%\n",
      "step 1500: train loss 4.7033, val loss 4.7040\n",
      "iter 1500: loss 1.3198, time 2851.38ms, mfu 0.01%\n",
      "iter 1600: loss 1.4111, time 115.44ms, mfu 0.01%\n",
      "iter 1700: loss 1.3269, time 95.99ms, mfu 0.01%\n",
      "step 1750: train loss 4.7107, val loss 4.7127\n",
      "iter 1800: loss 1.3249, time 77.07ms, mfu 0.01%\n",
      "iter 1900: loss 1.3437, time 118.72ms, mfu 0.01%\n",
      "step 2000: train loss 4.7008, val loss 4.7047\n",
      "iter 2000: loss 1.2157, time 1120.49ms, mfu 0.01%\n",
      "iter 2100: loss 1.2754, time 47.07ms, mfu 0.01%\n",
      "iter 2200: loss 1.3385, time 59.90ms, mfu 0.01%\n",
      "step 2250: train loss 4.7336, val loss 4.7357\n",
      "iter 2300: loss 1.3180, time 106.41ms, mfu 0.01%\n",
      "iter 2400: loss 1.2753, time 123.43ms, mfu 0.01%\n",
      "step 2500: train loss 4.7343, val loss 4.7358\n",
      "iter 2500: loss 1.2912, time 711.18ms, mfu 0.01%\n",
      "iter 2600: loss 1.2740, time 85.73ms, mfu 0.01%\n",
      "iter 2700: loss 1.1131, time 140.60ms, mfu 0.01%\n",
      "step 2750: train loss 4.7453, val loss 4.7479\n",
      "iter 2800: loss 1.2624, time 115.87ms, mfu 0.01%\n",
      "iter 2900: loss 1.1846, time 239.41ms, mfu 0.01%\n",
      "step 3000: train loss 4.7930, val loss 4.7950\n",
      "iter 3000: loss 1.8771, time 1110.64ms, mfu 0.01%\n",
      "iter 3100: loss 1.8155, time 97.96ms, mfu 0.01%\n",
      "iter 3200: loss 1.7093, time 45.21ms, mfu 0.01%\n",
      "step 3250: train loss 4.7715, val loss 4.7725\n",
      "iter 3300: loss 1.8564, time 65.47ms, mfu 0.01%\n",
      "iter 3400: loss 1.7808, time 44.02ms, mfu 0.01%\n",
      "step 3500: train loss 4.7867, val loss 4.7861\n",
      "iter 3500: loss 1.7753, time 671.36ms, mfu 0.01%\n",
      "iter 3600: loss 1.6825, time 127.15ms, mfu 0.01%\n",
      "iter 3700: loss 1.6160, time 72.61ms, mfu 0.01%\n",
      "step 3750: train loss 4.8035, val loss 4.8036\n",
      "iter 3800: loss 1.5742, time 76.87ms, mfu 0.01%\n",
      "iter 3900: loss 1.4877, time 78.48ms, mfu 0.01%\n",
      "step 4000: train loss 4.8035, val loss 4.8040\n",
      "iter 4000: loss 1.4745, time 1300.41ms, mfu 0.01%\n",
      "iter 4100: loss 1.5311, time 74.29ms, mfu 0.01%\n",
      "iter 4200: loss 1.8028, time 101.35ms, mfu 0.01%\n",
      "step 4250: train loss 4.8067, val loss 4.8073\n",
      "iter 4300: loss 1.7347, time 100.81ms, mfu 0.01%\n",
      "iter 4400: loss 1.6159, time 53.00ms, mfu 0.01%\n",
      "step 4500: train loss 4.8114, val loss 4.8115\n",
      "iter 4500: loss 1.6256, time 977.53ms, mfu 0.01%\n",
      "iter 4600: loss 1.6350, time 51.79ms, mfu 0.01%\n",
      "iter 4700: loss 1.5190, time 46.83ms, mfu 0.02%\n",
      "step 4750: train loss 4.8164, val loss 4.8166\n",
      "iter 4800: loss 1.5214, time 98.41ms, mfu 0.02%\n",
      "iter 4900: loss 1.5038, time 61.47ms, mfu 0.02%\n",
      "step 5000: train loss 4.8206, val loss 4.8213\n",
      "iter 5000: loss 1.4258, time 1254.24ms, mfu 0.01%\n",
      "iter 5100: loss 1.4111, time 89.51ms, mfu 0.01%\n",
      "iter 5200: loss 1.4598, time 68.75ms, mfu 0.01%\n",
      "step 5250: train loss 4.8251, val loss 4.8261\n",
      "iter 5300: loss 1.5178, time 86.89ms, mfu 0.01%\n",
      "iter 5400: loss 1.5219, time 45.89ms, mfu 0.02%\n",
      "step 5500: train loss 4.8198, val loss 4.8200\n",
      "iter 5500: loss 1.4490, time 967.57ms, mfu 0.01%\n",
      "iter 5600: loss 1.4981, time 97.31ms, mfu 0.01%\n",
      "iter 5700: loss 1.4208, time 59.84ms, mfu 0.02%\n",
      "step 5750: train loss 4.8270, val loss 4.8271\n",
      "iter 5800: loss 1.5325, time 75.92ms, mfu 0.02%\n",
      "iter 5900: loss 1.4907, time 48.59ms, mfu 0.02%\n",
      "step 6000: train loss 4.8299, val loss 4.8301\n",
      "iter 6000: loss 1.5006, time 789.03ms, mfu 0.01%\n",
      "iter 6100: loss 1.4595, time 49.28ms, mfu 0.02%\n",
      "iter 6200: loss 1.4147, time 89.89ms, mfu 0.02%\n",
      "step 6250: train loss 4.8310, val loss 4.8315\n",
      "iter 6300: loss 1.4289, time 53.39ms, mfu 0.02%\n",
      "iter 6400: loss 1.4432, time 44.98ms, mfu 0.02%\n",
      "step 6500: train loss 4.8327, val loss 4.8331\n",
      "iter 6500: loss 1.3337, time 819.17ms, mfu 0.02%\n",
      "iter 6600: loss 1.3338, time 95.40ms, mfu 0.02%\n",
      "iter 6700: loss 1.4083, time 46.56ms, mfu 0.02%\n",
      "step 6750: train loss 4.8325, val loss 4.8329\n",
      "iter 6800: loss 1.3591, time 47.48ms, mfu 0.02%\n",
      "iter 6900: loss 1.4304, time 44.08ms, mfu 0.02%\n",
      "step 7000: train loss 4.8335, val loss 4.8338\n",
      "iter 7000: loss 1.3956, time 824.12ms, mfu 0.02%\n",
      "iter 7100: loss 1.4808, time 44.51ms, mfu 0.02%\n",
      "iter 7200: loss 1.3873, time 45.04ms, mfu 0.02%\n",
      "step 7250: train loss 4.8344, val loss 4.8348\n",
      "iter 7300: loss 1.2877, time 122.47ms, mfu 0.02%\n",
      "iter 7400: loss 1.3232, time 76.84ms, mfu 0.02%\n",
      "step 7500: train loss 4.8355, val loss 4.8355\n",
      "iter 7500: loss 1.4002, time 954.35ms, mfu 0.02%\n",
      "iter 7600: loss 1.3462, time 44.22ms, mfu 0.02%\n",
      "iter 7700: loss 1.4165, time 44.51ms, mfu 0.02%\n",
      "step 7750: train loss 4.8368, val loss 4.8373\n",
      "iter 7800: loss 1.3450, time 75.39ms, mfu 0.02%\n",
      "iter 7900: loss 1.3788, time 102.94ms, mfu 0.02%\n",
      "step 8000: train loss 4.8354, val loss 4.8359\n",
      "iter 8000: loss 1.3256, time 713.38ms, mfu 0.02%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.01\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [0.125, 0.0625, 0.04, 0.02, 0.01]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"3\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for proposed_margin_by_weight_type_3_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a622220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.5\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8613, time 1118.65ms, mfu -100.00%\n",
      "iter 100: loss 2.5186, time 118.83ms, mfu 0.01%\n",
      "iter 200: loss 2.4473, time 71.79ms, mfu 0.01%\n",
      "step 250: train loss 2.5728, val loss 2.5958\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 300: loss 2.2726, time 93.24ms, mfu 0.01%\n",
      "iter 400: loss 2.2140, time 109.72ms, mfu 0.01%\n",
      "step 500: train loss 2.3127, val loss 2.3351\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 500: loss 2.3262, time 1369.97ms, mfu 0.01%\n",
      "iter 600: loss 2.1560, time 273.89ms, mfu 0.01%\n",
      "iter 700: loss 2.0760, time 116.77ms, mfu 0.01%\n",
      "step 750: train loss 2.1620, val loss 2.1775\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 800: loss 2.1476, time 110.15ms, mfu 0.01%\n",
      "iter 900: loss 2.0201, time 96.65ms, mfu 0.01%\n",
      "step 1000: train loss 2.0390, val loss 2.1262\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 1000: loss 2.0378, time 1555.57ms, mfu 0.01%\n",
      "iter 1100: loss 1.8728, time 230.11ms, mfu 0.01%\n",
      "iter 1200: loss 1.8498, time 153.23ms, mfu 0.01%\n",
      "step 1250: train loss 1.9561, val loss 2.0366\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 1300: loss 1.8878, time 108.61ms, mfu 0.01%\n",
      "iter 1400: loss 1.9083, time 89.19ms, mfu 0.01%\n",
      "step 1500: train loss 1.8713, val loss 1.9895\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 1500: loss 1.8105, time 1160.34ms, mfu 0.01%\n",
      "iter 1600: loss 1.9442, time 93.05ms, mfu 0.01%\n",
      "iter 1700: loss 1.7532, time 77.80ms, mfu 0.01%\n",
      "step 1750: train loss 1.8210, val loss 1.9052\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 1800: loss 1.8659, time 73.85ms, mfu 0.01%\n",
      "iter 1900: loss 1.7335, time 234.96ms, mfu 0.01%\n",
      "step 2000: train loss 1.7860, val loss 1.9175\n",
      "iter 2000: loss 1.8688, time 1016.15ms, mfu 0.01%\n",
      "iter 2100: loss 1.7622, time 79.12ms, mfu 0.01%\n",
      "iter 2200: loss 1.7259, time 126.07ms, mfu 0.01%\n",
      "step 2250: train loss 1.7255, val loss 1.8799\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 2300: loss 1.7653, time 117.23ms, mfu 0.01%\n",
      "iter 2400: loss 1.6574, time 145.40ms, mfu 0.01%\n",
      "step 2500: train loss 1.7262, val loss 1.8354\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 2500: loss 1.7244, time 1103.04ms, mfu 0.01%\n",
      "iter 2600: loss 1.6326, time 104.87ms, mfu 0.01%\n",
      "iter 2700: loss 1.6339, time 110.54ms, mfu 0.01%\n",
      "step 2750: train loss 1.6886, val loss 1.8071\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 2800: loss 1.8539, time 115.64ms, mfu 0.01%\n",
      "iter 2900: loss 1.6029, time 115.76ms, mfu 0.01%\n",
      "step 3000: train loss 1.6934, val loss 1.8272\n",
      "iter 3000: loss 1.6161, time 1113.16ms, mfu 0.01%\n",
      "iter 3100: loss 1.5453, time 147.48ms, mfu 0.01%\n",
      "iter 3200: loss 1.5473, time 143.74ms, mfu 0.01%\n",
      "step 3250: train loss 1.6301, val loss 1.7581\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 3300: loss 1.6781, time 93.53ms, mfu 0.01%\n",
      "iter 3400: loss 1.5940, time 123.92ms, mfu 0.01%\n",
      "step 3500: train loss 1.6112, val loss 1.7749\n",
      "iter 3500: loss 1.5164, time 1204.59ms, mfu 0.01%\n",
      "iter 3600: loss 1.4839, time 120.97ms, mfu 0.01%\n",
      "iter 3700: loss 1.5340, time 119.29ms, mfu 0.01%\n",
      "step 3750: train loss 1.5805, val loss 1.7253\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 3800: loss 1.5864, time 94.46ms, mfu 0.01%\n",
      "iter 3900: loss 1.4681, time 116.51ms, mfu 0.01%\n",
      "step 4000: train loss 1.5509, val loss 1.7629\n",
      "iter 4000: loss 1.6738, time 1153.56ms, mfu 0.01%\n",
      "iter 4100: loss 1.4398, time 103.59ms, mfu 0.01%\n",
      "iter 4200: loss 1.3812, time 93.37ms, mfu 0.01%\n",
      "step 4250: train loss 1.5441, val loss 1.7420\n",
      "iter 4300: loss 1.4261, time 151.43ms, mfu 0.01%\n",
      "iter 4400: loss 1.6340, time 85.81ms, mfu 0.01%\n",
      "step 4500: train loss 1.5389, val loss 1.7403\n",
      "iter 4500: loss 1.5914, time 1122.54ms, mfu 0.01%\n",
      "iter 4600: loss 1.5941, time 85.03ms, mfu 0.01%\n",
      "iter 4700: loss 1.4834, time 114.56ms, mfu 0.01%\n",
      "step 4750: train loss 1.5257, val loss 1.7332\n",
      "iter 4800: loss 1.5304, time 109.18ms, mfu 0.01%\n",
      "iter 4900: loss 1.3875, time 126.01ms, mfu 0.01%\n",
      "step 5000: train loss 1.4792, val loss 1.6976\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 5000: loss 1.6696, time 1202.13ms, mfu 0.01%\n",
      "iter 5100: loss 1.4753, time 142.21ms, mfu 0.01%\n",
      "iter 5200: loss 1.4945, time 143.13ms, mfu 0.01%\n",
      "step 5250: train loss 1.4987, val loss 1.6638\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 5300: loss 1.2834, time 149.18ms, mfu 0.01%\n",
      "iter 5400: loss 1.4801, time 74.61ms, mfu 0.01%\n",
      "step 5500: train loss 1.5169, val loss 1.6379\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 5500: loss 1.5723, time 1432.06ms, mfu 0.01%\n",
      "iter 5600: loss 1.5225, time 105.45ms, mfu 0.01%\n",
      "iter 5700: loss 1.3773, time 118.84ms, mfu 0.01%\n",
      "step 5750: train loss 1.4516, val loss 1.6300\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 5800: loss 1.4170, time 109.39ms, mfu 0.01%\n",
      "iter 5900: loss 1.4733, time 108.55ms, mfu 0.01%\n",
      "step 6000: train loss 1.4357, val loss 1.6490\n",
      "iter 6000: loss 1.5138, time 1488.67ms, mfu 0.01%\n",
      "iter 6100: loss 1.5287, time 110.62ms, mfu 0.01%\n",
      "iter 6200: loss 1.3980, time 151.98ms, mfu 0.01%\n",
      "step 6250: train loss 1.4570, val loss 1.6117\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.5_SE\n",
      "iter 6300: loss 1.5488, time 139.99ms, mfu 0.01%\n",
      "iter 6400: loss 1.3160, time 129.19ms, mfu 0.01%\n",
      "step 6500: train loss 1.4420, val loss 1.6301\n",
      "iter 6500: loss 1.4105, time 1249.63ms, mfu 0.01%\n",
      "iter 6600: loss 1.5294, time 75.92ms, mfu 0.01%\n",
      "iter 6700: loss 1.4206, time 78.53ms, mfu 0.01%\n",
      "step 6750: train loss 1.4369, val loss 1.6194\n",
      "iter 6800: loss 1.4477, time 91.88ms, mfu 0.01%\n",
      "iter 6900: loss 1.4217, time 76.38ms, mfu 0.01%\n",
      "step 7000: train loss 1.4059, val loss 1.6354\n",
      "iter 7000: loss 1.5511, time 1137.74ms, mfu 0.01%\n",
      "iter 7100: loss 1.4640, time 135.32ms, mfu 0.01%\n",
      "iter 7200: loss 1.3542, time 82.02ms, mfu 0.01%\n",
      "step 7250: train loss 1.4013, val loss 1.6205\n",
      "iter 7300: loss 1.4579, time 92.65ms, mfu 0.01%\n",
      "iter 7400: loss 1.4839, time 146.76ms, mfu 0.01%\n",
      "step 7500: train loss 1.3832, val loss 1.6155\n",
      "iter 7500: loss 1.4161, time 1321.38ms, mfu 0.01%\n",
      "iter 7600: loss 1.3781, time 169.15ms, mfu 0.01%\n",
      "iter 7700: loss 1.3594, time 130.38ms, mfu 0.01%\n",
      "step 7750: train loss 1.4169, val loss 1.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7800: loss 1.4859, time 72.67ms, mfu 0.01%\n",
      "iter 7900: loss 1.4124, time 97.95ms, mfu 0.01%\n",
      "step 8000: train loss 1.3864, val loss 1.6266\n",
      "iter 8000: loss 1.4427, time 1208.92ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.5\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.25\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.7770, time 1659.29ms, mfu -100.00%\n",
      "iter 100: loss 2.2692, time 106.29ms, mfu 0.01%\n",
      "iter 200: loss 2.2203, time 81.44ms, mfu 0.01%\n",
      "step 250: train loss 3.0696, val loss 3.0921\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 300: loss 2.0482, time 83.99ms, mfu 0.01%\n",
      "iter 400: loss 2.0061, time 114.22ms, mfu 0.01%\n",
      "step 500: train loss 2.7714, val loss 2.7889\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 500: loss 2.0949, time 1534.97ms, mfu 0.01%\n",
      "iter 600: loss 1.9514, time 129.21ms, mfu 0.01%\n",
      "iter 700: loss 1.8813, time 96.89ms, mfu 0.01%\n",
      "step 750: train loss 2.5417, val loss 2.5665\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 800: loss 1.9691, time 204.12ms, mfu 0.01%\n",
      "iter 900: loss 1.8154, time 101.19ms, mfu 0.01%\n",
      "step 1000: train loss 2.3618, val loss 2.4300\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 1000: loss 1.8506, time 1247.84ms, mfu 0.01%\n",
      "iter 1100: loss 1.6605, time 83.89ms, mfu 0.01%\n",
      "iter 1200: loss 1.7315, time 112.31ms, mfu 0.01%\n",
      "step 1250: train loss 2.2218, val loss 2.2788\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 1300: loss 1.7112, time 86.49ms, mfu 0.01%\n",
      "iter 1400: loss 1.7273, time 130.24ms, mfu 0.01%\n",
      "step 1500: train loss 2.0878, val loss 2.1839\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 1500: loss 1.6396, time 1319.38ms, mfu 0.01%\n",
      "iter 1600: loss 1.7530, time 108.86ms, mfu 0.01%\n",
      "iter 1700: loss 1.5756, time 126.36ms, mfu 0.01%\n",
      "step 1750: train loss 2.0301, val loss 2.0908\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 1800: loss 1.6995, time 131.22ms, mfu 0.01%\n",
      "iter 1900: loss 1.5947, time 139.88ms, mfu 0.01%\n",
      "step 2000: train loss 1.9757, val loss 2.0734\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 2000: loss 1.7089, time 1212.16ms, mfu 0.01%\n",
      "iter 2100: loss 1.6128, time 141.35ms, mfu 0.01%\n",
      "iter 2200: loss 1.6154, time 76.32ms, mfu 0.01%\n",
      "step 2250: train loss 1.9003, val loss 2.0115\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 2300: loss 1.6363, time 88.39ms, mfu 0.01%\n",
      "iter 2400: loss 1.5389, time 131.45ms, mfu 0.01%\n",
      "step 2500: train loss 1.8991, val loss 1.9839\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 2500: loss 1.6230, time 1434.68ms, mfu 0.01%\n",
      "iter 2600: loss 1.4989, time 116.27ms, mfu 0.01%\n",
      "iter 2700: loss 1.4946, time 113.21ms, mfu 0.01%\n",
      "step 2750: train loss 1.8545, val loss 1.9446\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 2800: loss 1.7120, time 114.83ms, mfu 0.01%\n",
      "iter 2900: loss 1.5518, time 83.32ms, mfu 0.01%\n",
      "step 3000: train loss 1.8506, val loss 1.9492\n",
      "iter 3000: loss 1.5200, time 1157.19ms, mfu 0.01%\n",
      "iter 3100: loss 1.4284, time 111.58ms, mfu 0.01%\n",
      "iter 3200: loss 1.4844, time 104.56ms, mfu 0.01%\n",
      "step 3250: train loss 1.7823, val loss 1.8978\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 3300: loss 1.5599, time 140.63ms, mfu 0.01%\n",
      "iter 3400: loss 1.5071, time 107.63ms, mfu 0.01%\n",
      "step 3500: train loss 1.7514, val loss 1.8861\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 3500: loss 1.4042, time 1367.42ms, mfu 0.01%\n",
      "iter 3600: loss 1.4239, time 122.40ms, mfu 0.01%\n",
      "iter 3700: loss 1.4340, time 99.16ms, mfu 0.01%\n",
      "step 3750: train loss 1.7251, val loss 1.8504\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 3800: loss 1.4668, time 106.22ms, mfu 0.01%\n",
      "iter 3900: loss 1.3812, time 110.31ms, mfu 0.01%\n",
      "step 4000: train loss 1.6903, val loss 1.8733\n",
      "iter 4000: loss 1.5676, time 1117.31ms, mfu 0.01%\n",
      "iter 4100: loss 1.3649, time 110.37ms, mfu 0.01%\n",
      "iter 4200: loss 1.3053, time 117.06ms, mfu 0.01%\n",
      "step 4250: train loss 1.6887, val loss 1.8360\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 4300: loss 1.3710, time 96.36ms, mfu 0.01%\n",
      "iter 4400: loss 1.5388, time 125.58ms, mfu 0.01%\n",
      "step 4500: train loss 1.6798, val loss 1.8375\n",
      "iter 4500: loss 1.4741, time 1101.41ms, mfu 0.01%\n",
      "iter 4600: loss 1.4931, time 121.99ms, mfu 0.01%\n",
      "iter 4700: loss 1.4054, time 191.44ms, mfu 0.01%\n",
      "step 4750: train loss 1.6608, val loss 1.8212\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 4800: loss 1.4308, time 102.18ms, mfu 0.01%\n",
      "iter 4900: loss 1.2917, time 108.41ms, mfu 0.01%\n",
      "step 5000: train loss 1.6073, val loss 1.7851\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 5000: loss 1.5303, time 1447.75ms, mfu 0.01%\n",
      "iter 5100: loss 1.3933, time 108.94ms, mfu 0.01%\n",
      "iter 5200: loss 1.4034, time 90.02ms, mfu 0.01%\n",
      "step 5250: train loss 1.6189, val loss 1.7572\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 5300: loss 1.2132, time 116.79ms, mfu 0.01%\n",
      "iter 5400: loss 1.4214, time 121.17ms, mfu 0.01%\n",
      "step 5500: train loss 1.6421, val loss 1.7401\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 5500: loss 1.4827, time 1430.68ms, mfu 0.01%\n",
      "iter 5600: loss 1.4276, time 78.04ms, mfu 0.01%\n",
      "iter 5700: loss 1.3232, time 146.51ms, mfu 0.01%\n",
      "step 5750: train loss 1.5828, val loss 1.7316\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 5800: loss 1.3692, time 94.98ms, mfu 0.01%\n",
      "iter 5900: loss 1.3706, time 88.92ms, mfu 0.01%\n",
      "step 6000: train loss 1.5735, val loss 1.7405\n",
      "iter 6000: loss 1.4305, time 978.66ms, mfu 0.01%\n",
      "iter 6100: loss 1.4420, time 248.03ms, mfu 0.01%\n",
      "iter 6200: loss 1.3437, time 107.99ms, mfu 0.01%\n",
      "step 6250: train loss 1.5897, val loss 1.7148\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 6300: loss 1.4226, time 86.93ms, mfu 0.01%\n",
      "iter 6400: loss 1.2587, time 104.59ms, mfu 0.01%\n",
      "step 6500: train loss 1.5771, val loss 1.7247\n",
      "iter 6500: loss 1.3531, time 1315.10ms, mfu 0.01%\n",
      "iter 6600: loss 1.4392, time 88.90ms, mfu 0.01%\n",
      "iter 6700: loss 1.3468, time 95.10ms, mfu 0.01%\n",
      "step 6750: train loss 1.5673, val loss 1.7187\n",
      "iter 6800: loss 1.3587, time 101.20ms, mfu 0.01%\n",
      "iter 6900: loss 1.3542, time 157.29ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000: train loss 1.5455, val loss 1.7288\n",
      "iter 7000: loss 1.4590, time 1030.17ms, mfu 0.01%\n",
      "iter 7100: loss 1.3706, time 105.37ms, mfu 0.01%\n",
      "iter 7200: loss 1.2911, time 79.98ms, mfu 0.01%\n",
      "step 7250: train loss 1.5433, val loss 1.7216\n",
      "iter 7300: loss 1.4014, time 78.62ms, mfu 0.01%\n",
      "iter 7400: loss 1.3871, time 90.44ms, mfu 0.01%\n",
      "step 7500: train loss 1.5199, val loss 1.7072\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.25_SE\n",
      "iter 7500: loss 1.3460, time 1236.87ms, mfu 0.01%\n",
      "iter 7600: loss 1.3124, time 109.67ms, mfu 0.01%\n",
      "iter 7700: loss 1.2751, time 120.15ms, mfu 0.01%\n",
      "step 7750: train loss 1.5481, val loss 1.7143\n",
      "iter 7800: loss 1.3996, time 113.01ms, mfu 0.01%\n",
      "iter 7900: loss 1.3156, time 126.94ms, mfu 0.01%\n",
      "step 8000: train loss 1.5268, val loss 1.7158\n",
      "iter 8000: loss 1.3952, time 1488.81ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.25\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.125\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.6226, time 1425.04ms, mfu -100.00%\n",
      "iter 100: loss 2.0971, time 98.34ms, mfu 0.01%\n",
      "iter 200: loss 2.0035, time 128.73ms, mfu 0.01%\n",
      "step 250: train loss 3.7015, val loss 3.7139\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 300: loss 1.8128, time 89.95ms, mfu 0.01%\n",
      "iter 400: loss 1.8324, time 82.55ms, mfu 0.01%\n",
      "step 500: train loss 3.5127, val loss 3.5199\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 500: loss 1.8545, time 1563.73ms, mfu 0.01%\n",
      "iter 600: loss 1.7383, time 92.49ms, mfu 0.01%\n",
      "iter 700: loss 1.6754, time 113.45ms, mfu 0.01%\n",
      "step 750: train loss 3.3505, val loss 3.3599\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 800: loss 1.7357, time 91.95ms, mfu 0.01%\n",
      "iter 900: loss 1.6529, time 106.41ms, mfu 0.01%\n",
      "step 1000: train loss 3.1867, val loss 3.2221\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 1000: loss 1.6505, time 1403.21ms, mfu 0.01%\n",
      "iter 1100: loss 1.4676, time 119.01ms, mfu 0.01%\n",
      "iter 1200: loss 1.5398, time 79.41ms, mfu 0.01%\n",
      "step 1250: train loss 3.0662, val loss 3.0941\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 1300: loss 1.5324, time 125.81ms, mfu 0.01%\n",
      "iter 1400: loss 1.5083, time 91.12ms, mfu 0.01%\n",
      "step 1500: train loss 2.9586, val loss 3.0113\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 1500: loss 1.4797, time 1469.50ms, mfu 0.01%\n",
      "iter 1600: loss 1.5824, time 81.79ms, mfu 0.01%\n",
      "iter 1700: loss 1.4150, time 130.93ms, mfu 0.01%\n",
      "step 1750: train loss 2.8699, val loss 2.9006\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 1800: loss 1.4997, time 76.34ms, mfu 0.01%\n",
      "iter 1900: loss 1.4354, time 100.53ms, mfu 0.01%\n",
      "step 2000: train loss 2.8204, val loss 2.8799\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 2000: loss 1.5217, time 1629.05ms, mfu 0.01%\n",
      "iter 2100: loss 1.4074, time 124.78ms, mfu 0.01%\n",
      "iter 2200: loss 1.4411, time 134.12ms, mfu 0.01%\n",
      "step 2250: train loss 2.7362, val loss 2.8034\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 2300: loss 1.4283, time 107.47ms, mfu 0.01%\n",
      "iter 2400: loss 1.3724, time 129.25ms, mfu 0.01%\n",
      "step 2500: train loss 2.7015, val loss 2.7489\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 2500: loss 1.4262, time 1225.53ms, mfu 0.01%\n",
      "iter 2600: loss 1.3585, time 260.79ms, mfu 0.01%\n",
      "iter 2700: loss 1.3475, time 137.59ms, mfu 0.01%\n",
      "step 2750: train loss 2.6693, val loss 2.7166\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 2800: loss 1.5183, time 79.30ms, mfu 0.01%\n",
      "iter 2900: loss 1.3607, time 111.62ms, mfu 0.01%\n",
      "step 3000: train loss 2.6483, val loss 2.6947\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 3000: loss 1.3606, time 1502.13ms, mfu 0.01%\n",
      "iter 3100: loss 1.2858, time 115.22ms, mfu 0.01%\n",
      "iter 3200: loss 1.3510, time 112.65ms, mfu 0.01%\n",
      "step 3250: train loss 2.5929, val loss 2.6498\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 3300: loss 1.4141, time 162.28ms, mfu 0.01%\n",
      "iter 3400: loss 1.3561, time 77.90ms, mfu 0.01%\n",
      "step 3500: train loss 2.5680, val loss 2.6435\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 3500: loss 1.2374, time 1384.57ms, mfu 0.01%\n",
      "iter 3600: loss 1.2560, time 118.54ms, mfu 0.01%\n",
      "iter 3700: loss 1.2637, time 79.91ms, mfu 0.01%\n",
      "step 3750: train loss 2.5469, val loss 2.6184\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 3800: loss 1.3290, time 128.22ms, mfu 0.01%\n",
      "iter 3900: loss 1.2384, time 101.35ms, mfu 0.01%\n",
      "step 4000: train loss 2.5139, val loss 2.6161\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 4000: loss 1.4126, time 1262.33ms, mfu 0.01%\n",
      "iter 4100: loss 1.2130, time 74.90ms, mfu 0.01%\n",
      "iter 4200: loss 1.1898, time 93.05ms, mfu 0.01%\n",
      "step 4250: train loss 2.5014, val loss 2.5839\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 4300: loss 1.2162, time 90.32ms, mfu 0.01%\n",
      "iter 4400: loss 1.3759, time 93.36ms, mfu 0.01%\n",
      "step 4500: train loss 2.4851, val loss 2.5755\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 4500: loss 1.3190, time 1158.97ms, mfu 0.01%\n",
      "iter 4600: loss 1.3543, time 78.40ms, mfu 0.01%\n",
      "iter 4700: loss 1.2733, time 181.53ms, mfu 0.01%\n",
      "step 4750: train loss 2.4815, val loss 2.5774\n",
      "iter 4800: loss 1.2848, time 107.21ms, mfu 0.01%\n",
      "iter 4900: loss 1.1711, time 110.92ms, mfu 0.01%\n",
      "step 5000: train loss 2.4374, val loss 2.5390\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 5000: loss 1.3524, time 1523.45ms, mfu 0.01%\n",
      "iter 5100: loss 1.2835, time 82.71ms, mfu 0.01%\n",
      "iter 5200: loss 1.2368, time 89.77ms, mfu 0.01%\n",
      "step 5250: train loss 2.4512, val loss 2.5272\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 5300: loss 1.1115, time 110.01ms, mfu 0.01%\n",
      "iter 5400: loss 1.2902, time 86.65ms, mfu 0.01%\n",
      "step 5500: train loss 2.4489, val loss 2.4995\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 5500: loss 1.3045, time 1279.14ms, mfu 0.01%\n",
      "iter 5600: loss 1.2754, time 76.67ms, mfu 0.01%\n",
      "iter 5700: loss 1.1927, time 168.56ms, mfu 0.01%\n",
      "step 5750: train loss 2.4213, val loss 2.5035\n",
      "iter 5800: loss 1.2378, time 119.69ms, mfu 0.01%\n",
      "iter 5900: loss 1.2270, time 118.04ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: train loss 2.4098, val loss 2.5079\n",
      "iter 6000: loss 1.2615, time 1113.99ms, mfu 0.01%\n",
      "iter 6100: loss 1.2855, time 116.18ms, mfu 0.01%\n",
      "iter 6200: loss 1.1752, time 99.80ms, mfu 0.01%\n",
      "step 6250: train loss 2.4327, val loss 2.5011\n",
      "iter 6300: loss 1.2884, time 114.46ms, mfu 0.01%\n",
      "iter 6400: loss 1.1230, time 83.37ms, mfu 0.01%\n",
      "step 6500: train loss 2.4204, val loss 2.5035\n",
      "iter 6500: loss 1.2217, time 1212.44ms, mfu 0.01%\n",
      "iter 6600: loss 1.2868, time 128.14ms, mfu 0.01%\n",
      "iter 6700: loss 1.2118, time 108.29ms, mfu 0.01%\n",
      "step 6750: train loss 2.4096, val loss 2.4960\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 6800: loss 1.2324, time 115.49ms, mfu 0.01%\n",
      "iter 6900: loss 1.2133, time 118.54ms, mfu 0.01%\n",
      "step 7000: train loss 2.3942, val loss 2.4998\n",
      "iter 7000: loss 1.3134, time 1241.54ms, mfu 0.01%\n",
      "iter 7100: loss 1.2442, time 83.23ms, mfu 0.01%\n",
      "iter 7200: loss 1.1361, time 108.87ms, mfu 0.01%\n",
      "step 7250: train loss 2.3982, val loss 2.4966\n",
      "iter 7300: loss 1.2454, time 86.46ms, mfu 0.01%\n",
      "iter 7400: loss 1.2570, time 115.57ms, mfu 0.01%\n",
      "step 7500: train loss 2.3842, val loss 2.4861\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.125_SE\n",
      "iter 7500: loss 1.2317, time 1298.95ms, mfu 0.01%\n",
      "iter 7600: loss 1.1835, time 120.17ms, mfu 0.01%\n",
      "iter 7700: loss 1.1419, time 83.70ms, mfu 0.01%\n",
      "step 7750: train loss 2.3973, val loss 2.4929\n",
      "iter 7800: loss 1.2689, time 82.90ms, mfu 0.01%\n",
      "iter 7900: loss 1.1863, time 102.25ms, mfu 0.01%\n",
      "step 8000: train loss 2.3808, val loss 2.4994\n",
      "iter 8000: loss 1.2364, time 1366.34ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.125\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.0625\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.4165, time 1654.87ms, mfu -100.00%\n",
      "iter 100: loss 1.9108, time 123.60ms, mfu 0.01%\n",
      "iter 200: loss 1.9486, time 108.61ms, mfu 0.01%\n",
      "step 250: train loss 4.1096, val loss 4.1117\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 300: loss 1.7483, time 101.02ms, mfu 0.01%\n",
      "iter 400: loss 1.7919, time 82.03ms, mfu 0.01%\n",
      "step 500: train loss 4.0590, val loss 4.0607\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 500: loss 1.7758, time 1166.96ms, mfu 0.01%\n",
      "iter 600: loss 1.6529, time 99.04ms, mfu 0.01%\n",
      "iter 700: loss 1.6048, time 148.65ms, mfu 0.01%\n",
      "step 750: train loss 3.9749, val loss 3.9782\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 800: loss 1.7002, time 111.61ms, mfu 0.01%\n",
      "iter 900: loss 1.5728, time 159.20ms, mfu 0.01%\n",
      "step 1000: train loss 3.8710, val loss 3.8907\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 1000: loss 1.5883, time 1273.30ms, mfu 0.01%\n",
      "iter 1100: loss 1.4087, time 154.74ms, mfu 0.01%\n",
      "iter 1200: loss 1.4845, time 108.63ms, mfu 0.01%\n",
      "step 1250: train loss 3.8074, val loss 3.8223\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 1300: loss 1.4472, time 83.04ms, mfu 0.01%\n",
      "iter 1400: loss 1.4278, time 116.24ms, mfu 0.01%\n",
      "step 1500: train loss 3.7621, val loss 3.7897\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 1500: loss 1.3640, time 1380.43ms, mfu 0.01%\n",
      "iter 1600: loss 1.5267, time 205.23ms, mfu 0.01%\n",
      "iter 1700: loss 1.3181, time 89.39ms, mfu 0.01%\n",
      "step 1750: train loss 3.6942, val loss 3.6943\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 1800: loss 1.4210, time 75.54ms, mfu 0.01%\n",
      "iter 1900: loss 1.3434, time 108.45ms, mfu 0.01%\n",
      "step 2000: train loss 3.6660, val loss 3.6892\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 2000: loss 1.4278, time 1354.81ms, mfu 0.01%\n",
      "iter 2100: loss 1.3684, time 99.24ms, mfu 0.01%\n",
      "iter 2200: loss 1.3221, time 139.25ms, mfu 0.01%\n",
      "step 2250: train loss 3.6229, val loss 3.6508\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 2300: loss 1.3185, time 86.67ms, mfu 0.01%\n",
      "iter 2400: loss 1.2407, time 97.52ms, mfu 0.01%\n",
      "step 2500: train loss 3.5981, val loss 3.6128\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 2500: loss 1.2822, time 1234.84ms, mfu 0.01%\n",
      "iter 2600: loss 1.2531, time 152.64ms, mfu 0.01%\n",
      "iter 2700: loss 1.2272, time 129.40ms, mfu 0.01%\n",
      "step 2750: train loss 3.5687, val loss 3.5999\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 2800: loss 1.3941, time 86.87ms, mfu 0.01%\n",
      "iter 2900: loss 1.2937, time 83.95ms, mfu 0.01%\n",
      "step 3000: train loss 3.5602, val loss 3.5776\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 3000: loss 1.2520, time 1374.98ms, mfu 0.01%\n",
      "iter 3100: loss 1.1984, time 89.13ms, mfu 0.01%\n",
      "iter 3200: loss 1.2395, time 91.14ms, mfu 0.01%\n",
      "step 3250: train loss 3.5560, val loss 3.5865\n",
      "iter 3300: loss 1.3024, time 160.35ms, mfu 0.01%\n",
      "iter 3400: loss 1.2618, time 86.29ms, mfu 0.01%\n",
      "step 3500: train loss 3.5527, val loss 3.6001\n",
      "iter 3500: loss 1.1915, time 1204.59ms, mfu 0.01%\n",
      "iter 3600: loss 1.1610, time 101.14ms, mfu 0.01%\n",
      "iter 3700: loss 1.1671, time 139.38ms, mfu 0.01%\n",
      "step 3750: train loss 3.5324, val loss 3.5685\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 3800: loss 1.2340, time 149.90ms, mfu 0.01%\n",
      "iter 3900: loss 1.1190, time 103.56ms, mfu 0.01%\n",
      "step 4000: train loss 3.5107, val loss 3.5602\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 4000: loss 1.3098, time 1745.59ms, mfu 0.01%\n",
      "iter 4100: loss 1.1420, time 145.00ms, mfu 0.01%\n",
      "iter 4200: loss 1.0981, time 122.26ms, mfu 0.01%\n",
      "step 4250: train loss 3.5307, val loss 3.5659\n",
      "iter 4300: loss 1.1308, time 118.85ms, mfu 0.01%\n",
      "iter 4400: loss 1.2181, time 120.33ms, mfu 0.01%\n",
      "step 4500: train loss 3.5120, val loss 3.5573\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 4500: loss 1.1628, time 1253.34ms, mfu 0.01%\n",
      "iter 4600: loss 1.2619, time 104.04ms, mfu 0.01%\n",
      "iter 4700: loss 1.1590, time 84.55ms, mfu 0.01%\n",
      "step 4750: train loss 3.5320, val loss 3.5754\n",
      "iter 4800: loss 1.2148, time 151.07ms, mfu 0.01%\n",
      "iter 4900: loss 1.0858, time 130.51ms, mfu 0.01%\n",
      "step 5000: train loss 3.4619, val loss 3.5208\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 5000: loss 1.2292, time 1502.55ms, mfu 0.01%\n",
      "iter 5100: loss 1.1854, time 130.92ms, mfu 0.01%\n",
      "iter 5200: loss 1.1514, time 79.35ms, mfu 0.01%\n",
      "step 5250: train loss 3.4942, val loss 3.5427\n",
      "iter 5300: loss 1.0295, time 108.05ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5400: loss 1.1868, time 237.75ms, mfu 0.01%\n",
      "step 5500: train loss 3.4845, val loss 3.5117\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 5500: loss 1.2106, time 1642.52ms, mfu 0.01%\n",
      "iter 5600: loss 1.1611, time 142.48ms, mfu 0.01%\n",
      "iter 5700: loss 1.0938, time 136.15ms, mfu 0.01%\n",
      "step 5750: train loss 3.4710, val loss 3.5073\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 5800: loss 1.1334, time 124.79ms, mfu 0.01%\n",
      "iter 5900: loss 1.1062, time 125.47ms, mfu 0.01%\n",
      "step 6000: train loss 3.4587, val loss 3.5138\n",
      "iter 6000: loss 1.1016, time 1260.50ms, mfu 0.01%\n",
      "iter 6100: loss 1.1503, time 122.97ms, mfu 0.01%\n",
      "iter 6200: loss 1.1246, time 110.91ms, mfu 0.01%\n",
      "step 6250: train loss 3.4586, val loss 3.4937\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 6300: loss 1.1189, time 106.22ms, mfu 0.01%\n",
      "iter 6400: loss 1.0513, time 191.62ms, mfu 0.01%\n",
      "step 6500: train loss 3.4839, val loss 3.5191\n",
      "iter 6500: loss 1.1256, time 1434.75ms, mfu 0.01%\n",
      "iter 6600: loss 1.1398, time 75.56ms, mfu 0.01%\n",
      "iter 6700: loss 1.0784, time 75.52ms, mfu 0.01%\n",
      "step 6750: train loss 3.4669, val loss 3.5117\n",
      "iter 6800: loss 1.1311, time 116.02ms, mfu 0.01%\n",
      "iter 6900: loss 1.1176, time 80.09ms, mfu 0.01%\n",
      "step 7000: train loss 3.4352, val loss 3.4888\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 7000: loss 1.1447, time 1430.46ms, mfu 0.01%\n",
      "iter 7100: loss 1.1102, time 75.44ms, mfu 0.01%\n",
      "iter 7200: loss 1.0501, time 80.72ms, mfu 0.01%\n",
      "step 7250: train loss 3.4623, val loss 3.5100\n",
      "iter 7300: loss 1.1249, time 190.92ms, mfu 0.01%\n",
      "iter 7400: loss 1.1266, time 102.36ms, mfu 0.01%\n",
      "step 7500: train loss 3.4457, val loss 3.4990\n",
      "iter 7500: loss 1.1382, time 1170.97ms, mfu 0.01%\n",
      "iter 7600: loss 1.0913, time 121.57ms, mfu 0.01%\n",
      "iter 7700: loss 1.0696, time 105.50ms, mfu 0.01%\n",
      "step 7750: train loss 3.4342, val loss 3.4798\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 7800: loss 1.1496, time 101.27ms, mfu 0.01%\n",
      "iter 7900: loss 1.0922, time 226.51ms, mfu 0.01%\n",
      "step 8000: train loss 3.4158, val loss 3.4714\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE\n",
      "iter 8000: loss 1.1496, time 1345.43ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.0625\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [0.5, 0.25, 0.125, 0.0625]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}_SE\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"3\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for proposed_margin_by_weight_type_3_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16f2636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.04\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.3067, time 1872.78ms, mfu -100.00%\n",
      "iter 100: loss 1.8615, time 74.17ms, mfu 0.02%\n",
      "iter 200: loss 1.8596, time 61.28ms, mfu 0.02%\n",
      "step 250: train loss 4.3130, val loss 4.3143\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 300: loss 1.7374, time 132.53ms, mfu 0.02%\n",
      "iter 400: loss 1.7919, time 117.76ms, mfu 0.02%\n",
      "step 500: train loss 4.3037, val loss 4.3037\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 500: loss 1.8123, time 1041.08ms, mfu 0.01%\n",
      "iter 600: loss 1.6925, time 85.30ms, mfu 0.01%\n",
      "iter 700: loss 1.6311, time 157.82ms, mfu 0.01%\n",
      "step 750: train loss 4.2672, val loss 4.2663\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 800: loss 1.6717, time 96.55ms, mfu 0.01%\n",
      "iter 900: loss 1.5547, time 62.37ms, mfu 0.01%\n",
      "step 1000: train loss 4.2247, val loss 4.2364\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 1000: loss 1.5802, time 1075.72ms, mfu 0.01%\n",
      "iter 1100: loss 1.4217, time 76.86ms, mfu 0.01%\n",
      "iter 1200: loss 1.4574, time 75.03ms, mfu 0.01%\n",
      "step 1250: train loss 4.1586, val loss 4.1666\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 1300: loss 1.4226, time 91.34ms, mfu 0.01%\n",
      "iter 1400: loss 1.3907, time 77.72ms, mfu 0.01%\n",
      "step 1500: train loss 4.1407, val loss 4.1579\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 1500: loss 1.3394, time 1106.11ms, mfu 0.01%\n",
      "iter 1600: loss 1.4494, time 125.32ms, mfu 0.01%\n",
      "iter 1700: loss 1.2756, time 1580.63ms, mfu 0.01%\n",
      "step 1750: train loss 4.1304, val loss 4.1319\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 1800: loss 1.4011, time 100.54ms, mfu 0.01%\n",
      "iter 1900: loss 1.3247, time 121.76ms, mfu 0.01%\n",
      "step 2000: train loss 4.0865, val loss 4.1052\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.04_SE\n",
      "iter 2000: loss 1.3736, time 1598.99ms, mfu 0.01%\n",
      "iter 2100: loss 1.3431, time 126.27ms, mfu 0.01%\n",
      "iter 2200: loss 1.3196, time 139.21ms, mfu 0.01%\n",
      "step 2250: train loss 4.0898, val loss 4.1109\n",
      "iter 2300: loss 1.2868, time 123.80ms, mfu 0.01%\n",
      "iter 2400: loss 1.2312, time 156.28ms, mfu 0.01%\n",
      "step 2500: train loss 4.1004, val loss 4.1108\n",
      "iter 2500: loss 1.2554, time 1357.58ms, mfu 0.01%\n",
      "iter 2600: loss 1.2140, time 99.44ms, mfu 0.01%\n",
      "iter 2700: loss 1.2182, time 130.89ms, mfu 0.01%\n",
      "step 2750: train loss 4.0941, val loss 4.1138\n",
      "iter 2800: loss 1.3568, time 84.47ms, mfu 0.01%\n",
      "iter 2900: loss 1.2279, time 126.94ms, mfu 0.01%\n",
      "step 3000: train loss 4.1178, val loss 4.1265\n",
      "iter 3000: loss 1.2221, time 1321.19ms, mfu 0.01%\n",
      "iter 3100: loss 1.1396, time 130.74ms, mfu 0.01%\n",
      "iter 3200: loss 1.1938, time 88.55ms, mfu 0.01%\n",
      "step 3250: train loss 4.1095, val loss 4.1273\n",
      "iter 3300: loss 1.2785, time 520.00ms, mfu 0.01%\n",
      "iter 3400: loss 1.2185, time 110.46ms, mfu 0.01%\n",
      "step 3500: train loss 4.1113, val loss 4.1422\n",
      "iter 3500: loss 1.1472, time 1458.54ms, mfu 0.01%\n",
      "iter 3600: loss 1.1214, time 121.43ms, mfu 0.01%\n",
      "iter 3700: loss 1.1310, time 144.40ms, mfu 0.01%\n",
      "step 3750: train loss 4.1151, val loss 4.1335\n",
      "iter 3800: loss 1.1916, time 125.40ms, mfu 0.01%\n",
      "iter 3900: loss 1.1427, time 155.88ms, mfu 0.01%\n",
      "step 4000: train loss 4.1003, val loss 4.1359\n",
      "iter 4000: loss 1.2478, time 1227.56ms, mfu 0.01%\n",
      "iter 4100: loss 1.0970, time 128.96ms, mfu 0.01%\n",
      "iter 4200: loss 1.0635, time 141.43ms, mfu 0.01%\n",
      "step 4250: train loss 4.1399, val loss 4.1608\n",
      "iter 4300: loss 1.1087, time 89.82ms, mfu 0.01%\n",
      "iter 4400: loss 1.2013, time 134.18ms, mfu 0.01%\n",
      "step 4500: train loss 4.1389, val loss 4.1556\n",
      "iter 4500: loss 1.1502, time 1317.94ms, mfu 0.01%\n",
      "iter 4600: loss 1.2138, time 91.88ms, mfu 0.01%\n",
      "iter 4700: loss 1.1306, time 88.88ms, mfu 0.01%\n",
      "step 4750: train loss 4.1530, val loss 4.1709\n",
      "iter 4800: loss 1.1847, time 115.61ms, mfu 0.01%\n",
      "iter 4900: loss 1.0611, time 174.85ms, mfu 0.01%\n",
      "step 5000: train loss 4.1196, val loss 4.1487\n",
      "iter 5000: loss 1.1960, time 1409.41ms, mfu 0.01%\n",
      "iter 5100: loss 1.1225, time 90.90ms, mfu 0.01%\n",
      "iter 5200: loss 1.0937, time 94.93ms, mfu 0.01%\n",
      "step 5250: train loss 4.1256, val loss 4.1550\n",
      "iter 5300: loss 0.9916, time 201.02ms, mfu 0.01%\n",
      "iter 5400: loss 1.1553, time 90.12ms, mfu 0.01%\n",
      "step 5500: train loss 4.1174, val loss 4.1300\n",
      "iter 5500: loss 1.1647, time 1348.55ms, mfu 0.01%\n",
      "iter 5600: loss 1.1317, time 116.68ms, mfu 0.01%\n",
      "iter 5700: loss 1.0607, time 83.27ms, mfu 0.01%\n",
      "step 5750: train loss 4.1225, val loss 4.1378\n",
      "iter 5800: loss 1.0830, time 315.74ms, mfu 0.01%\n",
      "iter 5900: loss 1.0984, time 131.32ms, mfu 0.01%\n",
      "step 6000: train loss 4.1024, val loss 4.1352\n",
      "iter 6000: loss 1.0802, time 1156.59ms, mfu 0.01%\n",
      "iter 6100: loss 1.1363, time 90.89ms, mfu 0.01%\n",
      "iter 6200: loss 1.0735, time 91.09ms, mfu 0.01%\n",
      "step 6250: train loss 4.1112, val loss 4.1259\n",
      "iter 6300: loss 1.0944, time 166.02ms, mfu 0.01%\n",
      "iter 6400: loss 0.9938, time 124.73ms, mfu 0.01%\n",
      "step 6500: train loss 4.1315, val loss 4.1460\n",
      "iter 6500: loss 1.1014, time 1286.37ms, mfu 0.01%\n",
      "iter 6600: loss 1.1210, time 129.57ms, mfu 0.01%\n",
      "iter 6700: loss 1.0339, time 78.55ms, mfu 0.01%\n",
      "step 6750: train loss 4.1123, val loss 4.1371\n",
      "iter 6800: loss 1.0889, time 115.22ms, mfu 0.01%\n",
      "iter 6900: loss 1.0956, time 108.90ms, mfu 0.01%\n",
      "step 7000: train loss 4.1072, val loss 4.1355\n",
      "iter 7000: loss 1.0961, time 1570.22ms, mfu 0.01%\n",
      "iter 7100: loss 1.0913, time 111.10ms, mfu 0.01%\n",
      "iter 7200: loss 1.0027, time 100.05ms, mfu 0.01%\n",
      "step 7250: train loss 4.1042, val loss 4.1248\n",
      "iter 7300: loss 1.1250, time 133.57ms, mfu 0.01%\n",
      "iter 7400: loss 1.0940, time 148.62ms, mfu 0.01%\n",
      "step 7500: train loss 4.1043, val loss 4.1301\n",
      "iter 7500: loss 1.0913, time 1547.72ms, mfu 0.01%\n",
      "iter 7600: loss 1.0660, time 149.02ms, mfu 0.01%\n",
      "iter 7700: loss 1.0382, time 96.89ms, mfu 0.01%\n",
      "step 7750: train loss 4.1023, val loss 4.1217\n",
      "iter 7800: loss 1.1156, time 87.90ms, mfu 0.01%\n",
      "iter 7900: loss 1.0579, time 131.95ms, mfu 0.01%\n",
      "step 8000: train loss 4.0939, val loss 4.1158\n",
      "iter 8000: loss 1.1008, time 1176.51ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.04\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.02_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.02\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.1888, time 1896.26ms, mfu -100.00%\n",
      "iter 100: loss 1.9356, time 101.08ms, mfu 0.01%\n",
      "iter 200: loss 1.8819, time 116.90ms, mfu 0.01%\n",
      "step 250: train loss 4.5130, val loss 4.5186\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.02_SE\n",
      "iter 300: loss 1.6673, time 128.27ms, mfu 0.01%\n",
      "iter 400: loss 1.6921, time 107.82ms, mfu 0.01%\n",
      "step 500: train loss 4.5556, val loss 4.5564\n",
      "iter 500: loss 1.7141, time 1593.89ms, mfu 0.01%\n",
      "iter 600: loss 1.5405, time 211.00ms, mfu 0.01%\n",
      "iter 700: loss 1.5173, time 79.42ms, mfu 0.01%\n",
      "step 750: train loss 4.5560, val loss 4.5539\n",
      "iter 800: loss 1.6841, time 86.15ms, mfu 0.01%\n",
      "iter 900: loss 1.5349, time 113.10ms, mfu 0.01%\n",
      "step 1000: train loss 4.5382, val loss 4.5425\n",
      "iter 1000: loss 1.5208, time 1259.06ms, mfu 0.01%\n",
      "iter 1100: loss 1.3570, time 83.75ms, mfu 0.01%\n",
      "iter 1200: loss 1.4230, time 95.88ms, mfu 0.01%\n",
      "step 1250: train loss 4.5155, val loss 4.5221\n",
      "iter 1300: loss 1.3616, time 120.89ms, mfu 0.01%\n",
      "iter 1400: loss 1.3369, time 114.38ms, mfu 0.01%\n",
      "step 1500: train loss 4.5018, val loss 4.5097\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.02_SE\n",
      "iter 1500: loss 1.3028, time 1479.01ms, mfu 0.01%\n",
      "iter 1600: loss 1.4195, time 100.80ms, mfu 0.01%\n",
      "iter 1700: loss 1.2859, time 111.51ms, mfu 0.01%\n",
      "step 1750: train loss 4.4985, val loss 4.5005\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.02_SE\n",
      "iter 1800: loss 1.3202, time 98.86ms, mfu 0.01%\n",
      "iter 1900: loss 1.2902, time 104.78ms, mfu 0.01%\n",
      "step 2000: train loss 4.5053, val loss 4.5134\n",
      "iter 2000: loss 1.3618, time 1261.23ms, mfu 0.01%\n",
      "iter 2100: loss 1.2896, time 91.15ms, mfu 0.01%\n",
      "iter 2200: loss 1.2541, time 117.15ms, mfu 0.01%\n",
      "step 2250: train loss 4.5246, val loss 4.5328\n",
      "iter 2300: loss 1.2545, time 192.69ms, mfu 0.01%\n",
      "iter 2400: loss 1.2342, time 203.50ms, mfu 0.01%\n",
      "step 2500: train loss 4.5460, val loss 4.5486\n",
      "iter 2500: loss 1.2633, time 1273.33ms, mfu 0.01%\n",
      "iter 2600: loss 1.1823, time 80.25ms, mfu 0.01%\n",
      "iter 2700: loss 1.1954, time 136.81ms, mfu 0.01%\n",
      "step 2750: train loss 4.5570, val loss 4.5663\n",
      "iter 2800: loss 1.3266, time 129.76ms, mfu 0.01%\n",
      "iter 2900: loss 1.1800, time 123.66ms, mfu 0.01%\n",
      "step 3000: train loss 4.5786, val loss 4.5841\n",
      "iter 3000: loss 1.1901, time 1600.26ms, mfu 0.01%\n",
      "iter 3100: loss 1.1009, time 158.62ms, mfu 0.01%\n",
      "iter 3200: loss 1.1480, time 98.05ms, mfu 0.01%\n",
      "step 3250: train loss 4.6027, val loss 4.6088\n",
      "iter 3300: loss 1.2189, time 107.39ms, mfu 0.01%\n",
      "iter 3400: loss 1.2186, time 89.47ms, mfu 0.01%\n",
      "step 3500: train loss 4.6245, val loss 4.6340\n",
      "iter 3500: loss 1.1144, time 1473.18ms, mfu 0.01%\n",
      "iter 3600: loss 1.0569, time 141.41ms, mfu 0.01%\n",
      "iter 3700: loss 1.0998, time 84.09ms, mfu 0.01%\n",
      "step 3750: train loss 4.6430, val loss 4.6498\n",
      "iter 3800: loss 1.1764, time 117.95ms, mfu 0.01%\n",
      "iter 3900: loss 1.0665, time 188.75ms, mfu 0.01%\n",
      "step 4000: train loss 4.6483, val loss 4.6570\n",
      "iter 4000: loss 1.2123, time 1274.19ms, mfu 0.01%\n",
      "iter 4100: loss 1.0579, time 122.01ms, mfu 0.01%\n",
      "iter 4200: loss 1.0373, time 89.76ms, mfu 0.01%\n",
      "step 4250: train loss 4.6635, val loss 4.6700\n",
      "iter 4300: loss 1.0378, time 132.31ms, mfu 0.01%\n",
      "iter 4400: loss 1.1877, time 94.34ms, mfu 0.01%\n",
      "step 4500: train loss 4.6798, val loss 4.6849\n",
      "iter 4500: loss 1.1306, time 2382.05ms, mfu 0.01%\n",
      "iter 4600: loss 1.1901, time 95.25ms, mfu 0.01%\n",
      "iter 4700: loss 1.1117, time 129.86ms, mfu 0.01%\n",
      "step 4750: train loss 4.6898, val loss 4.6948\n",
      "iter 4800: loss 1.1181, time 186.66ms, mfu 0.01%\n",
      "iter 4900: loss 1.0376, time 125.35ms, mfu 0.01%\n",
      "step 5000: train loss 4.6999, val loss 4.7054\n",
      "iter 5000: loss 1.1322, time 1383.64ms, mfu 0.01%\n",
      "iter 5100: loss 1.0845, time 165.15ms, mfu 0.01%\n",
      "iter 5200: loss 1.0652, time 104.28ms, mfu 0.01%\n",
      "step 5250: train loss 4.7101, val loss 4.7156\n",
      "iter 5300: loss 0.9837, time 128.79ms, mfu 0.01%\n",
      "iter 5400: loss 1.0929, time 105.55ms, mfu 0.01%\n",
      "step 5500: train loss 4.7152, val loss 4.7184\n",
      "iter 5500: loss 1.1351, time 1101.46ms, mfu 0.01%\n",
      "iter 5600: loss 1.1007, time 89.70ms, mfu 0.01%\n",
      "iter 5700: loss 1.0186, time 150.31ms, mfu 0.01%\n",
      "step 5750: train loss 4.7097, val loss 4.7118\n",
      "iter 5800: loss 1.0860, time 119.53ms, mfu 0.01%\n",
      "iter 5900: loss 1.0369, time 76.52ms, mfu 0.01%\n",
      "step 6000: train loss 4.7226, val loss 4.7291\n",
      "iter 6000: loss 1.0478, time 1546.78ms, mfu 0.01%\n",
      "iter 6100: loss 1.0752, time 122.65ms, mfu 0.01%\n",
      "iter 6200: loss 1.0519, time 84.53ms, mfu 0.01%\n",
      "step 6250: train loss 4.7257, val loss 4.7300\n",
      "iter 6300: loss 1.0640, time 91.46ms, mfu 0.01%\n",
      "iter 6400: loss 0.9662, time 101.13ms, mfu 0.01%\n",
      "step 6500: train loss 4.7315, val loss 4.7348\n",
      "iter 6500: loss 1.0602, time 1478.49ms, mfu 0.01%\n",
      "iter 6600: loss 1.0895, time 135.23ms, mfu 0.01%\n",
      "iter 6700: loss 0.9914, time 82.94ms, mfu 0.01%\n",
      "step 6750: train loss 4.7336, val loss 4.7369\n",
      "iter 6800: loss 1.0303, time 143.31ms, mfu 0.01%\n",
      "iter 6900: loss 1.0422, time 108.75ms, mfu 0.01%\n",
      "step 7000: train loss 4.7319, val loss 4.7365\n",
      "iter 7000: loss 1.0446, time 1209.32ms, mfu 0.01%\n",
      "iter 7100: loss 1.0456, time 132.25ms, mfu 0.01%\n",
      "iter 7200: loss 0.9829, time 103.41ms, mfu 0.01%\n",
      "step 7250: train loss 4.7368, val loss 4.7412\n",
      "iter 7300: loss 1.1061, time 136.06ms, mfu 0.01%\n",
      "iter 7400: loss 1.0797, time 134.67ms, mfu 0.01%\n",
      "step 7500: train loss 4.7357, val loss 4.7410\n",
      "iter 7500: loss 1.0639, time 1304.77ms, mfu 0.01%\n",
      "iter 7600: loss 1.0264, time 135.07ms, mfu 0.01%\n",
      "iter 7700: loss 1.0187, time 116.77ms, mfu 0.01%\n",
      "step 7750: train loss 4.7342, val loss 4.7387\n",
      "iter 7800: loss 1.0859, time 124.51ms, mfu 0.01%\n",
      "iter 7900: loss 1.0501, time 179.90ms, mfu 0.01%\n",
      "step 8000: train loss 4.7360, val loss 4.7382\n",
      "iter 8000: loss 1.0898, time 1640.21ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.02\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.01_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.01\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.1240, time 1904.48ms, mfu -100.00%\n",
      "iter 100: loss 1.8946, time 131.22ms, mfu 0.01%\n",
      "iter 200: loss 1.7997, time 154.45ms, mfu 0.01%\n",
      "step 250: train loss 4.6125, val loss 4.6144\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.01_SE\n",
      "iter 300: loss 1.6885, time 98.54ms, mfu 0.01%\n",
      "iter 400: loss 1.7841, time 143.45ms, mfu 0.01%\n",
      "step 500: train loss 4.6562, val loss 4.6568\n",
      "iter 500: loss 1.8112, time 1425.58ms, mfu 0.01%\n",
      "iter 600: loss 1.6685, time 124.74ms, mfu 0.01%\n",
      "iter 700: loss 1.6740, time 159.48ms, mfu 0.01%\n",
      "step 750: train loss 4.6818, val loss 4.6806\n",
      "iter 800: loss 1.6897, time 178.09ms, mfu 0.01%\n",
      "iter 900: loss 1.6093, time 81.55ms, mfu 0.01%\n",
      "step 1000: train loss 4.6941, val loss 4.6953\n",
      "iter 1000: loss 1.7530, time 1279.94ms, mfu 0.01%\n",
      "iter 1100: loss 1.4729, time 102.22ms, mfu 0.01%\n",
      "iter 1200: loss 1.5437, time 108.49ms, mfu 0.01%\n",
      "step 1250: train loss 4.6710, val loss 4.6724\n",
      "iter 1300: loss 1.5072, time 87.51ms, mfu 0.01%\n",
      "iter 1400: loss 1.4811, time 121.42ms, mfu 0.01%\n",
      "step 1500: train loss 4.6773, val loss 4.6779\n",
      "iter 1500: loss 1.3816, time 1240.65ms, mfu 0.01%\n",
      "iter 1600: loss 1.5309, time 106.34ms, mfu 0.01%\n",
      "iter 1700: loss 1.3833, time 98.67ms, mfu 0.01%\n",
      "step 1750: train loss 4.6719, val loss 4.6717\n",
      "iter 1800: loss 1.4775, time 84.09ms, mfu 0.01%\n",
      "iter 1900: loss 1.3264, time 115.22ms, mfu 0.01%\n",
      "step 2000: train loss 4.6735, val loss 4.6765\n",
      "iter 2000: loss 1.4269, time 1419.35ms, mfu 0.01%\n",
      "iter 2100: loss 1.3405, time 102.54ms, mfu 0.01%\n",
      "iter 2200: loss 2.0185, time 106.48ms, mfu 0.01%\n",
      "step 2250: train loss 4.6813, val loss 4.6828\n",
      "iter 2300: loss 1.5469, time 103.74ms, mfu 0.01%\n",
      "iter 2400: loss 1.3787, time 146.44ms, mfu 0.01%\n",
      "step 2500: train loss 4.6867, val loss 4.6863\n",
      "iter 2500: loss 1.3720, time 1164.77ms, mfu 0.01%\n",
      "iter 2600: loss 1.2687, time 140.72ms, mfu 0.01%\n",
      "iter 2700: loss 1.3312, time 134.25ms, mfu 0.01%\n",
      "step 2750: train loss 4.6937, val loss 4.6972\n",
      "iter 2800: loss 1.4376, time 84.02ms, mfu 0.01%\n",
      "iter 2900: loss 1.2671, time 122.31ms, mfu 0.01%\n",
      "step 3000: train loss 4.7001, val loss 4.7011\n",
      "iter 3000: loss 1.2722, time 1307.01ms, mfu 0.01%\n",
      "iter 3100: loss 1.2408, time 188.47ms, mfu 0.01%\n",
      "iter 3200: loss 1.2029, time 88.25ms, mfu 0.01%\n",
      "step 3250: train loss 4.7082, val loss 4.7114\n",
      "iter 3300: loss 1.3126, time 102.55ms, mfu 0.01%\n",
      "iter 3400: loss 1.2722, time 94.46ms, mfu 0.01%\n",
      "step 3500: train loss 4.7123, val loss 4.7157\n",
      "iter 3500: loss 1.1958, time 1215.94ms, mfu 0.01%\n",
      "iter 3600: loss 1.1719, time 94.38ms, mfu 0.01%\n",
      "iter 3700: loss 1.1707, time 214.95ms, mfu 0.01%\n",
      "step 3750: train loss 4.7223, val loss 4.7260\n",
      "iter 3800: loss 1.2593, time 196.61ms, mfu 0.01%\n",
      "iter 3900: loss 1.1680, time 103.28ms, mfu 0.01%\n",
      "step 4000: train loss 4.7248, val loss 4.7291\n",
      "iter 4000: loss 1.2838, time 1157.69ms, mfu 0.01%\n",
      "iter 4100: loss 1.2216, time 138.02ms, mfu 0.01%\n",
      "iter 4200: loss 1.1740, time 123.95ms, mfu 0.01%\n",
      "step 4250: train loss 4.7367, val loss 4.7397\n",
      "iter 4300: loss 1.1747, time 106.72ms, mfu 0.01%\n",
      "iter 4400: loss 1.3284, time 106.50ms, mfu 0.01%\n",
      "step 4500: train loss 4.7405, val loss 4.7427\n",
      "iter 4500: loss 1.2318, time 1096.57ms, mfu 0.01%\n",
      "iter 4600: loss 1.2862, time 114.26ms, mfu 0.01%\n",
      "iter 4700: loss 1.1906, time 212.44ms, mfu 0.01%\n",
      "step 4750: train loss 4.7425, val loss 4.7456\n",
      "iter 4800: loss 1.2633, time 209.14ms, mfu 0.01%\n",
      "iter 4900: loss 1.1038, time 112.80ms, mfu 0.01%\n",
      "step 5000: train loss 4.7432, val loss 4.7466\n",
      "iter 5000: loss 1.2575, time 1340.75ms, mfu 0.01%\n",
      "iter 5100: loss 1.2061, time 93.47ms, mfu 0.01%\n",
      "iter 5200: loss 1.1694, time 94.50ms, mfu 0.01%\n",
      "step 5250: train loss 4.7503, val loss 4.7541\n",
      "iter 5300: loss 1.0733, time 114.04ms, mfu 0.01%\n",
      "iter 5400: loss 1.1809, time 89.55ms, mfu 0.01%\n",
      "step 5500: train loss 4.7536, val loss 4.7552\n",
      "iter 5500: loss 1.2506, time 1450.81ms, mfu 0.01%\n",
      "iter 5600: loss 1.1694, time 94.12ms, mfu 0.01%\n",
      "iter 5700: loss 1.0974, time 105.04ms, mfu 0.01%\n",
      "step 5750: train loss 4.7545, val loss 4.7562\n",
      "iter 5800: loss 1.1408, time 100.35ms, mfu 0.01%\n",
      "iter 5900: loss 1.1344, time 108.14ms, mfu 0.01%\n",
      "step 6000: train loss 4.7545, val loss 4.7585\n",
      "iter 6000: loss 1.1650, time 1049.34ms, mfu 0.01%\n",
      "iter 6100: loss 1.2017, time 121.61ms, mfu 0.01%\n",
      "iter 6200: loss 1.1523, time 90.27ms, mfu 0.01%\n",
      "step 6250: train loss 4.7579, val loss 4.7596\n",
      "iter 6300: loss 1.1843, time 90.67ms, mfu 0.01%\n",
      "iter 6400: loss 1.0520, time 86.99ms, mfu 0.01%\n",
      "step 6500: train loss 4.7597, val loss 4.7608\n",
      "iter 6500: loss 1.1429, time 1472.13ms, mfu 0.01%\n",
      "iter 6600: loss 1.1823, time 107.13ms, mfu 0.01%\n",
      "iter 6700: loss 1.1272, time 98.12ms, mfu 0.01%\n",
      "step 6750: train loss 4.7584, val loss 4.7610\n",
      "iter 6800: loss 1.1192, time 104.97ms, mfu 0.01%\n",
      "iter 6900: loss 1.1451, time 138.38ms, mfu 0.01%\n",
      "step 7000: train loss 4.7593, val loss 4.7624\n",
      "iter 7000: loss 1.1838, time 1533.10ms, mfu 0.01%\n",
      "iter 7100: loss 1.1587, time 124.09ms, mfu 0.01%\n",
      "iter 7200: loss 1.0826, time 100.10ms, mfu 0.01%\n",
      "step 7250: train loss 4.7585, val loss 4.7612\n",
      "iter 7300: loss 1.1489, time 131.68ms, mfu 0.01%\n",
      "iter 7400: loss 1.1627, time 101.59ms, mfu 0.01%\n",
      "step 7500: train loss 4.7565, val loss 4.7595\n",
      "iter 7500: loss 1.1610, time 1151.57ms, mfu 0.01%\n",
      "iter 7600: loss 1.0814, time 114.30ms, mfu 0.01%\n",
      "iter 7700: loss 1.0770, time 116.82ms, mfu 0.01%\n",
      "step 7750: train loss 4.7581, val loss 4.7609\n",
      "iter 7800: loss 1.1517, time 143.93ms, mfu 0.01%\n",
      "iter 7900: loss 1.0954, time 150.00ms, mfu 0.01%\n",
      "step 8000: train loss 4.7571, val loss 4.7592\n",
      "iter 8000: loss 1.1561, time 1575.29ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.01\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [0.04, 0.02, 0.01]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}_SE\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"3\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for proposed_margin_by_weight_type_3_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326afbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/1649692800.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_by_weight_type_3_alpha_0.04_SE/ckpt.pt\n",
      "Fixed checkpoint saved to out/proposed_margin_by_weight_type_3_alpha_0.02_SE/ckpt.pt\n",
      "Fixed checkpoint saved to out/proposed_margin_by_weight_type_3_alpha_0.01_SE/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "\n",
    "for margin_by_weight_alpha in [0.04, 0.02, 0.01]: # [0.5, 0.25, 0.125, 0.0625]:\n",
    "    \n",
    "    # Path to your original checkpoint\n",
    "    out_path = f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}_SE\"\n",
    "    ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    old_ckpt_path = f\"{out_path}/ckpt_old.pt\"\n",
    "    torch.save(checkpoint, old_ckpt_path)\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0745465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_1_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"/home/app/repos/selfishTokens_2/nanoGPT/train.py\", line 114, in <module>\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
      "  File \"<string>\", line 42, in <module>\n",
      "AssertionError: ERROR:1|temperature\n",
      "\n",
      "done for margin=1.0, T=1\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.9\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8400, time 1405.74ms, mfu -100.00%\n",
      "iter 100: loss 2.3010, time 113.60ms, mfu 0.01%\n",
      "iter 200: loss 2.2211, time 77.42ms, mfu 0.01%\n",
      "step 250: train loss 2.8879, val loss 2.9148\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 300: loss 2.0678, time 109.04ms, mfu 0.01%\n",
      "iter 400: loss 1.9995, time 194.25ms, mfu 0.01%\n",
      "step 500: train loss 2.6984, val loss 2.7147\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 500: loss 2.0823, time 1587.70ms, mfu 0.01%\n",
      "iter 600: loss 1.9021, time 99.14ms, mfu 0.01%\n",
      "iter 700: loss 1.8506, time 112.56ms, mfu 0.01%\n",
      "step 750: train loss 2.5733, val loss 2.5958\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 800: loss 1.9263, time 129.65ms, mfu 0.01%\n",
      "iter 900: loss 1.7577, time 143.12ms, mfu 0.01%\n",
      "step 1000: train loss 2.4583, val loss 2.5222\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 1000: loss 1.7964, time 1490.28ms, mfu 0.01%\n",
      "iter 1100: loss 1.6199, time 110.35ms, mfu 0.01%\n",
      "iter 1200: loss 1.6380, time 107.17ms, mfu 0.01%\n",
      "step 1250: train loss 2.4058, val loss 2.4483\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 1300: loss 1.6165, time 133.53ms, mfu 0.01%\n",
      "iter 1400: loss 1.6189, time 131.48ms, mfu 0.01%\n",
      "step 1500: train loss 2.3008, val loss 2.3793\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 1500: loss 1.5470, time 1146.25ms, mfu 0.01%\n",
      "iter 1600: loss 1.6676, time 99.11ms, mfu 0.01%\n",
      "iter 1700: loss 1.4769, time 100.82ms, mfu 0.01%\n",
      "step 1750: train loss 2.2773, val loss 2.3260\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 1800: loss 1.6134, time 110.36ms, mfu 0.01%\n",
      "iter 1900: loss 1.4738, time 77.64ms, mfu 0.01%\n",
      "step 2000: train loss 2.2480, val loss 2.3296\n",
      "iter 2000: loss 1.6019, time 1181.86ms, mfu 0.01%\n",
      "iter 2100: loss 1.5309, time 108.49ms, mfu 0.01%\n",
      "iter 2200: loss 1.5017, time 107.51ms, mfu 0.01%\n",
      "step 2250: train loss 2.1927, val loss 2.2783\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 2300: loss 1.5082, time 147.02ms, mfu 0.01%\n",
      "iter 2400: loss 1.4373, time 119.97ms, mfu 0.01%\n",
      "step 2500: train loss 2.2068, val loss 2.2715\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 2500: loss 1.5093, time 1216.70ms, mfu 0.01%\n",
      "iter 2600: loss 1.3928, time 115.59ms, mfu 0.01%\n",
      "iter 2700: loss 1.4132, time 107.98ms, mfu 0.01%\n",
      "step 2750: train loss 2.1838, val loss 2.2508\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 2800: loss 1.5890, time 81.30ms, mfu 0.01%\n",
      "iter 2900: loss 1.4131, time 75.78ms, mfu 0.01%\n",
      "step 3000: train loss 2.1833, val loss 2.2505\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 3000: loss 1.4040, time 1540.63ms, mfu 0.01%\n",
      "iter 3100: loss 1.3295, time 115.33ms, mfu 0.01%\n",
      "iter 3200: loss 1.3860, time 91.46ms, mfu 0.01%\n",
      "step 3250: train loss 2.1352, val loss 2.2218\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 3300: loss 1.4505, time 155.56ms, mfu 0.01%\n",
      "iter 3400: loss 1.3871, time 83.91ms, mfu 0.01%\n",
      "step 3500: train loss 2.1175, val loss 2.2135\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 3500: loss 1.2920, time 1583.08ms, mfu 0.01%\n",
      "iter 3600: loss 1.3105, time 121.48ms, mfu 0.01%\n",
      "iter 3700: loss 1.3175, time 74.25ms, mfu 0.01%\n",
      "step 3750: train loss 2.0965, val loss 2.1887\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 3800: loss 1.3542, time 110.28ms, mfu 0.01%\n",
      "iter 3900: loss 1.2509, time 85.09ms, mfu 0.01%\n",
      "step 4000: train loss 2.0780, val loss 2.2043\n",
      "iter 4000: loss 1.4396, time 1221.44ms, mfu 0.01%\n",
      "iter 4100: loss 1.2483, time 144.50ms, mfu 0.01%\n",
      "iter 4200: loss 1.2046, time 108.00ms, mfu 0.01%\n",
      "step 4250: train loss 2.0734, val loss 2.1786\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 4300: loss 1.2555, time 112.05ms, mfu 0.01%\n",
      "iter 4400: loss 1.4040, time 189.15ms, mfu 0.01%\n",
      "step 4500: train loss 2.0640, val loss 2.1762\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 4500: loss 1.3369, time 1231.63ms, mfu 0.01%\n",
      "iter 4600: loss 1.3977, time 119.79ms, mfu 0.01%\n",
      "iter 4700: loss 1.2892, time 114.03ms, mfu 0.01%\n",
      "step 4750: train loss 2.0625, val loss 2.1894\n",
      "iter 4800: loss 1.3344, time 132.00ms, mfu 0.01%\n",
      "iter 4900: loss 1.2178, time 120.15ms, mfu 0.01%\n",
      "step 5000: train loss 2.0244, val loss 2.1506\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 5000: loss 1.3985, time 1393.44ms, mfu 0.01%\n",
      "iter 5100: loss 1.3020, time 100.92ms, mfu 0.01%\n",
      "iter 5200: loss 1.2623, time 125.71ms, mfu 0.01%\n",
      "step 5250: train loss 2.0408, val loss 2.1355\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5300: loss 1.1388, time 102.30ms, mfu 0.01%\n",
      "iter 5400: loss 1.3078, time 109.52ms, mfu 0.01%\n",
      "step 5500: train loss 2.0536, val loss 2.1218\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 5500: loss 1.3391, time 1432.67ms, mfu 0.01%\n",
      "iter 5600: loss 1.3224, time 114.13ms, mfu 0.01%\n",
      "iter 5700: loss 1.2293, time 92.40ms, mfu 0.01%\n",
      "step 5750: train loss 2.0110, val loss 2.1058\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 5800: loss 1.2462, time 98.84ms, mfu 0.01%\n",
      "iter 5900: loss 1.2553, time 114.23ms, mfu 0.01%\n",
      "step 6000: train loss 2.0085, val loss 2.1310\n",
      "iter 6000: loss 1.3036, time 1123.30ms, mfu 0.01%\n",
      "iter 6100: loss 1.3296, time 84.58ms, mfu 0.01%\n",
      "iter 6200: loss 1.2205, time 96.53ms, mfu 0.01%\n",
      "step 6250: train loss 2.0274, val loss 2.1129\n",
      "iter 6300: loss 1.3308, time 88.21ms, mfu 0.01%\n",
      "iter 6400: loss 1.1606, time 125.29ms, mfu 0.01%\n",
      "step 6500: train loss 2.0137, val loss 2.1190\n",
      "iter 6500: loss 1.2355, time 1118.82ms, mfu 0.01%\n",
      "iter 6600: loss 1.3294, time 91.98ms, mfu 0.01%\n",
      "iter 6700: loss 1.2395, time 119.20ms, mfu 0.01%\n",
      "step 6750: train loss 2.0072, val loss 2.1157\n",
      "iter 6800: loss 1.2482, time 166.95ms, mfu 0.01%\n",
      "iter 6900: loss 1.2298, time 85.92ms, mfu 0.01%\n",
      "step 7000: train loss 1.9880, val loss 2.1214\n",
      "iter 7000: loss 1.3276, time 1214.98ms, mfu 0.01%\n",
      "iter 7100: loss 1.2678, time 95.46ms, mfu 0.01%\n",
      "iter 7200: loss 1.1722, time 132.48ms, mfu 0.01%\n",
      "step 7250: train loss 1.9899, val loss 2.1107\n",
      "iter 7300: loss 1.2671, time 113.49ms, mfu 0.01%\n",
      "iter 7400: loss 1.2770, time 124.22ms, mfu 0.01%\n",
      "step 7500: train loss 1.9773, val loss 2.1051\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.9_separated_embeddings\n",
      "iter 7500: loss 1.2772, time 1588.32ms, mfu 0.01%\n",
      "iter 7600: loss 1.1944, time 107.75ms, mfu 0.01%\n",
      "iter 7700: loss 1.1696, time 188.14ms, mfu 0.01%\n",
      "step 7750: train loss 1.9915, val loss 2.1148\n",
      "iter 7800: loss 1.2779, time 123.28ms, mfu 0.01%\n",
      "iter 7900: loss 1.2139, time 110.63ms, mfu 0.01%\n",
      "step 8000: train loss 1.9770, val loss 2.1196\n",
      "iter 8000: loss 1.2576, time 1240.35ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.9\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.8\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8481, time 1853.76ms, mfu -100.00%\n",
      "iter 100: loss 2.3292, time 105.24ms, mfu 0.01%\n",
      "iter 200: loss 2.2133, time 83.96ms, mfu 0.01%\n",
      "step 250: train loss 2.8087, val loss 2.8328\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 300: loss 2.0572, time 110.51ms, mfu 0.01%\n",
      "iter 400: loss 2.0144, time 113.38ms, mfu 0.01%\n",
      "step 500: train loss 2.5853, val loss 2.6031\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 500: loss 2.0886, time 1380.96ms, mfu 0.01%\n",
      "iter 600: loss 1.9289, time 82.09ms, mfu 0.01%\n",
      "iter 700: loss 1.8699, time 115.18ms, mfu 0.01%\n",
      "step 750: train loss 2.4626, val loss 2.4832\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 800: loss 1.9427, time 86.07ms, mfu 0.01%\n",
      "iter 900: loss 1.7785, time 132.97ms, mfu 0.01%\n",
      "step 1000: train loss 2.3403, val loss 2.4036\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 1000: loss 1.8173, time 1261.86ms, mfu 0.01%\n",
      "iter 1100: loss 1.6550, time 107.35ms, mfu 0.01%\n",
      "iter 1200: loss 1.6561, time 101.93ms, mfu 0.01%\n",
      "step 1250: train loss 2.2761, val loss 2.3260\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 1300: loss 1.6503, time 92.86ms, mfu 0.01%\n",
      "iter 1400: loss 1.6497, time 148.13ms, mfu 0.01%\n",
      "step 1500: train loss 2.1904, val loss 2.2723\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 1500: loss 1.5648, time 1576.44ms, mfu 0.01%\n",
      "iter 1600: loss 1.6628, time 111.06ms, mfu 0.01%\n",
      "iter 1700: loss 1.4895, time 120.44ms, mfu 0.01%\n",
      "step 1750: train loss 2.1596, val loss 2.2062\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 1800: loss 1.6057, time 81.45ms, mfu 0.01%\n",
      "iter 1900: loss 1.4881, time 126.02ms, mfu 0.01%\n",
      "step 2000: train loss 2.1175, val loss 2.2139\n",
      "iter 2000: loss 1.6097, time 1241.57ms, mfu 0.01%\n",
      "iter 2100: loss 1.5539, time 103.54ms, mfu 0.01%\n",
      "iter 2200: loss 1.5188, time 119.26ms, mfu 0.01%\n",
      "step 2250: train loss 2.0616, val loss 2.1677\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 2300: loss 1.5202, time 130.08ms, mfu 0.01%\n",
      "iter 2400: loss 1.4229, time 131.51ms, mfu 0.01%\n",
      "step 2500: train loss 2.0671, val loss 2.1400\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 2500: loss 1.4944, time 1203.82ms, mfu 0.01%\n",
      "iter 2600: loss 1.4079, time 123.90ms, mfu 0.01%\n",
      "iter 2700: loss 1.4060, time 76.85ms, mfu 0.01%\n",
      "step 2750: train loss 2.0490, val loss 2.1159\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 2800: loss 1.5850, time 71.32ms, mfu 0.01%\n",
      "iter 2900: loss 1.4359, time 89.89ms, mfu 0.01%\n",
      "step 3000: train loss 2.0470, val loss 2.1265\n",
      "iter 3000: loss 1.4025, time 1082.48ms, mfu 0.01%\n",
      "iter 3100: loss 1.3324, time 87.76ms, mfu 0.01%\n",
      "iter 3200: loss 1.3992, time 103.14ms, mfu 0.01%\n",
      "step 3250: train loss 1.9885, val loss 2.0767\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 3300: loss 1.4617, time 124.24ms, mfu 0.01%\n",
      "iter 3400: loss 1.3897, time 101.25ms, mfu 0.01%\n",
      "step 3500: train loss 1.9766, val loss 2.0812\n",
      "iter 3500: loss 1.3098, time 1367.89ms, mfu 0.01%\n",
      "iter 3600: loss 1.3046, time 130.15ms, mfu 0.01%\n",
      "iter 3700: loss 1.3096, time 118.15ms, mfu 0.01%\n",
      "step 3750: train loss 1.9669, val loss 2.0675\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 3800: loss 1.3813, time 113.61ms, mfu 0.01%\n",
      "iter 3900: loss 1.2832, time 87.20ms, mfu 0.01%\n",
      "step 4000: train loss 1.9341, val loss 2.0799\n",
      "iter 4000: loss 1.4268, time 1402.37ms, mfu 0.01%\n",
      "iter 4100: loss 1.2700, time 98.61ms, mfu 0.01%\n",
      "iter 4200: loss 1.2199, time 90.72ms, mfu 0.01%\n",
      "step 4250: train loss 1.9295, val loss 2.0448\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 4300: loss 1.2716, time 114.70ms, mfu 0.01%\n",
      "iter 4400: loss 1.4198, time 133.08ms, mfu 0.01%\n",
      "step 4500: train loss 1.9311, val loss 2.0591\n",
      "iter 4500: loss 1.3736, time 1147.18ms, mfu 0.01%\n",
      "iter 4600: loss 1.3773, time 136.63ms, mfu 0.01%\n",
      "iter 4700: loss 1.2940, time 92.61ms, mfu 0.01%\n",
      "step 4750: train loss 1.9281, val loss 2.0622\n",
      "iter 4800: loss 1.3517, time 118.40ms, mfu 0.01%\n",
      "iter 4900: loss 1.2075, time 97.84ms, mfu 0.01%\n",
      "step 5000: train loss 1.8830, val loss 2.0270\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5000: loss 1.4123, time 1414.98ms, mfu 0.01%\n",
      "iter 5100: loss 1.2972, time 93.74ms, mfu 0.01%\n",
      "iter 5200: loss 1.2907, time 86.54ms, mfu 0.01%\n",
      "step 5250: train loss 1.8955, val loss 2.0007\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 5300: loss 1.1390, time 133.15ms, mfu 0.01%\n",
      "iter 5400: loss 1.3212, time 89.13ms, mfu 0.01%\n",
      "step 5500: train loss 1.9217, val loss 1.9970\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 5500: loss 1.3506, time 1119.03ms, mfu 0.01%\n",
      "iter 5600: loss 1.3152, time 81.73ms, mfu 0.01%\n",
      "iter 5700: loss 1.2148, time 134.15ms, mfu 0.01%\n",
      "step 5750: train loss 1.8621, val loss 1.9752\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 5800: loss 1.2424, time 96.17ms, mfu 0.01%\n",
      "iter 5900: loss 1.2645, time 256.73ms, mfu 0.01%\n",
      "step 6000: train loss 1.8582, val loss 1.9935\n",
      "iter 6000: loss 1.2927, time 1736.86ms, mfu 0.01%\n",
      "iter 6100: loss 1.3495, time 82.19ms, mfu 0.01%\n",
      "iter 6200: loss 1.2429, time 76.01ms, mfu 0.01%\n",
      "step 6250: train loss 1.8795, val loss 1.9744\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 6300: loss 1.3145, time 86.71ms, mfu 0.01%\n",
      "iter 6400: loss 1.1664, time 151.16ms, mfu 0.01%\n",
      "step 6500: train loss 1.8643, val loss 1.9801\n",
      "iter 6500: loss 1.2406, time 1075.90ms, mfu 0.01%\n",
      "iter 6600: loss 1.3317, time 108.44ms, mfu 0.01%\n",
      "iter 6700: loss 1.2566, time 99.29ms, mfu 0.01%\n",
      "step 6750: train loss 1.8579, val loss 1.9764\n",
      "iter 6800: loss 1.2488, time 110.97ms, mfu 0.01%\n",
      "iter 6900: loss 1.2539, time 150.02ms, mfu 0.01%\n",
      "step 7000: train loss 1.8379, val loss 1.9840\n",
      "iter 7000: loss 1.3315, time 1311.16ms, mfu 0.01%\n",
      "iter 7100: loss 1.2785, time 85.73ms, mfu 0.01%\n",
      "iter 7200: loss 1.1778, time 104.89ms, mfu 0.01%\n",
      "step 7250: train loss 1.8437, val loss 1.9804\n",
      "iter 7300: loss 1.2682, time 178.16ms, mfu 0.01%\n",
      "iter 7400: loss 1.2844, time 124.97ms, mfu 0.01%\n",
      "step 7500: train loss 1.8256, val loss 1.9709\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.8_separated_embeddings\n",
      "iter 7500: loss 1.2514, time 1335.08ms, mfu 0.01%\n",
      "iter 7600: loss 1.2235, time 103.43ms, mfu 0.01%\n",
      "iter 7700: loss 1.1745, time 134.61ms, mfu 0.01%\n",
      "step 7750: train loss 1.8494, val loss 1.9806\n",
      "iter 7800: loss 1.2837, time 220.63ms, mfu 0.01%\n",
      "iter 7900: loss 1.2213, time 173.40ms, mfu 0.01%\n",
      "step 8000: train loss 1.8251, val loss 1.9827\n",
      "iter 8000: loss 1.2641, time 1111.27ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.8\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.7\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8552, time 1866.73ms, mfu -100.00%\n",
      "iter 100: loss 2.3545, time 259.38ms, mfu 0.00%\n",
      "iter 200: loss 2.2090, time 110.27ms, mfu 0.01%\n",
      "step 250: train loss 2.6829, val loss 2.7120\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 300: loss 2.0544, time 113.09ms, mfu 0.01%\n",
      "iter 400: loss 2.0392, time 132.45ms, mfu 0.01%\n",
      "step 500: train loss 2.4777, val loss 2.5044\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 500: loss 2.0929, time 1285.43ms, mfu 0.01%\n",
      "iter 600: loss 1.9323, time 113.87ms, mfu 0.01%\n",
      "iter 700: loss 1.8712, time 106.08ms, mfu 0.01%\n",
      "step 750: train loss 2.3578, val loss 2.3728\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 800: loss 1.9568, time 77.46ms, mfu 0.01%\n",
      "iter 900: loss 1.7823, time 112.61ms, mfu 0.01%\n",
      "step 1000: train loss 2.2251, val loss 2.2942\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 1000: loss 1.8228, time 1486.72ms, mfu 0.01%\n",
      "iter 1100: loss 1.6367, time 80.77ms, mfu 0.01%\n",
      "iter 1200: loss 1.6660, time 89.26ms, mfu 0.01%\n",
      "step 1250: train loss 2.1496, val loss 2.2050\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 1300: loss 1.6527, time 124.24ms, mfu 0.01%\n",
      "iter 1400: loss 1.6599, time 118.50ms, mfu 0.01%\n",
      "step 1500: train loss 2.0685, val loss 2.1634\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 1500: loss 1.5622, time 1300.67ms, mfu 0.01%\n",
      "iter 1600: loss 1.6975, time 86.67ms, mfu 0.01%\n",
      "iter 1700: loss 1.4980, time 90.84ms, mfu 0.01%\n",
      "step 1750: train loss 2.0500, val loss 2.0939\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 1800: loss 1.6085, time 276.52ms, mfu 0.01%\n",
      "iter 1900: loss 1.5096, time 103.30ms, mfu 0.01%\n",
      "step 2000: train loss 2.0046, val loss 2.0964\n",
      "iter 2000: loss 1.6197, time 1139.47ms, mfu 0.01%\n",
      "iter 2100: loss 1.5325, time 108.96ms, mfu 0.01%\n",
      "iter 2200: loss 1.5185, time 81.79ms, mfu 0.01%\n",
      "step 2250: train loss 1.9496, val loss 2.0540\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 2300: loss 1.5257, time 149.85ms, mfu 0.01%\n",
      "iter 2400: loss 1.4597, time 117.12ms, mfu 0.01%\n",
      "step 2500: train loss 1.9530, val loss 2.0242\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 2500: loss 1.5167, time 1189.29ms, mfu 0.01%\n",
      "iter 2600: loss 1.3909, time 86.02ms, mfu 0.01%\n",
      "iter 2700: loss 1.4147, time 104.32ms, mfu 0.01%\n",
      "step 2750: train loss 1.9246, val loss 2.0019\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 2800: loss 1.5823, time 96.74ms, mfu 0.01%\n",
      "iter 2900: loss 1.4218, time 114.62ms, mfu 0.01%\n",
      "step 3000: train loss 1.9316, val loss 2.0147\n",
      "iter 3000: loss 1.4023, time 1468.41ms, mfu 0.01%\n",
      "iter 3100: loss 1.3320, time 73.05ms, mfu 0.01%\n",
      "iter 3200: loss 1.3903, time 80.92ms, mfu 0.01%\n",
      "step 3250: train loss 1.8708, val loss 1.9588\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 3300: loss 1.4625, time 114.07ms, mfu 0.01%\n",
      "iter 3400: loss 1.4275, time 114.45ms, mfu 0.01%\n",
      "step 3500: train loss 1.8552, val loss 1.9739\n",
      "iter 3500: loss 1.2925, time 1462.69ms, mfu 0.01%\n",
      "iter 3600: loss 1.3168, time 107.64ms, mfu 0.01%\n",
      "iter 3700: loss 1.3233, time 127.85ms, mfu 0.01%\n",
      "step 3750: train loss 1.8294, val loss 1.9385\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 3800: loss 1.3613, time 113.54ms, mfu 0.01%\n",
      "iter 3900: loss 1.2664, time 102.26ms, mfu 0.01%\n",
      "step 4000: train loss 1.7927, val loss 1.9529\n",
      "iter 4000: loss 1.4474, time 1096.74ms, mfu 0.01%\n",
      "iter 4100: loss 1.2408, time 131.04ms, mfu 0.01%\n",
      "iter 4200: loss 1.2318, time 138.68ms, mfu 0.01%\n",
      "step 4250: train loss 1.7976, val loss 1.9381\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 4300: loss 1.2544, time 101.05ms, mfu 0.01%\n",
      "iter 4400: loss 1.4247, time 84.99ms, mfu 0.01%\n",
      "step 4500: train loss 1.7925, val loss 1.9335\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500: loss 1.3647, time 1386.83ms, mfu 0.01%\n",
      "iter 4600: loss 1.4030, time 115.19ms, mfu 0.01%\n",
      "iter 4700: loss 1.2884, time 133.06ms, mfu 0.01%\n",
      "step 4750: train loss 1.7836, val loss 1.9283\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 4800: loss 1.3409, time 89.87ms, mfu 0.01%\n",
      "iter 4900: loss 1.2090, time 116.72ms, mfu 0.01%\n",
      "step 5000: train loss 1.7349, val loss 1.8961\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 5000: loss 1.4065, time 1206.69ms, mfu 0.01%\n",
      "iter 5100: loss 1.3015, time 101.36ms, mfu 0.01%\n",
      "iter 5200: loss 1.2752, time 84.88ms, mfu 0.01%\n",
      "step 5250: train loss 1.7544, val loss 1.8696\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 5300: loss 1.1399, time 110.98ms, mfu 0.01%\n",
      "iter 5400: loss 1.3057, time 152.84ms, mfu 0.01%\n",
      "step 5500: train loss 1.7818, val loss 1.8663\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 5500: loss 1.3619, time 1494.23ms, mfu 0.01%\n",
      "iter 5600: loss 1.3181, time 90.09ms, mfu 0.01%\n",
      "iter 5700: loss 1.2205, time 78.30ms, mfu 0.01%\n",
      "step 5750: train loss 1.7216, val loss 1.8437\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 5800: loss 1.2359, time 106.59ms, mfu 0.01%\n",
      "iter 5900: loss 1.2409, time 80.77ms, mfu 0.01%\n",
      "step 6000: train loss 1.7160, val loss 1.8677\n",
      "iter 6000: loss 1.3121, time 1246.01ms, mfu 0.01%\n",
      "iter 6100: loss 1.3571, time 103.97ms, mfu 0.01%\n",
      "iter 6200: loss 1.2408, time 71.40ms, mfu 0.01%\n",
      "step 6250: train loss 1.7398, val loss 1.8451\n",
      "iter 6300: loss 1.3255, time 145.40ms, mfu 0.01%\n",
      "iter 6400: loss 1.1478, time 130.14ms, mfu 0.01%\n",
      "step 6500: train loss 1.7256, val loss 1.8549\n",
      "iter 6500: loss 1.2394, time 1545.46ms, mfu 0.01%\n",
      "iter 6600: loss 1.3454, time 86.39ms, mfu 0.01%\n",
      "iter 6700: loss 1.2439, time 306.48ms, mfu 0.01%\n",
      "step 6750: train loss 1.7119, val loss 1.8461\n",
      "iter 6800: loss 1.2442, time 139.87ms, mfu 0.01%\n",
      "iter 6900: loss 1.2292, time 126.68ms, mfu 0.01%\n",
      "step 7000: train loss 1.6914, val loss 1.8546\n",
      "iter 7000: loss 1.3480, time 1138.96ms, mfu 0.01%\n",
      "iter 7100: loss 1.2710, time 120.09ms, mfu 0.01%\n",
      "iter 7200: loss 1.1811, time 173.88ms, mfu 0.01%\n",
      "step 7250: train loss 1.6903, val loss 1.8484\n",
      "iter 7300: loss 1.2742, time 85.63ms, mfu 0.01%\n",
      "iter 7400: loss 1.2922, time 71.88ms, mfu 0.01%\n",
      "step 7500: train loss 1.6787, val loss 1.8371\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.7_separated_embeddings\n",
      "iter 7500: loss 1.2413, time 1572.20ms, mfu 0.01%\n",
      "iter 7600: loss 1.2276, time 105.07ms, mfu 0.01%\n",
      "iter 7700: loss 1.1656, time 113.16ms, mfu 0.01%\n",
      "step 7750: train loss 1.7018, val loss 1.8463\n",
      "iter 7800: loss 1.2774, time 81.10ms, mfu 0.01%\n",
      "iter 7900: loss 1.2174, time 123.52ms, mfu 0.01%\n",
      "step 8000: train loss 1.6856, val loss 1.8522\n",
      "iter 8000: loss 1.2574, time 1182.39ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.7\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.6\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8590, time 1665.09ms, mfu -100.00%\n",
      "iter 100: loss 2.3898, time 97.47ms, mfu 0.01%\n",
      "iter 200: loss 2.2411, time 91.71ms, mfu 0.01%\n",
      "step 250: train loss 2.5882, val loss 2.6162\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 300: loss 2.0753, time 101.27ms, mfu 0.01%\n",
      "iter 400: loss 2.0202, time 99.14ms, mfu 0.01%\n",
      "step 500: train loss 2.3847, val loss 2.4118\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 500: loss 2.1097, time 1141.83ms, mfu 0.01%\n",
      "iter 600: loss 1.9726, time 114.65ms, mfu 0.01%\n",
      "iter 700: loss 1.8691, time 136.23ms, mfu 0.01%\n",
      "step 750: train loss 2.2641, val loss 2.2921\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 800: loss 1.9594, time 93.08ms, mfu 0.01%\n",
      "iter 900: loss 1.8120, time 84.80ms, mfu 0.01%\n",
      "step 1000: train loss 2.1262, val loss 2.2034\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 1000: loss 1.8281, time 1300.45ms, mfu 0.01%\n",
      "iter 1100: loss 1.6435, time 129.04ms, mfu 0.01%\n",
      "iter 1200: loss 1.6663, time 87.87ms, mfu 0.01%\n",
      "step 1250: train loss 2.0660, val loss 2.1232\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 1300: loss 1.6418, time 131.53ms, mfu 0.01%\n",
      "iter 1400: loss 1.6584, time 95.59ms, mfu 0.01%\n",
      "step 1500: train loss 1.9738, val loss 2.0740\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 1500: loss 1.5926, time 1220.19ms, mfu 0.01%\n",
      "iter 1600: loss 1.7122, time 170.65ms, mfu 0.01%\n",
      "iter 1700: loss 1.5248, time 95.04ms, mfu 0.01%\n",
      "step 1750: train loss 1.9425, val loss 2.0138\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 1800: loss 1.6150, time 157.40ms, mfu 0.01%\n",
      "iter 1900: loss 1.5161, time 111.25ms, mfu 0.01%\n",
      "step 2000: train loss 1.9080, val loss 2.0227\n",
      "iter 2000: loss 1.6398, time 1532.48ms, mfu 0.01%\n",
      "iter 2100: loss 1.5511, time 160.55ms, mfu 0.01%\n",
      "iter 2200: loss 1.5269, time 77.36ms, mfu 0.01%\n",
      "step 2250: train loss 1.8426, val loss 1.9690\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 2300: loss 1.5470, time 110.90ms, mfu 0.01%\n",
      "iter 2400: loss 1.4696, time 104.09ms, mfu 0.01%\n",
      "step 2500: train loss 1.8555, val loss 1.9515\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 2500: loss 1.5155, time 1262.58ms, mfu 0.01%\n",
      "iter 2600: loss 1.4044, time 119.38ms, mfu 0.01%\n",
      "iter 2700: loss 1.4359, time 124.90ms, mfu 0.01%\n",
      "step 2750: train loss 1.8139, val loss 1.9079\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 2800: loss 1.6105, time 67.21ms, mfu 0.01%\n",
      "iter 2900: loss 1.4023, time 92.25ms, mfu 0.01%\n",
      "step 3000: train loss 1.8294, val loss 1.9383\n",
      "iter 3000: loss 1.4149, time 1209.07ms, mfu 0.01%\n",
      "iter 3100: loss 1.3253, time 92.98ms, mfu 0.01%\n",
      "iter 3200: loss 1.3959, time 110.96ms, mfu 0.01%\n",
      "step 3250: train loss 1.7521, val loss 1.8669\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 3300: loss 1.4775, time 127.31ms, mfu 0.01%\n",
      "iter 3400: loss 1.4253, time 92.78ms, mfu 0.01%\n",
      "step 3500: train loss 1.7449, val loss 1.8827\n",
      "iter 3500: loss 1.2946, time 1320.10ms, mfu 0.01%\n",
      "iter 3600: loss 1.3225, time 135.45ms, mfu 0.01%\n",
      "iter 3700: loss 1.3003, time 140.94ms, mfu 0.01%\n",
      "step 3750: train loss 1.7302, val loss 1.8481\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 3800: loss 1.3639, time 110.27ms, mfu 0.01%\n",
      "iter 3900: loss 1.2786, time 116.09ms, mfu 0.01%\n",
      "step 4000: train loss 1.6926, val loss 1.8651\n",
      "iter 4000: loss 1.4447, time 1271.76ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4100: loss 1.2499, time 115.60ms, mfu 0.01%\n",
      "iter 4200: loss 1.2332, time 85.45ms, mfu 0.01%\n",
      "step 4250: train loss 1.6877, val loss 1.8447\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 4300: loss 1.2692, time 171.78ms, mfu 0.01%\n",
      "iter 4400: loss 1.4446, time 109.08ms, mfu 0.01%\n",
      "step 4500: train loss 1.6862, val loss 1.8492\n",
      "iter 4500: loss 1.3722, time 1447.03ms, mfu 0.01%\n",
      "iter 4600: loss 1.3996, time 127.67ms, mfu 0.01%\n",
      "iter 4700: loss 1.2977, time 95.11ms, mfu 0.01%\n",
      "step 4750: train loss 1.6798, val loss 1.8421\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 4800: loss 1.3275, time 88.02ms, mfu 0.01%\n",
      "iter 4900: loss 1.2018, time 104.07ms, mfu 0.01%\n",
      "step 5000: train loss 1.6249, val loss 1.8117\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 5000: loss 1.4299, time 1707.25ms, mfu 0.01%\n",
      "iter 5100: loss 1.3275, time 87.61ms, mfu 0.01%\n",
      "iter 5200: loss 1.3036, time 92.67ms, mfu 0.01%\n",
      "step 5250: train loss 1.6380, val loss 1.7755\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 5300: loss 1.1534, time 130.33ms, mfu 0.01%\n",
      "iter 5400: loss 1.3024, time 92.73ms, mfu 0.01%\n",
      "step 5500: train loss 1.6670, val loss 1.7640\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 5500: loss 1.3759, time 1448.57ms, mfu 0.01%\n",
      "iter 5600: loss 1.3100, time 82.20ms, mfu 0.01%\n",
      "iter 5700: loss 1.2281, time 131.84ms, mfu 0.01%\n",
      "step 5750: train loss 1.6082, val loss 1.7435\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 5800: loss 1.2270, time 90.80ms, mfu 0.01%\n",
      "iter 5900: loss 1.2403, time 126.00ms, mfu 0.01%\n",
      "step 6000: train loss 1.5999, val loss 1.7619\n",
      "iter 6000: loss 1.3281, time 1257.77ms, mfu 0.01%\n",
      "iter 6100: loss 1.3527, time 92.55ms, mfu 0.01%\n",
      "iter 6200: loss 1.2369, time 101.52ms, mfu 0.01%\n",
      "step 6250: train loss 1.6126, val loss 1.7353\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 6300: loss 1.3397, time 140.13ms, mfu 0.01%\n",
      "iter 6400: loss 1.1438, time 105.53ms, mfu 0.01%\n",
      "step 6500: train loss 1.6078, val loss 1.7501\n",
      "iter 6500: loss 1.2480, time 1305.77ms, mfu 0.01%\n",
      "iter 6600: loss 1.3456, time 90.75ms, mfu 0.01%\n",
      "iter 6700: loss 1.2672, time 109.70ms, mfu 0.01%\n",
      "step 6750: train loss 1.5939, val loss 1.7422\n",
      "iter 6800: loss 1.2514, time 117.99ms, mfu 0.01%\n",
      "iter 6900: loss 1.2417, time 121.01ms, mfu 0.01%\n",
      "step 7000: train loss 1.5689, val loss 1.7533\n",
      "iter 7000: loss 1.3598, time 1373.25ms, mfu 0.01%\n",
      "iter 7100: loss 1.2877, time 102.27ms, mfu 0.01%\n",
      "iter 7200: loss 1.1706, time 106.50ms, mfu 0.01%\n",
      "step 7250: train loss 1.5749, val loss 1.7410\n",
      "iter 7300: loss 1.2858, time 134.51ms, mfu 0.01%\n",
      "iter 7400: loss 1.2745, time 86.87ms, mfu 0.01%\n",
      "step 7500: train loss 1.5550, val loss 1.7352\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.6_separated_embeddings\n",
      "iter 7500: loss 1.2528, time 1784.34ms, mfu 0.01%\n",
      "iter 7600: loss 1.2139, time 84.00ms, mfu 0.01%\n",
      "iter 7700: loss 1.1807, time 81.85ms, mfu 0.01%\n",
      "step 7750: train loss 1.5796, val loss 1.7390\n",
      "iter 7800: loss 1.2988, time 110.98ms, mfu 0.01%\n",
      "iter 7900: loss 1.2262, time 118.70ms, mfu 0.01%\n",
      "step 8000: train loss 1.5614, val loss 1.7411\n",
      "iter 8000: loss 1.2686, time 1146.52ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.6\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.5\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8599, time 1367.64ms, mfu -100.00%\n",
      "iter 100: loss 2.4727, time 101.40ms, mfu 0.01%\n",
      "iter 200: loss 2.2598, time 99.74ms, mfu 0.01%\n",
      "step 250: train loss 2.5394, val loss 2.5705\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 300: loss 2.0779, time 125.96ms, mfu 0.01%\n",
      "iter 400: loss 2.0183, time 89.50ms, mfu 0.01%\n",
      "step 500: train loss 2.3533, val loss 2.3763\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 500: loss 2.1291, time 1462.14ms, mfu 0.01%\n",
      "iter 600: loss 1.9604, time 109.59ms, mfu 0.01%\n",
      "iter 700: loss 1.8901, time 117.73ms, mfu 0.01%\n",
      "step 750: train loss 2.2278, val loss 2.2493\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 800: loss 1.9714, time 162.71ms, mfu 0.01%\n",
      "iter 900: loss 1.8020, time 105.25ms, mfu 0.01%\n",
      "step 1000: train loss 2.0844, val loss 2.1695\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 1000: loss 1.8735, time 1550.68ms, mfu 0.01%\n",
      "iter 1100: loss 1.6846, time 106.47ms, mfu 0.01%\n",
      "iter 1200: loss 1.6988, time 119.77ms, mfu 0.01%\n",
      "step 1250: train loss 2.0279, val loss 2.0954\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 1300: loss 1.6814, time 108.04ms, mfu 0.01%\n",
      "iter 1400: loss 1.6810, time 98.54ms, mfu 0.01%\n",
      "step 1500: train loss 1.9273, val loss 2.0371\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 1500: loss 1.5997, time 1316.28ms, mfu 0.01%\n",
      "iter 1600: loss 1.7236, time 130.95ms, mfu 0.01%\n",
      "iter 1700: loss 1.5264, time 106.59ms, mfu 0.01%\n",
      "step 1750: train loss 1.8999, val loss 1.9774\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 1800: loss 1.6158, time 173.86ms, mfu 0.01%\n",
      "iter 1900: loss 1.5236, time 121.24ms, mfu 0.01%\n",
      "step 2000: train loss 1.8524, val loss 1.9667\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 2000: loss 1.6425, time 1293.17ms, mfu 0.01%\n",
      "iter 2100: loss 1.5345, time 118.48ms, mfu 0.01%\n",
      "iter 2200: loss 1.5587, time 118.68ms, mfu 0.01%\n",
      "step 2250: train loss 1.7985, val loss 1.9541\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 2300: loss 1.5394, time 134.25ms, mfu 0.01%\n",
      "iter 2400: loss 1.4897, time 83.11ms, mfu 0.01%\n",
      "step 2500: train loss 1.8022, val loss 1.9070\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 2500: loss 1.5353, time 1345.77ms, mfu 0.01%\n",
      "iter 2600: loss 1.4401, time 110.56ms, mfu 0.01%\n",
      "iter 2700: loss 1.4106, time 94.78ms, mfu 0.01%\n",
      "step 2750: train loss 1.7703, val loss 1.8744\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 2800: loss 1.6263, time 122.01ms, mfu 0.01%\n",
      "iter 2900: loss 1.4417, time 136.44ms, mfu 0.01%\n",
      "step 3000: train loss 1.7721, val loss 1.8834\n",
      "iter 3000: loss 1.4240, time 1285.17ms, mfu 0.01%\n",
      "iter 3100: loss 1.3644, time 276.43ms, mfu 0.01%\n",
      "iter 3200: loss 1.3876, time 116.43ms, mfu 0.01%\n",
      "step 3250: train loss 1.7047, val loss 1.8256\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 3300: loss 1.4698, time 149.46ms, mfu 0.01%\n",
      "iter 3400: loss 1.3976, time 92.99ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 1.6732, val loss 1.8337\n",
      "iter 3500: loss 1.3107, time 1452.39ms, mfu 0.01%\n",
      "iter 3600: loss 1.3451, time 117.25ms, mfu 0.01%\n",
      "iter 3700: loss 1.3239, time 78.10ms, mfu 0.01%\n",
      "step 3750: train loss 1.6570, val loss 1.7983\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 3800: loss 1.3371, time 110.15ms, mfu 0.01%\n",
      "iter 3900: loss 1.2806, time 118.89ms, mfu 0.01%\n",
      "step 4000: train loss 1.6191, val loss 1.8200\n",
      "iter 4000: loss 1.4426, time 1428.38ms, mfu 0.01%\n",
      "iter 4100: loss 1.2836, time 137.32ms, mfu 0.01%\n",
      "iter 4200: loss 1.2392, time 113.11ms, mfu 0.01%\n",
      "step 4250: train loss 1.6200, val loss 1.7848\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 4300: loss 1.2594, time 108.56ms, mfu 0.01%\n",
      "iter 4400: loss 1.4336, time 133.30ms, mfu 0.01%\n",
      "step 4500: train loss 1.6210, val loss 1.7980\n",
      "iter 4500: loss 1.3850, time 1218.25ms, mfu 0.01%\n",
      "iter 4600: loss 1.3971, time 135.61ms, mfu 0.01%\n",
      "iter 4700: loss 1.3161, time 103.24ms, mfu 0.01%\n",
      "step 4750: train loss 1.6054, val loss 1.7875\n",
      "iter 4800: loss 1.3578, time 119.77ms, mfu 0.01%\n",
      "iter 4900: loss 1.2088, time 151.56ms, mfu 0.01%\n",
      "step 5000: train loss 1.5496, val loss 1.7484\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 5000: loss 1.4187, time 1444.89ms, mfu 0.01%\n",
      "iter 5100: loss 1.3377, time 90.48ms, mfu 0.01%\n",
      "iter 5200: loss 1.2940, time 121.25ms, mfu 0.01%\n",
      "step 5250: train loss 1.5693, val loss 1.7216\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 5300: loss 1.1211, time 90.54ms, mfu 0.01%\n",
      "iter 5400: loss 1.2982, time 81.03ms, mfu 0.01%\n",
      "step 5500: train loss 1.5935, val loss 1.6993\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 5500: loss 1.3698, time 1777.90ms, mfu 0.01%\n",
      "iter 5600: loss 1.3097, time 131.16ms, mfu 0.01%\n",
      "iter 5700: loss 1.2420, time 79.87ms, mfu 0.01%\n",
      "step 5750: train loss 1.5280, val loss 1.6751\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 5800: loss 1.2447, time 102.69ms, mfu 0.01%\n",
      "iter 5900: loss 1.2667, time 134.15ms, mfu 0.01%\n",
      "step 6000: train loss 1.5178, val loss 1.7047\n",
      "iter 6000: loss 1.3335, time 1118.58ms, mfu 0.01%\n",
      "iter 6100: loss 1.3542, time 85.87ms, mfu 0.01%\n",
      "iter 6200: loss 1.2263, time 390.87ms, mfu 0.01%\n",
      "step 6250: train loss 1.5395, val loss 1.6750\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 6300: loss 1.3399, time 147.92ms, mfu 0.01%\n",
      "iter 6400: loss 1.1544, time 102.99ms, mfu 0.01%\n",
      "step 6500: train loss 1.5252, val loss 1.6799\n",
      "iter 6500: loss 1.2558, time 1236.29ms, mfu 0.01%\n",
      "iter 6600: loss 1.3510, time 125.92ms, mfu 0.01%\n",
      "iter 6700: loss 1.2707, time 115.00ms, mfu 0.01%\n",
      "step 6750: train loss 1.5141, val loss 1.6793\n",
      "iter 6800: loss 1.2613, time 82.96ms, mfu 0.01%\n",
      "iter 6900: loss 1.2399, time 110.36ms, mfu 0.01%\n",
      "step 7000: train loss 1.4838, val loss 1.6896\n",
      "iter 7000: loss 1.3521, time 1145.39ms, mfu 0.01%\n",
      "iter 7100: loss 1.2881, time 121.50ms, mfu 0.01%\n",
      "iter 7200: loss 1.1719, time 125.56ms, mfu 0.01%\n",
      "step 7250: train loss 1.4879, val loss 1.6786\n",
      "iter 7300: loss 1.2877, time 118.22ms, mfu 0.01%\n",
      "iter 7400: loss 1.2828, time 146.29ms, mfu 0.01%\n",
      "step 7500: train loss 1.4691, val loss 1.6702\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.5_separated_embeddings\n",
      "iter 7500: loss 1.2596, time 1771.67ms, mfu 0.01%\n",
      "iter 7600: loss 1.2052, time 100.39ms, mfu 0.01%\n",
      "iter 7700: loss 1.1717, time 126.05ms, mfu 0.01%\n",
      "step 7750: train loss 1.4996, val loss 1.6747\n",
      "iter 7800: loss 1.2831, time 107.39ms, mfu 0.01%\n",
      "iter 7900: loss 1.2253, time 123.75ms, mfu 0.01%\n",
      "step 8000: train loss 1.4761, val loss 1.6745\n",
      "iter 8000: loss 1.2793, time 1780.11ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.5\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.4\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8602, time 1671.21ms, mfu -100.00%\n",
      "iter 100: loss 2.5996, time 112.72ms, mfu 0.01%\n",
      "iter 200: loss 2.2615, time 79.34ms, mfu 0.01%\n",
      "step 250: train loss 2.6068, val loss 2.6472\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 300: loss 2.0797, time 80.35ms, mfu 0.01%\n",
      "iter 400: loss 2.0349, time 134.54ms, mfu 0.01%\n",
      "step 500: train loss 2.4245, val loss 2.4526\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 500: loss 2.1625, time 1543.80ms, mfu 0.01%\n",
      "iter 600: loss 2.0078, time 176.87ms, mfu 0.01%\n",
      "iter 700: loss 1.8976, time 140.76ms, mfu 0.01%\n",
      "step 750: train loss 2.2833, val loss 2.3064\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 800: loss 1.9604, time 101.03ms, mfu 0.01%\n",
      "iter 900: loss 1.8267, time 98.18ms, mfu 0.01%\n",
      "step 1000: train loss 2.1390, val loss 2.2333\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 1000: loss 1.8767, time 1305.74ms, mfu 0.01%\n",
      "iter 1100: loss 1.6974, time 93.62ms, mfu 0.01%\n",
      "iter 1200: loss 1.6929, time 111.03ms, mfu 0.01%\n",
      "step 1250: train loss 2.0563, val loss 2.1467\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 1300: loss 1.6881, time 92.72ms, mfu 0.01%\n",
      "iter 1400: loss 1.6768, time 128.32ms, mfu 0.01%\n",
      "step 1500: train loss 1.9952, val loss 2.1228\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 1500: loss 1.6167, time 1286.01ms, mfu 0.01%\n",
      "iter 1600: loss 1.7373, time 193.83ms, mfu 0.01%\n",
      "iter 1700: loss 1.5331, time 89.96ms, mfu 0.01%\n",
      "step 1750: train loss 1.9617, val loss 2.0480\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 1800: loss 1.6270, time 137.11ms, mfu 0.01%\n",
      "iter 1900: loss 1.5532, time 88.60ms, mfu 0.01%\n",
      "step 2000: train loss 1.9028, val loss 2.0443\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 2000: loss 1.6833, time 1628.25ms, mfu 0.01%\n",
      "iter 2100: loss 1.5850, time 111.36ms, mfu 0.01%\n",
      "iter 2200: loss 1.5616, time 78.82ms, mfu 0.01%\n",
      "step 2250: train loss 1.8561, val loss 2.0315\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 2300: loss 1.5635, time 128.71ms, mfu 0.01%\n",
      "iter 2400: loss 1.4973, time 83.81ms, mfu 0.01%\n",
      "step 2500: train loss 1.8636, val loss 1.9790\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 2500: loss 1.5724, time 1486.14ms, mfu 0.01%\n",
      "iter 2600: loss 1.4612, time 183.18ms, mfu 0.01%\n",
      "iter 2700: loss 1.4330, time 117.73ms, mfu 0.01%\n",
      "step 2750: train loss 1.8078, val loss 1.9276\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 2800: loss 1.6268, time 103.25ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900: loss 1.4201, time 146.77ms, mfu 0.01%\n",
      "step 3000: train loss 1.8146, val loss 1.9514\n",
      "iter 3000: loss 1.4521, time 1048.42ms, mfu 0.01%\n",
      "iter 3100: loss 1.3730, time 260.22ms, mfu 0.01%\n",
      "iter 3200: loss 1.4208, time 114.06ms, mfu 0.01%\n",
      "step 3250: train loss 1.7397, val loss 1.8733\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 3300: loss 1.4810, time 79.96ms, mfu 0.01%\n",
      "iter 3400: loss 1.4387, time 302.94ms, mfu 0.01%\n",
      "step 3500: train loss 1.7194, val loss 1.9108\n",
      "iter 3500: loss 1.3194, time 1124.41ms, mfu 0.01%\n",
      "iter 3600: loss 1.3516, time 144.64ms, mfu 0.01%\n",
      "iter 3700: loss 1.3269, time 76.22ms, mfu 0.01%\n",
      "step 3750: train loss 1.6783, val loss 1.8315\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 3800: loss 1.3651, time 121.49ms, mfu 0.01%\n",
      "iter 3900: loss 1.2769, time 71.60ms, mfu 0.01%\n",
      "step 4000: train loss 1.6539, val loss 1.8856\n",
      "iter 4000: loss 1.4509, time 1186.20ms, mfu 0.01%\n",
      "iter 4100: loss 1.2934, time 111.79ms, mfu 0.01%\n",
      "iter 4200: loss 1.2416, time 133.72ms, mfu 0.01%\n",
      "step 4250: train loss 1.6419, val loss 1.8402\n",
      "iter 4300: loss 1.2850, time 91.01ms, mfu 0.01%\n",
      "iter 4400: loss 1.4572, time 122.42ms, mfu 0.01%\n",
      "step 4500: train loss 1.6501, val loss 1.8566\n",
      "iter 4500: loss 1.3819, time 1414.38ms, mfu 0.01%\n",
      "iter 4600: loss 1.4122, time 130.87ms, mfu 0.01%\n",
      "iter 4700: loss 1.3190, time 138.04ms, mfu 0.01%\n",
      "step 4750: train loss 1.6354, val loss 1.8497\n",
      "iter 4800: loss 1.3428, time 118.30ms, mfu 0.01%\n",
      "iter 4900: loss 1.2289, time 137.50ms, mfu 0.01%\n",
      "step 5000: train loss 1.5794, val loss 1.8112\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 5000: loss 1.4431, time 2278.45ms, mfu 0.01%\n",
      "iter 5100: loss 1.3234, time 113.14ms, mfu 0.01%\n",
      "iter 5200: loss 1.3133, time 116.37ms, mfu 0.01%\n",
      "step 5250: train loss 1.5975, val loss 1.7728\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 5300: loss 1.1401, time 138.81ms, mfu 0.01%\n",
      "iter 5400: loss 1.3087, time 123.42ms, mfu 0.01%\n",
      "step 5500: train loss 1.6303, val loss 1.7501\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 5500: loss 1.3901, time 1382.33ms, mfu 0.01%\n",
      "iter 5600: loss 1.3338, time 82.05ms, mfu 0.01%\n",
      "iter 5700: loss 1.2384, time 90.05ms, mfu 0.01%\n",
      "step 5750: train loss 1.5448, val loss 1.7234\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 5800: loss 1.2447, time 142.20ms, mfu 0.01%\n",
      "iter 5900: loss 1.2710, time 122.30ms, mfu 0.01%\n",
      "step 6000: train loss 1.5359, val loss 1.7449\n",
      "iter 6000: loss 1.3398, time 1045.78ms, mfu 0.01%\n",
      "iter 6100: loss 1.3490, time 88.17ms, mfu 0.01%\n",
      "iter 6200: loss 1.2412, time 130.81ms, mfu 0.01%\n",
      "step 6250: train loss 1.5570, val loss 1.7154\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 6300: loss 1.3493, time 98.75ms, mfu 0.01%\n",
      "iter 6400: loss 1.1586, time 88.62ms, mfu 0.01%\n",
      "step 6500: train loss 1.5422, val loss 1.7157\n",
      "iter 6500: loss 1.2524, time 1170.09ms, mfu 0.01%\n",
      "iter 6600: loss 1.3587, time 140.20ms, mfu 0.01%\n",
      "iter 6700: loss 1.2959, time 176.24ms, mfu 0.01%\n",
      "step 6750: train loss 1.5277, val loss 1.7017\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.4_separated_embeddings\n",
      "iter 6800: loss 1.2716, time 79.49ms, mfu 0.01%\n",
      "iter 6900: loss 1.2627, time 270.44ms, mfu 0.01%\n",
      "step 7000: train loss 1.4933, val loss 1.7214\n",
      "iter 7000: loss 1.3958, time 1213.98ms, mfu 0.01%\n",
      "iter 7100: loss 1.3004, time 131.24ms, mfu 0.01%\n",
      "iter 7200: loss 1.1745, time 110.95ms, mfu 0.01%\n",
      "step 7250: train loss 1.4958, val loss 1.7118\n",
      "iter 7300: loss 1.2871, time 132.85ms, mfu 0.01%\n",
      "iter 7400: loss 1.3210, time 132.38ms, mfu 0.01%\n",
      "step 7500: train loss 1.4686, val loss 1.7045\n",
      "iter 7500: loss 1.2760, time 1120.00ms, mfu 0.01%\n",
      "iter 7600: loss 1.2280, time 85.66ms, mfu 0.01%\n",
      "iter 7700: loss 1.1884, time 263.21ms, mfu 0.01%\n",
      "step 7750: train loss 1.5119, val loss 1.7088\n",
      "iter 7800: loss 1.3124, time 79.43ms, mfu 0.01%\n",
      "iter 7900: loss 1.2196, time 102.12ms, mfu 0.01%\n",
      "step 8000: train loss 1.4846, val loss 1.7166\n",
      "iter 8000: loss 1.2887, time 1369.81ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.4\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 0.3\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8611, time 1789.11ms, mfu -100.00%\n",
      "iter 100: loss 2.7715, time 91.70ms, mfu 0.01%\n",
      "iter 200: loss 2.3207, time 76.20ms, mfu 0.01%\n",
      "step 250: train loss 2.9769, val loss 3.0232\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 300: loss 2.0991, time 121.03ms, mfu 0.01%\n",
      "iter 400: loss 2.0590, time 146.72ms, mfu 0.01%\n",
      "step 500: train loss 2.7276, val loss 2.7595\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 500: loss 2.1911, time 1265.53ms, mfu 0.01%\n",
      "iter 600: loss 2.0639, time 79.89ms, mfu 0.01%\n",
      "iter 700: loss 1.9483, time 110.88ms, mfu 0.01%\n",
      "step 750: train loss 2.5928, val loss 2.6079\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 800: loss 1.9837, time 116.99ms, mfu 0.01%\n",
      "iter 900: loss 1.8588, time 102.88ms, mfu 0.01%\n",
      "step 1000: train loss 2.3807, val loss 2.4725\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 1000: loss 1.8841, time 1470.27ms, mfu 0.01%\n",
      "iter 1100: loss 1.7005, time 87.86ms, mfu 0.01%\n",
      "iter 1200: loss 1.7172, time 114.51ms, mfu 0.01%\n",
      "step 1250: train loss 2.3195, val loss 2.4330\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 1300: loss 1.7028, time 88.92ms, mfu 0.01%\n",
      "iter 1400: loss 1.7130, time 139.24ms, mfu 0.01%\n",
      "step 1500: train loss 2.2612, val loss 2.4015\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 1500: loss 1.6353, time 1330.72ms, mfu 0.01%\n",
      "iter 1600: loss 1.7634, time 71.76ms, mfu 0.01%\n",
      "iter 1700: loss 1.5298, time 94.18ms, mfu 0.01%\n",
      "step 1750: train loss 2.1889, val loss 2.2978\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 1800: loss 1.6287, time 112.57ms, mfu 0.01%\n",
      "iter 1900: loss 1.5476, time 98.04ms, mfu 0.01%\n",
      "step 2000: train loss 2.1567, val loss 2.3285\n",
      "iter 2000: loss 1.6660, time 1501.03ms, mfu 0.01%\n",
      "iter 2100: loss 1.5750, time 124.46ms, mfu 0.01%\n",
      "iter 2200: loss 1.6023, time 120.34ms, mfu 0.01%\n",
      "step 2250: train loss 2.0883, val loss 2.3009\n",
      "iter 2300: loss 1.5797, time 92.07ms, mfu 0.01%\n",
      "iter 2400: loss 1.4692, time 115.06ms, mfu 0.01%\n",
      "step 2500: train loss 2.0803, val loss 2.2370\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 2500: loss 1.5535, time 1309.75ms, mfu 0.01%\n",
      "iter 2600: loss 1.4484, time 107.42ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2700: loss 1.4514, time 108.52ms, mfu 0.01%\n",
      "step 2750: train loss 2.0312, val loss 2.1636\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 2800: loss 1.6342, time 110.18ms, mfu 0.01%\n",
      "iter 2900: loss 1.4258, time 89.16ms, mfu 0.01%\n",
      "step 3000: train loss 2.0237, val loss 2.2043\n",
      "iter 3000: loss 1.4526, time 1225.82ms, mfu 0.01%\n",
      "iter 3100: loss 1.3754, time 119.08ms, mfu 0.01%\n",
      "iter 3200: loss 1.4061, time 131.85ms, mfu 0.01%\n",
      "step 3250: train loss 1.9328, val loss 2.1000\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 3300: loss 1.4888, time 119.90ms, mfu 0.01%\n",
      "iter 3400: loss 1.4583, time 78.49ms, mfu 0.01%\n",
      "step 3500: train loss 1.9062, val loss 2.1479\n",
      "iter 3500: loss 1.3320, time 1062.69ms, mfu 0.01%\n",
      "iter 3600: loss 1.3494, time 99.71ms, mfu 0.01%\n",
      "iter 3700: loss 1.3502, time 133.08ms, mfu 0.01%\n",
      "step 3750: train loss 1.8616, val loss 2.0802\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 3800: loss 1.3705, time 86.93ms, mfu 0.01%\n",
      "iter 3900: loss 1.2803, time 93.53ms, mfu 0.01%\n",
      "step 4000: train loss 1.8369, val loss 2.1210\n",
      "iter 4000: loss 1.4439, time 1351.93ms, mfu 0.01%\n",
      "iter 4100: loss 1.2812, time 163.12ms, mfu 0.01%\n",
      "iter 4200: loss 1.2585, time 78.64ms, mfu 0.01%\n",
      "step 4250: train loss 1.8144, val loss 2.0564\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 4300: loss 1.2768, time 117.74ms, mfu 0.01%\n",
      "iter 4400: loss 1.4336, time 102.49ms, mfu 0.01%\n",
      "step 4500: train loss 1.8430, val loss 2.0898\n",
      "iter 4500: loss 1.4030, time 1406.44ms, mfu 0.01%\n",
      "iter 4600: loss 1.4174, time 264.04ms, mfu 0.01%\n",
      "iter 4700: loss 1.3455, time 119.00ms, mfu 0.01%\n",
      "step 4750: train loss 1.8097, val loss 2.0797\n",
      "iter 4800: loss 1.3506, time 108.98ms, mfu 0.01%\n",
      "iter 4900: loss 1.2140, time 112.08ms, mfu 0.01%\n",
      "step 5000: train loss 1.7656, val loss 2.0510\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 5000: loss 1.4311, time 1279.83ms, mfu 0.01%\n",
      "iter 5100: loss 1.3047, time 123.85ms, mfu 0.01%\n",
      "iter 5200: loss 1.2895, time 151.47ms, mfu 0.01%\n",
      "step 5250: train loss 1.7553, val loss 1.9897\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 5300: loss 1.1429, time 124.98ms, mfu 0.01%\n",
      "iter 5400: loss 1.3054, time 109.48ms, mfu 0.01%\n",
      "step 5500: train loss 1.8076, val loss 1.9530\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 5500: loss 1.3831, time 1479.99ms, mfu 0.01%\n",
      "iter 5600: loss 1.3276, time 305.42ms, mfu 0.01%\n",
      "iter 5700: loss 1.2395, time 112.00ms, mfu 0.01%\n",
      "step 5750: train loss 1.7264, val loss 1.9400\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 5800: loss 1.2478, time 105.83ms, mfu 0.01%\n",
      "iter 5900: loss 1.2786, time 135.09ms, mfu 0.01%\n",
      "step 6000: train loss 1.6928, val loss 1.9392\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 6000: loss 1.3566, time 1280.09ms, mfu 0.01%\n",
      "iter 6100: loss 1.3775, time 116.36ms, mfu 0.01%\n",
      "iter 6200: loss 1.2421, time 119.58ms, mfu 0.01%\n",
      "step 6250: train loss 1.7266, val loss 1.9208\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 6300: loss 1.3489, time 82.97ms, mfu 0.01%\n",
      "iter 6400: loss 1.1617, time 136.58ms, mfu 0.01%\n",
      "step 6500: train loss 1.6994, val loss 1.9169\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 6500: loss 1.2517, time 1192.83ms, mfu 0.01%\n",
      "iter 6600: loss 1.3466, time 76.49ms, mfu 0.01%\n",
      "iter 6700: loss 1.2845, time 132.12ms, mfu 0.01%\n",
      "step 6750: train loss 1.6774, val loss 1.9007\n",
      "saving checkpoint to out/proposed_margin_1.0_T_0.3_separated_embeddings\n",
      "iter 6800: loss 1.2673, time 110.08ms, mfu 0.01%\n",
      "iter 6900: loss 1.2609, time 83.39ms, mfu 0.01%\n",
      "step 7000: train loss 1.6422, val loss 1.9230\n",
      "iter 7000: loss 1.3918, time 1335.06ms, mfu 0.01%\n",
      "iter 7100: loss 1.2961, time 76.77ms, mfu 0.01%\n",
      "iter 7200: loss 1.1883, time 95.22ms, mfu 0.01%\n",
      "step 7250: train loss 1.6477, val loss 1.9110\n",
      "iter 7300: loss 1.2803, time 120.12ms, mfu 0.01%\n",
      "iter 7400: loss 1.3098, time 128.63ms, mfu 0.01%\n",
      "step 7500: train loss 1.6197, val loss 1.9132\n",
      "iter 7500: loss 1.2812, time 1180.87ms, mfu 0.01%\n",
      "iter 7600: loss 1.2228, time 117.16ms, mfu 0.01%\n",
      "iter 7700: loss 1.1788, time 130.00ms, mfu 0.01%\n",
      "step 7750: train loss 1.6718, val loss 1.9101\n",
      "iter 7800: loss 1.3049, time 125.40ms, mfu 0.01%\n",
      "iter 7900: loss 1.2332, time 81.80ms, mfu 0.01%\n",
      "step 8000: train loss 1.6417, val loss 1.9229\n",
      "iter 8000: loss 1.2670, time 1128.35ms, mfu 0.01%\n",
      "done for margin=1.0, T=0.3\n"
     ]
    }
   ],
   "source": [
    "margin = 1.0\n",
    "for temperature in [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"temperature\": temperature,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}, T={temperature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "131f0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.0\n",
      "Overriding: temperature = 1.0\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8321, time 792.90ms, mfu -100.00%\n",
      "iter 100: loss 2.2954, time 136.01ms, mfu 0.01%\n",
      "iter 200: loss 2.2312, time 83.06ms, mfu 0.01%\n",
      "step 250: train loss 2.9973, val loss 3.0214\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 300: loss 2.0583, time 43.36ms, mfu 0.01%\n",
      "iter 400: loss 2.0045, time 68.75ms, mfu 0.01%\n",
      "step 500: train loss 2.8148, val loss 2.8296\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 500: loss 2.0596, time 992.77ms, mfu 0.01%\n",
      "iter 600: loss 1.9194, time 100.48ms, mfu 0.01%\n",
      "iter 700: loss 1.8230, time 74.09ms, mfu 0.01%\n",
      "step 750: train loss 2.6691, val loss 2.6905\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 800: loss 1.9144, time 51.13ms, mfu 0.01%\n",
      "iter 900: loss 1.7526, time 81.98ms, mfu 0.01%\n",
      "step 1000: train loss 2.5676, val loss 2.6273\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 1000: loss 1.7817, time 716.26ms, mfu 0.01%\n",
      "iter 1100: loss 1.5963, time 50.76ms, mfu 0.01%\n",
      "iter 1200: loss 1.6248, time 92.87ms, mfu 0.01%\n",
      "step 1250: train loss 2.5192, val loss 2.5571\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 1300: loss 1.6245, time 72.82ms, mfu 0.01%\n",
      "iter 1400: loss 1.6226, time 60.64ms, mfu 0.01%\n",
      "step 1500: train loss 2.4306, val loss 2.5012\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 1500: loss 1.5595, time 772.69ms, mfu 0.01%\n",
      "iter 1600: loss 1.6617, time 75.86ms, mfu 0.01%\n",
      "iter 1700: loss 1.4951, time 109.47ms, mfu 0.01%\n",
      "step 1750: train loss 2.4067, val loss 2.4529\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 1800: loss 1.5951, time 71.68ms, mfu 0.01%\n",
      "iter 1900: loss 1.4859, time 55.36ms, mfu 0.02%\n",
      "step 2000: train loss 2.3731, val loss 2.4470\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 2000: loss 1.5977, time 908.14ms, mfu 0.01%\n",
      "iter 2100: loss 1.5007, time 82.14ms, mfu 0.01%\n",
      "iter 2200: loss 1.4842, time 52.74ms, mfu 0.01%\n",
      "step 2250: train loss 2.3207, val loss 2.4012\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 2300: loss 1.4938, time 68.67ms, mfu 0.02%\n",
      "iter 2400: loss 1.4365, time 190.28ms, mfu 0.01%\n",
      "step 2500: train loss 2.3312, val loss 2.3948\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 2500: loss 1.4922, time 934.84ms, mfu 0.01%\n",
      "iter 2600: loss 1.3835, time 99.38ms, mfu 0.01%\n",
      "iter 2700: loss 1.3882, time 59.63ms, mfu 0.01%\n",
      "step 2750: train loss 2.3057, val loss 2.3710\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 2800: loss 1.5944, time 69.42ms, mfu 0.01%\n",
      "iter 2900: loss 1.4135, time 111.12ms, mfu 0.01%\n",
      "step 3000: train loss 2.3223, val loss 2.3966\n",
      "iter 3000: loss 1.4291, time 962.97ms, mfu 0.01%\n",
      "iter 3100: loss 1.3431, time 89.15ms, mfu 0.01%\n",
      "iter 3200: loss 1.3921, time 88.35ms, mfu 0.01%\n",
      "step 3250: train loss 2.2634, val loss 2.3421\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 3300: loss 1.4560, time 60.08ms, mfu 0.01%\n",
      "iter 3400: loss 1.3927, time 75.04ms, mfu 0.01%\n",
      "step 3500: train loss 2.2564, val loss 2.3491\n",
      "iter 3500: loss 1.3070, time 1034.11ms, mfu 0.01%\n",
      "iter 3600: loss 1.3129, time 102.29ms, mfu 0.01%\n",
      "iter 3700: loss 1.3082, time 126.97ms, mfu 0.01%\n",
      "step 3750: train loss 2.2387, val loss 2.3262\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 3800: loss 1.3451, time 60.02ms, mfu 0.01%\n",
      "iter 3900: loss 1.2609, time 61.84ms, mfu 0.01%\n",
      "step 4000: train loss 2.2086, val loss 2.3278\n",
      "iter 4000: loss 1.4231, time 840.30ms, mfu 0.01%\n",
      "iter 4100: loss 1.2661, time 98.99ms, mfu 0.01%\n",
      "iter 4200: loss 1.2210, time 96.17ms, mfu 0.01%\n",
      "step 4250: train loss 2.2121, val loss 2.3161\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 4300: loss 1.2483, time 90.98ms, mfu 0.01%\n",
      "iter 4400: loss 1.3970, time 64.46ms, mfu 0.01%\n",
      "step 4500: train loss 2.1993, val loss 2.3022\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 4500: loss 1.3438, time 902.17ms, mfu 0.01%\n",
      "iter 4600: loss 1.4031, time 74.53ms, mfu 0.01%\n",
      "iter 4700: loss 1.2840, time 88.01ms, mfu 0.01%\n",
      "step 4750: train loss 2.2031, val loss 2.3139\n",
      "iter 4800: loss 1.3293, time 130.04ms, mfu 0.01%\n",
      "iter 4900: loss 1.2022, time 61.32ms, mfu 0.01%\n",
      "step 5000: train loss 2.1608, val loss 2.2782\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 5000: loss 1.3989, time 1161.39ms, mfu 0.01%\n",
      "iter 5100: loss 1.2956, time 177.56ms, mfu 0.01%\n",
      "iter 5200: loss 1.2745, time 74.01ms, mfu 0.01%\n",
      "step 5250: train loss 2.1791, val loss 2.2664\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 5300: loss 1.1424, time 66.49ms, mfu 0.01%\n",
      "iter 5400: loss 1.3209, time 57.32ms, mfu 0.01%\n",
      "step 5500: train loss 2.1915, val loss 2.2570\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 5500: loss 1.3553, time 1244.10ms, mfu 0.01%\n",
      "iter 5600: loss 1.3319, time 65.53ms, mfu 0.01%\n",
      "iter 5700: loss 1.2282, time 73.94ms, mfu 0.01%\n",
      "step 5750: train loss 2.1565, val loss 2.2472\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 5800: loss 1.2473, time 118.82ms, mfu 0.01%\n",
      "iter 5900: loss 1.2561, time 59.48ms, mfu 0.01%\n",
      "step 6000: train loss 2.1461, val loss 2.2575\n",
      "iter 6000: loss 1.2814, time 1500.22ms, mfu 0.01%\n",
      "iter 6100: loss 1.3293, time 114.12ms, mfu 0.01%\n",
      "iter 6200: loss 1.2420, time 88.94ms, mfu 0.01%\n",
      "step 6250: train loss 2.1587, val loss 2.2434\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 6300: loss 1.2902, time 98.24ms, mfu 0.01%\n",
      "iter 6400: loss 1.1416, time 142.43ms, mfu 0.01%\n",
      "step 6500: train loss 2.1483, val loss 2.2466\n",
      "iter 6500: loss 1.2460, time 2136.37ms, mfu 0.01%\n",
      "iter 6600: loss 1.2812, time 93.67ms, mfu 0.01%\n",
      "iter 6700: loss 1.2188, time 79.68ms, mfu 0.01%\n",
      "step 6750: train loss 2.1473, val loss 2.2475\n",
      "iter 6800: loss 1.2428, time 71.35ms, mfu 0.01%\n",
      "iter 6900: loss 1.2445, time 109.62ms, mfu 0.01%\n",
      "step 7000: train loss 2.1244, val loss 2.2457\n",
      "iter 7000: loss 1.2982, time 1212.38ms, mfu 0.01%\n",
      "iter 7100: loss 1.2536, time 60.98ms, mfu 0.01%\n",
      "iter 7200: loss 1.1549, time 62.10ms, mfu 0.01%\n",
      "step 7250: train loss 2.1241, val loss 2.2408\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 7300: loss 1.2639, time 428.19ms, mfu 0.01%\n",
      "iter 7400: loss 1.2660, time 96.34ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: train loss 2.1152, val loss 2.2345\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 7500: loss 1.2565, time 1637.51ms, mfu 0.01%\n",
      "iter 7600: loss 1.2141, time 134.54ms, mfu 0.01%\n",
      "iter 7700: loss 1.1772, time 87.71ms, mfu 0.01%\n",
      "step 7750: train loss 2.1249, val loss 2.2322\n",
      "saving checkpoint to out/proposed_margin_1.0_T_1.0_separated_embeddings\n",
      "iter 7800: loss 1.2702, time 63.71ms, mfu 0.01%\n",
      "iter 7900: loss 1.2139, time 80.01ms, mfu 0.01%\n",
      "step 8000: train loss 2.1069, val loss 2.2356\n",
      "iter 8000: loss 1.2710, time 983.52ms, mfu 0.01%\n",
      "done for margin=1.0, T=1.0\n"
     ]
    }
   ],
   "source": [
    "margin = 1.0\n",
    "for temperature in [1.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"temperature\": temperature,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}, T={temperature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5990df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2964746794.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.0_T_1.0_separated_embeddings/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "margin = 1.0\n",
    "for temperature in [1.0]: # , 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]:\n",
    "    out_path = f\"out/proposed_margin_{margin}_T_{temperature}_separated_embeddings\"\n",
    "    ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    old_ckpt_path = f\"{out_path}/ckpt_old.pt\"\n",
    "    torch.save(checkpoint, old_ckpt_path)\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4ec74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 4.0\n",
      "Overriding: weight_tying = False\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 139, with 17,792 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8823, val loss 4.8816\n",
      "iter 0: loss 4.8817, time 1954.15ms, mfu -100.00%\n",
      "iter 100: loss 2.6193, time 82.56ms, mfu 0.02%\n",
      "iter 200: loss 2.4761, time 176.47ms, mfu 0.02%\n",
      "step 250: train loss 2.4796, val loss 2.4999\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 300: loss 2.2832, time 181.02ms, mfu 0.01%\n",
      "iter 400: loss 2.2288, time 108.35ms, mfu 0.01%\n",
      "step 500: train loss 2.2746, val loss 2.2971\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 500: loss 2.3081, time 1153.60ms, mfu 0.01%\n",
      "iter 600: loss 2.1505, time 69.59ms, mfu 0.01%\n",
      "iter 700: loss 2.0657, time 99.45ms, mfu 0.01%\n",
      "step 750: train loss 2.1390, val loss 2.1585\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 800: loss 2.1561, time 125.52ms, mfu 0.01%\n",
      "iter 900: loss 1.9877, time 111.21ms, mfu 0.01%\n",
      "step 1000: train loss 2.0077, val loss 2.0959\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 1000: loss 1.9898, time 667.77ms, mfu 0.01%\n",
      "iter 1100: loss 1.8121, time 66.77ms, mfu 0.01%\n",
      "iter 1200: loss 1.8214, time 96.85ms, mfu 0.01%\n",
      "step 1250: train loss 1.9376, val loss 2.0159\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 1300: loss 1.8651, time 129.16ms, mfu 0.01%\n",
      "iter 1400: loss 1.8917, time 147.72ms, mfu 0.01%\n",
      "step 1500: train loss 1.8503, val loss 1.9563\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 1500: loss 1.7463, time 1276.99ms, mfu 0.01%\n",
      "iter 1600: loss 1.8729, time 57.27ms, mfu 0.01%\n",
      "iter 1700: loss 1.6809, time 105.75ms, mfu 0.01%\n",
      "step 1750: train loss 1.8216, val loss 1.8946\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 1800: loss 1.8058, time 57.79ms, mfu 0.01%\n",
      "iter 1900: loss 1.6941, time 79.50ms, mfu 0.01%\n",
      "step 2000: train loss 1.7744, val loss 1.8976\n",
      "iter 2000: loss 1.8217, time 685.28ms, mfu 0.01%\n",
      "iter 2100: loss 1.7313, time 108.45ms, mfu 0.01%\n",
      "iter 2200: loss 1.7010, time 70.86ms, mfu 0.01%\n",
      "step 2250: train loss 1.7285, val loss 1.8632\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 2300: loss 1.7022, time 87.98ms, mfu 0.01%\n",
      "iter 2400: loss 1.6377, time 353.18ms, mfu 0.01%\n",
      "step 2500: train loss 1.7312, val loss 1.8457\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 2500: loss 1.7167, time 907.54ms, mfu 0.01%\n",
      "iter 2600: loss 1.5991, time 109.07ms, mfu 0.01%\n",
      "iter 2700: loss 1.5773, time 81.33ms, mfu 0.01%\n",
      "step 2750: train loss 1.6866, val loss 1.8025\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 2800: loss 1.8337, time 92.64ms, mfu 0.01%\n",
      "iter 2900: loss 1.5987, time 108.65ms, mfu 0.01%\n",
      "step 3000: train loss 1.7015, val loss 1.8252\n",
      "iter 3000: loss 1.6110, time 914.79ms, mfu 0.01%\n",
      "iter 3100: loss 1.5084, time 109.76ms, mfu 0.01%\n",
      "iter 3200: loss 1.5626, time 139.77ms, mfu 0.01%\n",
      "step 3250: train loss 1.6428, val loss 1.7788\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 3300: loss 1.6319, time 110.18ms, mfu 0.01%\n",
      "iter 3400: loss 1.5624, time 111.37ms, mfu 0.01%\n",
      "step 3500: train loss 1.6110, val loss 1.7769\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 3500: loss 1.4826, time 1228.91ms, mfu 0.01%\n",
      "iter 3600: loss 1.4642, time 146.99ms, mfu 0.01%\n",
      "iter 3700: loss 1.4946, time 100.78ms, mfu 0.01%\n",
      "step 3750: train loss 1.5846, val loss 1.7261\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 3800: loss 1.5316, time 118.36ms, mfu 0.01%\n",
      "iter 3900: loss 1.4358, time 81.68ms, mfu 0.01%\n",
      "step 4000: train loss 1.5577, val loss 1.7633\n",
      "iter 4000: loss 1.6019, time 1008.61ms, mfu 0.01%\n",
      "iter 4100: loss 1.4295, time 98.40ms, mfu 0.01%\n",
      "iter 4200: loss 1.3466, time 116.83ms, mfu 0.01%\n",
      "step 4250: train loss 1.5534, val loss 1.7308\n",
      "iter 4300: loss 1.4298, time 99.07ms, mfu 0.01%\n",
      "iter 4400: loss 1.5800, time 139.76ms, mfu 0.01%\n",
      "step 4500: train loss 1.5460, val loss 1.7384\n",
      "iter 4500: loss 1.5149, time 1254.52ms, mfu 0.01%\n",
      "iter 4600: loss 1.5809, time 91.11ms, mfu 0.01%\n",
      "iter 4700: loss 1.4553, time 157.95ms, mfu 0.01%\n",
      "step 4750: train loss 1.5331, val loss 1.7316\n",
      "iter 4800: loss 1.5262, time 104.74ms, mfu 0.01%\n",
      "iter 4900: loss 1.3489, time 79.98ms, mfu 0.01%\n",
      "step 5000: train loss 1.4842, val loss 1.6985\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 5000: loss 1.6270, time 1124.91ms, mfu 0.01%\n",
      "iter 5100: loss 1.4279, time 77.20ms, mfu 0.01%\n",
      "iter 5200: loss 1.4508, time 125.18ms, mfu 0.01%\n",
      "step 5250: train loss 1.4994, val loss 1.6590\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 5300: loss 1.2801, time 125.68ms, mfu 0.01%\n",
      "iter 5400: loss 1.4546, time 80.32ms, mfu 0.01%\n",
      "step 5500: train loss 1.5218, val loss 1.6392\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 5500: loss 1.5555, time 1025.36ms, mfu 0.01%\n",
      "iter 5600: loss 1.4989, time 133.97ms, mfu 0.01%\n",
      "iter 5700: loss 1.3712, time 128.95ms, mfu 0.01%\n",
      "step 5750: train loss 1.4630, val loss 1.6170\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 5800: loss 1.3881, time 90.08ms, mfu 0.01%\n",
      "iter 5900: loss 1.4317, time 85.35ms, mfu 0.01%\n",
      "step 6000: train loss 1.4473, val loss 1.6455\n",
      "iter 6000: loss 1.4842, time 929.15ms, mfu 0.01%\n",
      "iter 6100: loss 1.5037, time 222.06ms, mfu 0.01%\n",
      "iter 6200: loss 1.3853, time 81.47ms, mfu 0.01%\n",
      "step 6250: train loss 1.4738, val loss 1.6089\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying_SE\n",
      "iter 6300: loss 1.4899, time 108.78ms, mfu 0.01%\n",
      "iter 6400: loss 1.3079, time 105.00ms, mfu 0.01%\n",
      "step 6500: train loss 1.4595, val loss 1.6376\n",
      "iter 6500: loss 1.4132, time 988.44ms, mfu 0.01%\n",
      "iter 6600: loss 1.4915, time 101.45ms, mfu 0.01%\n",
      "iter 6700: loss 1.4017, time 116.17ms, mfu 0.01%\n",
      "step 6750: train loss 1.4460, val loss 1.6201\n",
      "iter 6800: loss 1.3978, time 120.12ms, mfu 0.01%\n",
      "iter 6900: loss 1.4034, time 123.34ms, mfu 0.01%\n",
      "step 7000: train loss 1.4191, val loss 1.6301\n",
      "iter 7000: loss 1.5141, time 919.68ms, mfu 0.01%\n",
      "iter 7100: loss 1.4296, time 84.12ms, mfu 0.01%\n",
      "iter 7200: loss 1.3274, time 90.08ms, mfu 0.01%\n",
      "step 7250: train loss 1.4147, val loss 1.6257\n",
      "iter 7300: loss 1.4357, time 124.10ms, mfu 0.01%\n",
      "iter 7400: loss 1.4505, time 136.48ms, mfu 0.01%\n",
      "step 7500: train loss 1.3958, val loss 1.6162\n",
      "iter 7500: loss 1.3928, time 1145.91ms, mfu 0.01%\n",
      "iter 7600: loss 1.3625, time 87.63ms, mfu 0.01%\n",
      "iter 7700: loss 1.3257, time 128.07ms, mfu 0.01%\n",
      "step 7750: train loss 1.4294, val loss 1.6254\n",
      "iter 7800: loss 1.4702, time 81.89ms, mfu 0.01%\n",
      "iter 7900: loss 1.3559, time 91.51ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000: train loss 1.4052, val loss 1.6223\n",
      "iter 8000: loss 1.4203, time 1267.38ms, mfu 0.01%\n",
      "done for margin=4.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [4.0]: # [1.0, 2.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_wo_weight_tying_SE\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"weight_tying\": \"False\",\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a6a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/baseline_wo_weight_tying_SE\n",
      "Overriding: weight_tying = False\n",
      "Overriding: separated_embeddings = True\n",
      "Overriding: model_type = baseline\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 139, with 17,792 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8823, val loss 4.8816\n",
      "iter 0: loss 4.8817, time 1016.93ms, mfu -100.00%\n",
      "iter 100: loss 2.6564, time 88.23ms, mfu 0.01%\n",
      "iter 200: loss 2.4985, time 122.44ms, mfu 0.01%\n",
      "step 250: train loss 2.4626, val loss 2.4805\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 300: loss 2.2995, time 94.86ms, mfu 0.01%\n",
      "iter 400: loss 2.2490, time 180.91ms, mfu 0.01%\n",
      "step 500: train loss 2.2555, val loss 2.2731\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 500: loss 2.3359, time 718.82ms, mfu 0.01%\n",
      "iter 600: loss 2.1791, time 97.69ms, mfu 0.01%\n",
      "iter 700: loss 2.1024, time 79.34ms, mfu 0.01%\n",
      "step 750: train loss 2.1180, val loss 2.1394\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 800: loss 2.1669, time 69.33ms, mfu 0.01%\n",
      "iter 900: loss 2.0194, time 89.58ms, mfu 0.01%\n",
      "step 1000: train loss 2.0024, val loss 2.0970\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 1000: loss 2.0497, time 843.27ms, mfu 0.01%\n",
      "iter 1100: loss 1.8753, time 134.34ms, mfu 0.01%\n",
      "iter 1200: loss 1.8439, time 61.21ms, mfu 0.01%\n",
      "step 1250: train loss 1.9240, val loss 2.0108\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 1300: loss 1.8750, time 59.96ms, mfu 0.01%\n",
      "iter 1400: loss 1.9430, time 104.95ms, mfu 0.01%\n",
      "step 1500: train loss 1.8451, val loss 1.9634\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 1500: loss 1.7915, time 2052.33ms, mfu 0.01%\n",
      "iter 1600: loss 1.9040, time 177.34ms, mfu 0.01%\n",
      "iter 1700: loss 1.7204, time 180.31ms, mfu 0.01%\n",
      "step 1750: train loss 1.8033, val loss 1.8831\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 1800: loss 1.8281, time 141.68ms, mfu 0.01%\n",
      "iter 1900: loss 1.7424, time 192.19ms, mfu 0.01%\n",
      "step 2000: train loss 1.7557, val loss 1.8924\n",
      "iter 2000: loss 1.8303, time 1982.55ms, mfu 0.01%\n",
      "iter 2100: loss 1.7489, time 64.42ms, mfu 0.01%\n",
      "iter 2200: loss 1.7205, time 102.96ms, mfu 0.01%\n",
      "step 2250: train loss 1.7075, val loss 1.8556\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 2300: loss 1.7378, time 63.86ms, mfu 0.01%\n",
      "iter 2400: loss 1.6734, time 98.84ms, mfu 0.01%\n",
      "step 2500: train loss 1.7135, val loss 1.8286\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 2500: loss 1.7248, time 827.85ms, mfu 0.01%\n",
      "iter 2600: loss 1.6652, time 88.18ms, mfu 0.01%\n",
      "iter 2700: loss 1.6351, time 145.34ms, mfu 0.01%\n",
      "step 2750: train loss 1.6789, val loss 1.8037\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 2800: loss 1.8569, time 89.65ms, mfu 0.01%\n",
      "iter 2900: loss 1.6240, time 55.55ms, mfu 0.01%\n",
      "step 3000: train loss 1.6847, val loss 1.8269\n",
      "iter 3000: loss 1.6512, time 587.65ms, mfu 0.01%\n",
      "iter 3100: loss 1.5423, time 87.15ms, mfu 0.01%\n",
      "iter 3200: loss 1.5630, time 97.57ms, mfu 0.01%\n",
      "step 3250: train loss 1.6334, val loss 1.7725\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 3300: loss 1.6753, time 60.63ms, mfu 0.01%\n",
      "iter 3400: loss 1.5831, time 96.48ms, mfu 0.01%\n",
      "step 3500: train loss 1.5983, val loss 1.7735\n",
      "iter 3500: loss 1.5166, time 550.48ms, mfu 0.01%\n",
      "iter 3600: loss 1.4864, time 103.62ms, mfu 0.01%\n",
      "iter 3700: loss 1.4966, time 58.94ms, mfu 0.01%\n",
      "step 3750: train loss 1.5718, val loss 1.7299\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 3800: loss 1.5923, time 66.12ms, mfu 0.01%\n",
      "iter 3900: loss 1.4479, time 58.41ms, mfu 0.01%\n",
      "step 4000: train loss 1.5465, val loss 1.7520\n",
      "iter 4000: loss 1.6775, time 629.20ms, mfu 0.01%\n",
      "iter 4100: loss 1.4450, time 152.39ms, mfu 0.01%\n",
      "iter 4200: loss 1.3961, time 92.44ms, mfu 0.01%\n",
      "step 4250: train loss 1.5362, val loss 1.7423\n",
      "iter 4300: loss 1.4273, time 108.45ms, mfu 0.01%\n",
      "iter 4400: loss 1.5775, time 151.76ms, mfu 0.01%\n",
      "step 4500: train loss 1.5391, val loss 1.7567\n",
      "iter 4500: loss 1.5781, time 759.02ms, mfu 0.01%\n",
      "iter 4600: loss 1.6110, time 69.04ms, mfu 0.01%\n",
      "iter 4700: loss 1.4699, time 98.28ms, mfu 0.01%\n",
      "step 4750: train loss 1.5183, val loss 1.7289\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 4800: loss 1.5508, time 93.50ms, mfu 0.01%\n",
      "iter 4900: loss 1.3789, time 104.68ms, mfu 0.01%\n",
      "step 5000: train loss 1.4732, val loss 1.7017\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 5000: loss 1.6473, time 894.17ms, mfu 0.01%\n",
      "iter 5100: loss 1.4901, time 113.55ms, mfu 0.01%\n",
      "iter 5200: loss 1.4776, time 110.84ms, mfu 0.01%\n",
      "step 5250: train loss 1.4913, val loss 1.6667\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 5300: loss 1.2996, time 65.83ms, mfu 0.01%\n",
      "iter 5400: loss 1.4912, time 266.82ms, mfu 0.01%\n",
      "step 5500: train loss 1.5184, val loss 1.6439\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 5500: loss 1.5955, time 864.26ms, mfu 0.01%\n",
      "iter 5600: loss 1.5368, time 75.05ms, mfu 0.01%\n",
      "iter 5700: loss 1.3752, time 71.86ms, mfu 0.01%\n",
      "step 5750: train loss 1.4464, val loss 1.6303\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 5800: loss 1.4288, time 114.05ms, mfu 0.01%\n",
      "iter 5900: loss 1.4587, time 67.85ms, mfu 0.01%\n",
      "step 6000: train loss 1.4330, val loss 1.6543\n",
      "iter 6000: loss 1.4948, time 958.34ms, mfu 0.01%\n",
      "iter 6100: loss 1.5384, time 71.43ms, mfu 0.01%\n",
      "iter 6200: loss 1.4077, time 122.39ms, mfu 0.01%\n",
      "step 6250: train loss 1.4581, val loss 1.6216\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 6300: loss 1.5085, time 170.59ms, mfu 0.01%\n",
      "iter 6400: loss 1.3094, time 125.85ms, mfu 0.01%\n",
      "step 6500: train loss 1.4458, val loss 1.6431\n",
      "iter 6500: loss 1.4404, time 857.91ms, mfu 0.01%\n",
      "iter 6600: loss 1.5331, time 90.83ms, mfu 0.01%\n",
      "iter 6700: loss 1.4404, time 72.10ms, mfu 0.01%\n",
      "step 6750: train loss 1.4361, val loss 1.6370\n",
      "iter 6800: loss 1.4303, time 74.55ms, mfu 0.01%\n",
      "iter 6900: loss 1.4442, time 79.42ms, mfu 0.01%\n",
      "step 7000: train loss 1.4019, val loss 1.6412\n",
      "iter 7000: loss 1.5237, time 1392.56ms, mfu 0.01%\n",
      "iter 7100: loss 1.4488, time 250.57ms, mfu 0.01%\n",
      "iter 7200: loss 1.3631, time 209.99ms, mfu 0.01%\n",
      "step 7250: train loss 1.4008, val loss 1.6211\n",
      "saving checkpoint to out/baseline_wo_weight_tying_SE\n",
      "iter 7300: loss 1.4720, time 120.15ms, mfu 0.01%\n",
      "iter 7400: loss 1.4893, time 112.43ms, mfu 0.01%\n",
      "step 7500: train loss 1.3846, val loss 1.6255\n",
      "iter 7500: loss 1.4053, time 2258.85ms, mfu 0.01%\n",
      "iter 7600: loss 1.3885, time 184.61ms, mfu 0.01%\n",
      "iter 7700: loss 1.3587, time 131.15ms, mfu 0.01%\n",
      "step 7750: train loss 1.4231, val loss 1.6290\n",
      "iter 7800: loss 1.5071, time 119.82ms, mfu 0.01%\n",
      "iter 7900: loss 1.4048, time 223.43ms, mfu 0.01%\n",
      "step 8000: train loss 1.3967, val loss 1.6338\n",
      "iter 8000: loss 1.5008, time 1410.09ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "#baseline wo weight_tying\n",
    "args = {\n",
    "    \"out_dir\": \"out/baseline_wo_weight_tying_SE\",\n",
    "    \"weight_tying\": \"False\",\n",
    "    \"separated_embeddings\": \"True\",\n",
    "    \"model_type\": \"baseline\"\n",
    "}\n",
    "run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89bb6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/3673994607.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_4.0_wo_weight_tying_SE/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "margin = 1.0\n",
    "for model_name in [\n",
    "    'proposed_margin_1.0_wo_weight_tying_SE',\n",
    "    'proposed_margin_2.0_wo_weight_tying_SE',\n",
    "    'proposed_margin_4.0_wo_weight_tying_SE'\n",
    "    \"baseline_wo_weight_tying_SE\",\n",
    "]:\n",
    "    out_path = f\"out/{model_name}\"\n",
    "    ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    old_ckpt_path = f\"{out_path}/ckpt_old.pt\"\n",
    "    torch.save(checkpoint, old_ckpt_path)\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "563cdcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin_by_weight = 3\n",
      "Overriding: margin_by_weight_alpha = 0.0625\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.4165, time 2249.63ms, mfu -100.00%\n",
      "iter 100: loss 1.9108, time 63.57ms, mfu 0.02%\n",
      "iter 200: loss 1.9771, time 91.08ms, mfu 0.02%\n",
      "step 250: train loss 4.1153, val loss 4.1206\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 300: loss 1.7935, time 135.20ms, mfu 0.02%\n",
      "iter 400: loss 1.7621, time 48.13ms, mfu 0.02%\n",
      "step 500: train loss 4.0571, val loss 4.0579\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 500: loss 1.7594, time 819.77ms, mfu 0.02%\n",
      "iter 600: loss 1.6452, time 73.81ms, mfu 0.02%\n",
      "iter 700: loss 1.5949, time 56.22ms, mfu 0.02%\n",
      "step 750: train loss 3.9820, val loss 3.9847\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 800: loss 1.6689, time 214.54ms, mfu 0.02%\n",
      "iter 900: loss 1.5856, time 52.84ms, mfu 0.02%\n",
      "step 1000: train loss 3.8952, val loss 3.9125\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 1000: loss 1.5981, time 682.00ms, mfu 0.02%\n",
      "iter 1100: loss 1.4200, time 88.27ms, mfu 0.02%\n",
      "iter 1200: loss 1.4822, time 63.56ms, mfu 0.02%\n",
      "step 1250: train loss 3.8019, val loss 3.8140\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 1300: loss 1.4133, time 248.66ms, mfu 0.02%\n",
      "iter 1400: loss 1.4060, time 113.75ms, mfu 0.01%\n",
      "step 1500: train loss 3.7615, val loss 3.7907\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 1500: loss 1.3860, time 768.15ms, mfu 0.01%\n",
      "iter 1600: loss 1.5062, time 62.71ms, mfu 0.01%\n",
      "iter 1700: loss 1.3131, time 70.51ms, mfu 0.01%\n",
      "step 1750: train loss 3.7052, val loss 3.7066\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 1800: loss 1.4121, time 87.51ms, mfu 0.01%\n",
      "iter 1900: loss 1.3436, time 70.99ms, mfu 0.01%\n",
      "step 2000: train loss 3.6737, val loss 3.7021\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 2000: loss 1.4094, time 740.42ms, mfu 0.01%\n",
      "iter 2100: loss 1.3487, time 81.54ms, mfu 0.01%\n",
      "iter 2200: loss 1.3411, time 103.15ms, mfu 0.01%\n",
      "step 2250: train loss 3.6112, val loss 3.6431\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 2300: loss 1.3213, time 63.61ms, mfu 0.01%\n",
      "iter 2400: loss 1.2690, time 68.26ms, mfu 0.01%\n",
      "step 2500: train loss 3.6064, val loss 3.6207\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 2500: loss 1.3111, time 684.75ms, mfu 0.01%\n",
      "iter 2600: loss 1.2712, time 54.17ms, mfu 0.01%\n",
      "iter 2700: loss 1.2334, time 51.86ms, mfu 0.02%\n",
      "step 2750: train loss 3.5750, val loss 3.6076\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 2800: loss 1.4106, time 99.99ms, mfu 0.02%\n",
      "iter 2900: loss 1.2596, time 60.54ms, mfu 0.02%\n",
      "step 3000: train loss 3.5899, val loss 3.6082\n",
      "iter 3000: loss 1.2598, time 1227.85ms, mfu 0.01%\n",
      "iter 3100: loss 1.2025, time 49.19ms, mfu 0.02%\n",
      "iter 3200: loss 1.2388, time 49.56ms, mfu 0.02%\n",
      "step 3250: train loss 3.5852, val loss 3.6166\n",
      "iter 3300: loss 1.2977, time 49.70ms, mfu 0.02%\n",
      "iter 3400: loss 1.2429, time 65.41ms, mfu 0.02%\n",
      "step 3500: train loss 3.6052, val loss 3.6518\n",
      "iter 3500: loss 1.1969, time 729.80ms, mfu 0.02%\n",
      "iter 3600: loss 1.1701, time 53.12ms, mfu 0.02%\n",
      "iter 3700: loss 1.1515, time 50.05ms, mfu 0.02%\n",
      "step 3750: train loss 3.5549, val loss 3.5896\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 3800: loss 1.2134, time 105.68ms, mfu 0.02%\n",
      "iter 3900: loss 1.1318, time 50.87ms, mfu 0.02%\n",
      "step 4000: train loss 3.5063, val loss 3.5637\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 4000: loss 1.2760, time 1038.62ms, mfu 0.02%\n",
      "iter 4100: loss 1.1279, time 93.00ms, mfu 0.02%\n",
      "iter 4200: loss 1.0901, time 75.11ms, mfu 0.02%\n",
      "step 4250: train loss 3.5633, val loss 3.6059\n",
      "iter 4300: loss 1.1191, time 98.83ms, mfu 0.02%\n",
      "iter 4400: loss 1.2546, time 179.48ms, mfu 0.02%\n",
      "step 4500: train loss 3.5603, val loss 3.6009\n",
      "iter 4500: loss 1.1806, time 1137.75ms, mfu 0.01%\n",
      "iter 4600: loss 1.2640, time 50.22ms, mfu 0.01%\n",
      "iter 4700: loss 1.1296, time 233.21ms, mfu 0.01%\n",
      "step 4750: train loss 3.5724, val loss 3.6173\n",
      "iter 4800: loss 1.2075, time 62.54ms, mfu 0.01%\n",
      "iter 4900: loss 1.0823, time 93.86ms, mfu 0.01%\n",
      "step 5000: train loss 3.5176, val loss 3.5754\n",
      "iter 5000: loss 1.2442, time 851.80ms, mfu 0.01%\n",
      "iter 5100: loss 1.1598, time 50.19ms, mfu 0.01%\n",
      "iter 5200: loss 1.1223, time 51.43ms, mfu 0.02%\n",
      "step 5250: train loss 3.5324, val loss 3.5846\n",
      "iter 5300: loss 1.0325, time 100.08ms, mfu 0.02%\n",
      "iter 5400: loss 1.2119, time 86.06ms, mfu 0.02%\n",
      "step 5500: train loss 3.5287, val loss 3.5551\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 5500: loss 1.1699, time 863.00ms, mfu 0.01%\n",
      "iter 5600: loss 1.1644, time 48.39ms, mfu 0.02%\n",
      "iter 5700: loss 1.0757, time 54.53ms, mfu 0.02%\n",
      "step 5750: train loss 3.5015, val loss 3.5443\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 5800: loss 1.1399, time 50.69ms, mfu 0.02%\n",
      "iter 5900: loss 1.1193, time 81.39ms, mfu 0.02%\n",
      "step 6000: train loss 3.4965, val loss 3.5545\n",
      "iter 6000: loss 1.1155, time 1567.13ms, mfu 0.02%\n",
      "iter 6100: loss 1.1555, time 55.51ms, mfu 0.02%\n",
      "iter 6200: loss 1.1084, time 107.55ms, mfu 0.02%\n",
      "step 6250: train loss 3.5031, val loss 3.5327\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 6300: loss 1.1162, time 92.52ms, mfu 0.02%\n",
      "iter 6400: loss 1.0301, time 118.99ms, mfu 0.01%\n",
      "step 6500: train loss 3.5090, val loss 3.5472\n",
      "iter 6500: loss 1.1041, time 869.42ms, mfu 0.01%\n",
      "iter 6600: loss 1.1458, time 80.59ms, mfu 0.01%\n",
      "iter 6700: loss 1.0670, time 103.74ms, mfu 0.01%\n",
      "step 6750: train loss 3.4932, val loss 3.5360\n",
      "iter 6800: loss 1.1145, time 58.61ms, mfu 0.01%\n",
      "iter 6900: loss 1.1285, time 59.09ms, mfu 0.02%\n",
      "step 7000: train loss 3.4808, val loss 3.5329\n",
      "iter 7000: loss 1.1324, time 1047.46ms, mfu 0.01%\n",
      "iter 7100: loss 1.1024, time 607.89ms, mfu 0.01%\n",
      "iter 7200: loss 1.0472, time 167.86ms, mfu 0.01%\n",
      "step 7250: train loss 3.4863, val loss 3.5354\n",
      "iter 7300: loss 1.1406, time 95.37ms, mfu 0.01%\n",
      "iter 7400: loss 1.1155, time 73.65ms, mfu 0.01%\n",
      "step 7500: train loss 3.4628, val loss 3.5173\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 7500: loss 1.1272, time 2137.34ms, mfu 0.01%\n",
      "iter 7600: loss 1.0897, time 59.43ms, mfu 0.01%\n",
      "iter 7700: loss 1.0692, time 111.03ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7750: train loss 3.4635, val loss 3.5110\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 7800: loss 1.1612, time 92.44ms, mfu 0.01%\n",
      "iter 7900: loss 1.0898, time 181.08ms, mfu 0.01%\n",
      "step 8000: train loss 3.4513, val loss 3.5071\n",
      "saving checkpoint to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2\n",
      "iter 8000: loss 1.1267, time 897.41ms, mfu 0.01%\n",
      "done for proposed_margin_by_weight_type_3_alpha=0.0625\n"
     ]
    }
   ],
   "source": [
    "for margin_by_weight_alpha in [0.0625]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_by_weight_type_3_alpha_{margin_by_weight_alpha}_SE_2\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin_by_weight\": \"3\",\n",
    "        \"margin_by_weight_alpha\": margin_by_weight_alpha,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for proposed_margin_by_weight_type_3_alpha={margin_by_weight_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e857175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/1850883804.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_by_weight_type_3_alpha_0.0625_SE_2/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# workaround to save embeddings in old format\n",
    "\n",
    "import torch\n",
    "margin = 1.0\n",
    "for model_name in [\n",
    "    'proposed_margin_by_weight_type_3_alpha_0.0625_SE_2',\n",
    "]:\n",
    "    out_path = f\"out/{model_name}\"\n",
    "    ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    old_ckpt_path = f\"{out_path}/ckpt_old.pt\"\n",
    "    torch.save(checkpoint, old_ckpt_path)\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d7e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ckpt(model_name):\n",
    "    out_path = f\"out/{model_name}\"\n",
    "    ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    # Path to save the fixed checkpoint\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    old_ckpt_path = f\"{out_path}/ckpt_old.pt\"\n",
    "    torch.save(checkpoint, old_ckpt_path)\n",
    "\n",
    "    # Assuming the structure of the list of weights\n",
    "    # is in transformer.wte.weights.X and lm_head.weights.X\n",
    "    # Stack the list of weights for transformer.wte\n",
    "    wte_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        wte_weight_list.append(checkpoint['model']['transformer.wte.weights.{}'.format(i)])\n",
    "    wte_stacked = torch.stack(wte_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Stack the list of weights for lm_head\n",
    "    lm_head_weight_list = []\n",
    "    for i in range(130):  # Assuming 130 vectors\n",
    "        lm_head_weight_list.append(checkpoint['model']['lm_head.weights.{}'.format(i)])\n",
    "    lm_head_stacked = torch.stack(lm_head_weight_list)  # Shape: (130, 128)\n",
    "\n",
    "    # Now replace the old list of weights with the stacked versions\n",
    "    checkpoint['model']['transformer.wte.weight'] = wte_stacked\n",
    "    checkpoint['model']['lm_head.weight'] = lm_head_stacked\n",
    "\n",
    "    # Optionally, delete the old entries to avoid confusion\n",
    "    for i in range(130):\n",
    "        del checkpoint['model']['transformer.wte.weights.{}'.format(i)]\n",
    "        del checkpoint['model']['lm_head.weights.{}'.format(i)]\n",
    "\n",
    "    # Save the updated checkpoint\n",
    "    fixed_ckpt_path = f\"{out_path}/ckpt.pt\"\n",
    "    torch.save(checkpoint, fixed_ckpt_path)\n",
    "\n",
    "    print(f\"Fixed checkpoint saved to {fixed_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df1dd028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.5_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.5\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8650, time 1031.73ms, mfu -100.00%\n",
      "iter 100: loss 2.4087, time 57.59ms, mfu 0.02%\n",
      "iter 200: loss 2.3165, time 464.33ms, mfu 0.02%\n",
      "step 250: train loss 2.7587, val loss 2.7848\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 300: loss 2.1381, time 111.27ms, mfu 0.02%\n",
      "iter 400: loss 2.0902, time 55.89ms, mfu 0.02%\n",
      "step 500: train loss 2.5585, val loss 2.5814\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 500: loss 2.1504, time 927.42ms, mfu 0.02%\n",
      "iter 600: loss 2.0146, time 75.29ms, mfu 0.02%\n",
      "iter 700: loss 1.9337, time 94.55ms, mfu 0.02%\n",
      "step 750: train loss 2.4216, val loss 2.4416\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 800: loss 1.9951, time 99.15ms, mfu 0.02%\n",
      "iter 900: loss 1.8569, time 89.39ms, mfu 0.02%\n",
      "step 1000: train loss 2.3124, val loss 2.3749\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 1000: loss 1.8715, time 839.64ms, mfu 0.02%\n",
      "iter 1100: loss 1.6879, time 76.57ms, mfu 0.02%\n",
      "iter 1200: loss 1.7225, time 78.83ms, mfu 0.02%\n",
      "step 1250: train loss 2.2408, val loss 2.2976\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 1300: loss 1.7045, time 65.97ms, mfu 0.02%\n",
      "iter 1400: loss 1.7156, time 67.96ms, mfu 0.02%\n",
      "step 1500: train loss 2.1712, val loss 2.2529\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 1500: loss 1.6219, time 1161.63ms, mfu 0.01%\n",
      "iter 1600: loss 1.7454, time 85.78ms, mfu 0.01%\n",
      "iter 1700: loss 1.5766, time 105.14ms, mfu 0.01%\n",
      "step 1750: train loss 2.1315, val loss 2.1868\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 1800: loss 1.6736, time 65.06ms, mfu 0.01%\n",
      "iter 1900: loss 1.5900, time 89.79ms, mfu 0.01%\n",
      "step 2000: train loss 2.0915, val loss 2.1898\n",
      "iter 2000: loss 1.6663, time 730.33ms, mfu 0.01%\n",
      "iter 2100: loss 1.5933, time 247.33ms, mfu 0.01%\n",
      "iter 2200: loss 1.5727, time 59.58ms, mfu 0.01%\n",
      "step 2250: train loss 2.0288, val loss 2.1357\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 2300: loss 1.5858, time 108.63ms, mfu 0.01%\n",
      "iter 2400: loss 1.4993, time 62.57ms, mfu 0.01%\n",
      "step 2500: train loss 2.0492, val loss 2.1197\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 2500: loss 1.5716, time 905.30ms, mfu 0.01%\n",
      "iter 2600: loss 1.4770, time 58.94ms, mfu 0.01%\n",
      "iter 2700: loss 1.4693, time 100.09ms, mfu 0.01%\n",
      "step 2750: train loss 2.0227, val loss 2.0942\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 2800: loss 1.6844, time 106.08ms, mfu 0.01%\n",
      "iter 2900: loss 1.4802, time 51.94ms, mfu 0.01%\n",
      "step 3000: train loss 2.0244, val loss 2.1041\n",
      "iter 3000: loss 1.4716, time 952.59ms, mfu 0.01%\n",
      "iter 3100: loss 1.4007, time 62.88ms, mfu 0.01%\n",
      "iter 3200: loss 1.4514, time 92.37ms, mfu 0.01%\n",
      "step 3250: train loss 1.9654, val loss 2.0592\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 3300: loss 1.5270, time 75.49ms, mfu 0.01%\n",
      "iter 3400: loss 1.4624, time 130.76ms, mfu 0.01%\n",
      "step 3500: train loss 1.9471, val loss 2.0616\n",
      "iter 3500: loss 1.3768, time 862.08ms, mfu 0.01%\n",
      "iter 3600: loss 1.3805, time 103.69ms, mfu 0.01%\n",
      "iter 3700: loss 1.4013, time 89.38ms, mfu 0.01%\n",
      "step 3750: train loss 1.9418, val loss 2.0501\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 3800: loss 1.4245, time 66.07ms, mfu 0.01%\n",
      "iter 3900: loss 1.3275, time 105.69ms, mfu 0.01%\n",
      "step 4000: train loss 1.9105, val loss 2.0508\n",
      "iter 4000: loss 1.4978, time 1023.40ms, mfu 0.01%\n",
      "iter 4100: loss 1.3280, time 119.00ms, mfu 0.01%\n",
      "iter 4200: loss 1.2973, time 118.86ms, mfu 0.01%\n",
      "step 4250: train loss 1.9040, val loss 2.0314\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 4300: loss 1.3024, time 89.60ms, mfu 0.01%\n",
      "iter 4400: loss 1.4821, time 72.39ms, mfu 0.01%\n",
      "step 4500: train loss 1.9027, val loss 2.0314\n",
      "iter 4500: loss 1.4151, time 1046.24ms, mfu 0.01%\n",
      "iter 4600: loss 1.4605, time 72.47ms, mfu 0.01%\n",
      "iter 4700: loss 1.3378, time 204.46ms, mfu 0.01%\n",
      "step 4750: train loss 1.8921, val loss 2.0331\n",
      "iter 4800: loss 1.3919, time 119.24ms, mfu 0.01%\n",
      "iter 4900: loss 1.2785, time 67.39ms, mfu 0.01%\n",
      "step 5000: train loss 1.8442, val loss 1.9945\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 5000: loss 1.4716, time 1329.87ms, mfu 0.01%\n",
      "iter 5100: loss 1.3734, time 81.32ms, mfu 0.01%\n",
      "iter 5200: loss 1.3255, time 123.62ms, mfu 0.01%\n",
      "step 5250: train loss 1.8557, val loss 1.9720\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 5300: loss 1.2009, time 76.59ms, mfu 0.01%\n",
      "iter 5400: loss 1.3694, time 94.31ms, mfu 0.01%\n",
      "step 5500: train loss 1.8802, val loss 1.9606\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 5500: loss 1.4096, time 1369.04ms, mfu 0.01%\n",
      "iter 5600: loss 1.3684, time 103.77ms, mfu 0.01%\n",
      "iter 5700: loss 1.2785, time 134.07ms, mfu 0.01%\n",
      "step 5750: train loss 1.8295, val loss 1.9428\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 5800: loss 1.3021, time 99.19ms, mfu 0.01%\n",
      "iter 5900: loss 1.3080, time 258.54ms, mfu 0.01%\n",
      "step 6000: train loss 1.8227, val loss 1.9641\n",
      "iter 6000: loss 1.3668, time 1103.24ms, mfu 0.01%\n",
      "iter 6100: loss 1.3887, time 129.46ms, mfu 0.01%\n",
      "iter 6200: loss 1.2941, time 120.90ms, mfu 0.01%\n",
      "step 6250: train loss 1.8433, val loss 1.9487\n",
      "iter 6300: loss 1.3574, time 94.21ms, mfu 0.01%\n",
      "iter 6400: loss 1.2124, time 122.06ms, mfu 0.01%\n",
      "step 6500: train loss 1.8266, val loss 1.9463\n",
      "iter 6500: loss 1.3097, time 1205.08ms, mfu 0.01%\n",
      "iter 6600: loss 1.3785, time 132.41ms, mfu 0.01%\n",
      "iter 6700: loss 1.2573, time 110.69ms, mfu 0.01%\n",
      "step 6750: train loss 1.8210, val loss 1.9394\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 6800: loss 1.3006, time 99.77ms, mfu 0.01%\n",
      "iter 6900: loss 1.2856, time 226.30ms, mfu 0.01%\n",
      "step 7000: train loss 1.7997, val loss 1.9482\n",
      "iter 7000: loss 1.3715, time 1513.68ms, mfu 0.01%\n",
      "iter 7100: loss 1.3334, time 82.82ms, mfu 0.01%\n",
      "iter 7200: loss 1.2556, time 125.35ms, mfu 0.01%\n",
      "step 7250: train loss 1.8005, val loss 1.9488\n",
      "iter 7300: loss 1.3253, time 87.19ms, mfu 0.01%\n",
      "iter 7400: loss 1.3371, time 80.36ms, mfu 0.01%\n",
      "step 7500: train loss 1.7825, val loss 1.9348\n",
      "saving checkpoint to out/proposed_margin_1.5_separated_embeddings\n",
      "iter 7500: loss 1.3149, time 1508.09ms, mfu 0.01%\n",
      "iter 7600: loss 1.2691, time 121.53ms, mfu 0.01%\n",
      "iter 7700: loss 1.2385, time 133.03ms, mfu 0.01%\n",
      "step 7750: train loss 1.7996, val loss 1.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7800: loss 1.3370, time 95.11ms, mfu 0.01%\n",
      "iter 7900: loss 1.2700, time 124.03ms, mfu 0.01%\n",
      "step 8000: train loss 1.7846, val loss 1.9441\n",
      "iter 8000: loss 1.3367, time 1126.23ms, mfu 0.01%\n",
      "done for margin=1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.5_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.4_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.4\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8620, time 1682.24ms, mfu -100.00%\n",
      "iter 100: loss 2.3936, time 108.86ms, mfu 0.01%\n",
      "iter 200: loss 2.3010, time 90.84ms, mfu 0.01%\n",
      "step 250: train loss 2.7938, val loss 2.8198\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 300: loss 2.1198, time 87.46ms, mfu 0.01%\n",
      "iter 400: loss 2.0659, time 79.24ms, mfu 0.01%\n",
      "step 500: train loss 2.6055, val loss 2.6250\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 500: loss 2.1459, time 1363.00ms, mfu 0.01%\n",
      "iter 600: loss 1.9848, time 126.43ms, mfu 0.01%\n",
      "iter 700: loss 1.9264, time 134.97ms, mfu 0.01%\n",
      "step 750: train loss 2.4539, val loss 2.4745\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 800: loss 2.0026, time 150.48ms, mfu 0.01%\n",
      "iter 900: loss 1.8306, time 83.47ms, mfu 0.01%\n",
      "step 1000: train loss 2.3462, val loss 2.4126\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 1000: loss 1.8451, time 1100.06ms, mfu 0.01%\n",
      "iter 1100: loss 1.6766, time 91.19ms, mfu 0.01%\n",
      "iter 1200: loss 1.6937, time 82.64ms, mfu 0.01%\n",
      "step 1250: train loss 2.2852, val loss 2.3370\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 1300: loss 1.6764, time 114.84ms, mfu 0.01%\n",
      "iter 1400: loss 1.6953, time 81.21ms, mfu 0.01%\n",
      "step 1500: train loss 2.1907, val loss 2.2743\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 1500: loss 1.6002, time 1568.11ms, mfu 0.01%\n",
      "iter 1600: loss 1.7082, time 91.02ms, mfu 0.01%\n",
      "iter 1700: loss 1.5500, time 204.00ms, mfu 0.01%\n",
      "step 1750: train loss 2.1591, val loss 2.2137\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 1800: loss 1.6499, time 119.36ms, mfu 0.01%\n",
      "iter 1900: loss 1.5642, time 73.67ms, mfu 0.01%\n",
      "step 2000: train loss 2.1239, val loss 2.2130\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 2000: loss 1.6371, time 1397.61ms, mfu 0.01%\n",
      "iter 2100: loss 1.5735, time 130.15ms, mfu 0.01%\n",
      "iter 2200: loss 1.5473, time 110.59ms, mfu 0.01%\n",
      "step 2250: train loss 2.0664, val loss 2.1672\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 2300: loss 1.5676, time 112.89ms, mfu 0.01%\n",
      "iter 2400: loss 1.4794, time 143.77ms, mfu 0.01%\n",
      "step 2500: train loss 2.0882, val loss 2.1553\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 2500: loss 1.5645, time 1368.89ms, mfu 0.01%\n",
      "iter 2600: loss 1.4349, time 121.77ms, mfu 0.01%\n",
      "iter 2700: loss 1.4506, time 105.22ms, mfu 0.01%\n",
      "step 2750: train loss 2.0603, val loss 2.1407\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 2800: loss 1.6485, time 157.95ms, mfu 0.01%\n",
      "iter 2900: loss 1.4839, time 112.93ms, mfu 0.01%\n",
      "step 3000: train loss 2.0693, val loss 2.1502\n",
      "iter 3000: loss 1.4511, time 1043.41ms, mfu 0.01%\n",
      "iter 3100: loss 1.3622, time 94.04ms, mfu 0.01%\n",
      "iter 3200: loss 1.4376, time 104.42ms, mfu 0.01%\n",
      "step 3250: train loss 2.0126, val loss 2.1054\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 3300: loss 1.5060, time 105.65ms, mfu 0.01%\n",
      "iter 3400: loss 1.4535, time 115.64ms, mfu 0.01%\n",
      "step 3500: train loss 1.9862, val loss 2.1017\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 3500: loss 1.3540, time 1288.43ms, mfu 0.01%\n",
      "iter 3600: loss 1.3487, time 133.90ms, mfu 0.01%\n",
      "iter 3700: loss 1.3549, time 110.69ms, mfu 0.01%\n",
      "step 3750: train loss 1.9808, val loss 2.0840\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 3800: loss 1.4072, time 107.23ms, mfu 0.01%\n",
      "iter 3900: loss 1.2940, time 165.19ms, mfu 0.01%\n",
      "step 4000: train loss 1.9516, val loss 2.0903\n",
      "iter 4000: loss 1.5040, time 1181.90ms, mfu 0.01%\n",
      "iter 4100: loss 1.3019, time 131.58ms, mfu 0.01%\n",
      "iter 4200: loss 1.2603, time 108.89ms, mfu 0.01%\n",
      "step 4250: train loss 1.9516, val loss 2.0740\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 4300: loss 1.3083, time 120.02ms, mfu 0.01%\n",
      "iter 4400: loss 1.4812, time 80.03ms, mfu 0.01%\n",
      "step 4500: train loss 1.9417, val loss 2.0684\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 4500: loss 1.3979, time 1234.24ms, mfu 0.01%\n",
      "iter 4600: loss 1.4466, time 112.94ms, mfu 0.01%\n",
      "iter 4700: loss 1.3362, time 247.20ms, mfu 0.01%\n",
      "step 4750: train loss 1.9347, val loss 2.0690\n",
      "iter 4800: loss 1.3743, time 105.87ms, mfu 0.01%\n",
      "iter 4900: loss 1.2638, time 128.56ms, mfu 0.01%\n",
      "step 5000: train loss 1.8860, val loss 2.0214\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 5000: loss 1.4664, time 1325.31ms, mfu 0.01%\n",
      "iter 5100: loss 1.3534, time 96.30ms, mfu 0.01%\n",
      "iter 5200: loss 1.3174, time 81.77ms, mfu 0.01%\n",
      "step 5250: train loss 1.9003, val loss 2.0110\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 5300: loss 1.1739, time 101.31ms, mfu 0.01%\n",
      "iter 5400: loss 1.3437, time 87.80ms, mfu 0.01%\n",
      "step 5500: train loss 1.9260, val loss 2.0033\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 5500: loss 1.4135, time 1019.61ms, mfu 0.01%\n",
      "iter 5600: loss 1.3929, time 134.07ms, mfu 0.01%\n",
      "iter 5700: loss 1.2710, time 110.19ms, mfu 0.01%\n",
      "step 5750: train loss 1.8743, val loss 1.9927\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 5800: loss 1.2871, time 108.58ms, mfu 0.01%\n",
      "iter 5900: loss 1.3032, time 195.36ms, mfu 0.01%\n",
      "step 6000: train loss 1.8696, val loss 2.0154\n",
      "iter 6000: loss 1.3456, time 1219.95ms, mfu 0.01%\n",
      "iter 6100: loss 1.4039, time 120.52ms, mfu 0.01%\n",
      "iter 6200: loss 1.2707, time 159.30ms, mfu 0.01%\n",
      "step 6250: train loss 1.8854, val loss 1.9844\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 6300: loss 1.3602, time 119.61ms, mfu 0.01%\n",
      "iter 6400: loss 1.2112, time 103.01ms, mfu 0.01%\n",
      "step 6500: train loss 1.8781, val loss 1.9988\n",
      "iter 6500: loss 1.2964, time 1087.01ms, mfu 0.01%\n",
      "iter 6600: loss 1.3394, time 76.19ms, mfu 0.01%\n",
      "iter 6700: loss 1.2762, time 133.85ms, mfu 0.01%\n",
      "step 6750: train loss 1.8604, val loss 1.9842\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 6800: loss 1.2950, time 160.86ms, mfu 0.01%\n",
      "iter 6900: loss 1.2726, time 74.00ms, mfu 0.01%\n",
      "step 7000: train loss 1.8428, val loss 1.9934\n",
      "iter 7000: loss 1.3754, time 1200.34ms, mfu 0.01%\n",
      "iter 7100: loss 1.3200, time 124.83ms, mfu 0.01%\n",
      "iter 7200: loss 1.2179, time 79.54ms, mfu 0.01%\n",
      "step 7250: train loss 1.8431, val loss 1.9929\n",
      "iter 7300: loss 1.3240, time 110.36ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7400: loss 1.3326, time 126.36ms, mfu 0.01%\n",
      "step 7500: train loss 1.8321, val loss 1.9778\n",
      "saving checkpoint to out/proposed_margin_1.4_separated_embeddings\n",
      "iter 7500: loss 1.3059, time 1711.94ms, mfu 0.01%\n",
      "iter 7600: loss 1.2578, time 105.27ms, mfu 0.01%\n",
      "iter 7700: loss 1.2182, time 116.43ms, mfu 0.01%\n",
      "step 7750: train loss 1.8500, val loss 1.9815\n",
      "iter 7800: loss 1.3406, time 79.21ms, mfu 0.01%\n",
      "iter 7900: loss 1.2593, time 74.34ms, mfu 0.01%\n",
      "step 8000: train loss 1.8291, val loss 1.9898\n",
      "iter 8000: loss 1.3165, time 1366.14ms, mfu 0.01%\n",
      "done for margin=1.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.4_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.3_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.3\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8577, time 1436.45ms, mfu -100.00%\n",
      "iter 100: loss 2.3704, time 118.19ms, mfu 0.01%\n",
      "iter 200: loss 2.2782, time 110.74ms, mfu 0.01%\n",
      "step 250: train loss 2.8343, val loss 2.8633\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 300: loss 2.1029, time 115.70ms, mfu 0.01%\n",
      "iter 400: loss 2.0489, time 75.92ms, mfu 0.01%\n",
      "step 500: train loss 2.6387, val loss 2.6572\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 500: loss 2.1336, time 1087.35ms, mfu 0.01%\n",
      "iter 600: loss 1.9679, time 133.93ms, mfu 0.01%\n",
      "iter 700: loss 1.8864, time 133.98ms, mfu 0.01%\n",
      "step 750: train loss 2.5024, val loss 2.5208\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 800: loss 1.9653, time 70.90ms, mfu 0.01%\n",
      "iter 900: loss 1.8190, time 81.59ms, mfu 0.01%\n",
      "step 1000: train loss 2.3899, val loss 2.4559\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 1000: loss 1.8173, time 1506.97ms, mfu 0.01%\n",
      "iter 1100: loss 1.6595, time 71.23ms, mfu 0.01%\n",
      "iter 1200: loss 1.6747, time 233.39ms, mfu 0.01%\n",
      "step 1250: train loss 2.3232, val loss 2.3708\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 1300: loss 1.6671, time 108.80ms, mfu 0.01%\n",
      "iter 1400: loss 1.6736, time 78.30ms, mfu 0.01%\n",
      "step 1500: train loss 2.2453, val loss 2.3179\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 1500: loss 1.5888, time 1293.61ms, mfu 0.01%\n",
      "iter 1600: loss 1.7094, time 98.16ms, mfu 0.01%\n",
      "iter 1700: loss 1.5257, time 92.28ms, mfu 0.01%\n",
      "step 1750: train loss 2.2079, val loss 2.2547\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 1800: loss 1.6526, time 90.04ms, mfu 0.01%\n",
      "iter 1900: loss 1.5404, time 65.18ms, mfu 0.01%\n",
      "step 2000: train loss 2.1805, val loss 2.2663\n",
      "iter 2000: loss 1.6386, time 1213.26ms, mfu 0.01%\n",
      "iter 2100: loss 1.5633, time 70.78ms, mfu 0.01%\n",
      "iter 2200: loss 1.5450, time 90.12ms, mfu 0.01%\n",
      "step 2250: train loss 2.1195, val loss 2.2165\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 2300: loss 1.5567, time 94.77ms, mfu 0.01%\n",
      "iter 2400: loss 1.4563, time 118.68ms, mfu 0.01%\n",
      "step 2500: train loss 2.1274, val loss 2.1902\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 2500: loss 1.5297, time 1257.41ms, mfu 0.01%\n",
      "iter 2600: loss 1.4429, time 133.59ms, mfu 0.01%\n",
      "iter 2700: loss 1.4511, time 95.83ms, mfu 0.01%\n",
      "step 2750: train loss 2.1084, val loss 2.1784\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 2800: loss 1.6376, time 75.96ms, mfu 0.01%\n",
      "iter 2900: loss 1.4442, time 131.60ms, mfu 0.01%\n",
      "step 3000: train loss 2.1218, val loss 2.1894\n",
      "iter 3000: loss 1.4705, time 1375.08ms, mfu 0.01%\n",
      "iter 3100: loss 1.3777, time 107.90ms, mfu 0.01%\n",
      "iter 3200: loss 1.4278, time 96.50ms, mfu 0.01%\n",
      "step 3250: train loss 2.0723, val loss 2.1555\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 3300: loss 1.4854, time 101.99ms, mfu 0.01%\n",
      "iter 3400: loss 1.4370, time 86.72ms, mfu 0.01%\n",
      "step 3500: train loss 2.0551, val loss 2.1609\n",
      "iter 3500: loss 1.3365, time 1147.96ms, mfu 0.01%\n",
      "iter 3600: loss 1.3314, time 113.94ms, mfu 0.01%\n",
      "iter 3700: loss 1.3350, time 85.73ms, mfu 0.01%\n",
      "step 3750: train loss 2.0306, val loss 2.1192\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 3800: loss 1.4056, time 132.22ms, mfu 0.01%\n",
      "iter 3900: loss 1.2841, time 133.78ms, mfu 0.01%\n",
      "step 4000: train loss 2.0032, val loss 2.1400\n",
      "iter 4000: loss 1.4699, time 1263.29ms, mfu 0.01%\n",
      "iter 4100: loss 1.3016, time 79.46ms, mfu 0.01%\n",
      "iter 4200: loss 1.2630, time 74.47ms, mfu 0.01%\n",
      "step 4250: train loss 2.0043, val loss 2.1176\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 4300: loss 1.2841, time 65.53ms, mfu 0.01%\n",
      "iter 4400: loss 1.4679, time 94.44ms, mfu 0.01%\n",
      "step 4500: train loss 1.9923, val loss 2.1155\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 4500: loss 1.3996, time 1314.70ms, mfu 0.01%\n",
      "iter 4600: loss 1.4256, time 105.64ms, mfu 0.01%\n",
      "iter 4700: loss 1.3210, time 116.48ms, mfu 0.01%\n",
      "step 4750: train loss 1.9893, val loss 2.1180\n",
      "iter 4800: loss 1.3757, time 73.08ms, mfu 0.01%\n",
      "iter 4900: loss 1.2297, time 80.35ms, mfu 0.01%\n",
      "step 5000: train loss 1.9513, val loss 2.0852\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 5000: loss 1.4622, time 1348.89ms, mfu 0.01%\n",
      "iter 5100: loss 1.3376, time 105.56ms, mfu 0.01%\n",
      "iter 5200: loss 1.3166, time 122.08ms, mfu 0.01%\n",
      "step 5250: train loss 1.9634, val loss 2.0607\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 5300: loss 1.1769, time 118.85ms, mfu 0.01%\n",
      "iter 5400: loss 1.3530, time 109.42ms, mfu 0.01%\n",
      "step 5500: train loss 1.9801, val loss 2.0526\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 5500: loss 1.3820, time 1491.90ms, mfu 0.01%\n",
      "iter 5600: loss 1.3528, time 83.33ms, mfu 0.01%\n",
      "iter 5700: loss 1.2696, time 117.69ms, mfu 0.01%\n",
      "step 5750: train loss 1.9340, val loss 2.0450\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 5800: loss 1.2913, time 80.12ms, mfu 0.01%\n",
      "iter 5900: loss 1.2956, time 106.28ms, mfu 0.01%\n",
      "step 6000: train loss 1.9282, val loss 2.0632\n",
      "iter 6000: loss 1.3382, time 1075.03ms, mfu 0.01%\n",
      "iter 6100: loss 1.3884, time 244.01ms, mfu 0.01%\n",
      "iter 6200: loss 1.2792, time 117.74ms, mfu 0.01%\n",
      "step 6250: train loss 1.9494, val loss 2.0407\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 6300: loss 1.3680, time 104.12ms, mfu 0.01%\n",
      "iter 6400: loss 1.1889, time 113.60ms, mfu 0.01%\n",
      "step 6500: train loss 1.9358, val loss 2.0497\n",
      "iter 6500: loss 1.2781, time 1194.86ms, mfu 0.01%\n",
      "iter 6600: loss 1.3529, time 142.23ms, mfu 0.01%\n",
      "iter 6700: loss 1.2923, time 120.26ms, mfu 0.01%\n",
      "step 6750: train loss 1.9257, val loss 2.0442\n",
      "iter 6800: loss 1.2879, time 145.52ms, mfu 0.01%\n",
      "iter 6900: loss 1.2707, time 101.31ms, mfu 0.01%\n",
      "step 7000: train loss 1.9088, val loss 2.0486\n",
      "iter 7000: loss 1.3763, time 1202.01ms, mfu 0.01%\n",
      "iter 7100: loss 1.2994, time 68.40ms, mfu 0.01%\n",
      "iter 7200: loss 1.2158, time 109.88ms, mfu 0.01%\n",
      "step 7250: train loss 1.9117, val loss 2.0493\n",
      "iter 7300: loss 1.3165, time 105.71ms, mfu 0.01%\n",
      "iter 7400: loss 1.3274, time 163.58ms, mfu 0.01%\n",
      "step 7500: train loss 1.8990, val loss 2.0356\n",
      "saving checkpoint to out/proposed_margin_1.3_separated_embeddings\n",
      "iter 7500: loss 1.2960, time 1606.53ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7600: loss 1.2427, time 82.69ms, mfu 0.01%\n",
      "iter 7700: loss 1.2006, time 75.54ms, mfu 0.01%\n",
      "step 7750: train loss 1.9149, val loss 2.0423\n",
      "iter 7800: loss 1.3294, time 95.83ms, mfu 0.01%\n",
      "iter 7900: loss 1.2526, time 187.78ms, mfu 0.01%\n",
      "step 8000: train loss 1.8982, val loss 2.0464\n",
      "iter 8000: loss 1.3174, time 1205.29ms, mfu 0.01%\n",
      "done for margin=1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.3_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.2_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.2\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8513, time 1222.85ms, mfu -100.00%\n",
      "iter 100: loss 2.3433, time 101.89ms, mfu 0.01%\n",
      "iter 200: loss 2.2574, time 73.41ms, mfu 0.01%\n",
      "step 250: train loss 2.8889, val loss 2.9151\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 300: loss 2.1037, time 80.98ms, mfu 0.01%\n",
      "iter 400: loss 2.0364, time 94.76ms, mfu 0.01%\n",
      "step 500: train loss 2.6864, val loss 2.7050\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 500: loss 2.1080, time 1289.08ms, mfu 0.01%\n",
      "iter 600: loss 1.9581, time 84.38ms, mfu 0.01%\n",
      "iter 700: loss 1.8813, time 105.18ms, mfu 0.01%\n",
      "step 750: train loss 2.5512, val loss 2.5718\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 800: loss 1.9551, time 109.23ms, mfu 0.01%\n",
      "iter 900: loss 1.7970, time 68.79ms, mfu 0.01%\n",
      "step 1000: train loss 2.4454, val loss 2.5055\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 1000: loss 1.8096, time 1257.46ms, mfu 0.01%\n",
      "iter 1100: loss 1.6519, time 115.03ms, mfu 0.01%\n",
      "iter 1200: loss 1.6611, time 229.68ms, mfu 0.01%\n",
      "step 1250: train loss 2.3843, val loss 2.4321\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 1300: loss 1.6623, time 91.78ms, mfu 0.01%\n",
      "iter 1400: loss 1.6592, time 119.07ms, mfu 0.01%\n",
      "step 1500: train loss 2.2940, val loss 2.3754\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 1500: loss 1.5626, time 1292.85ms, mfu 0.01%\n",
      "iter 1600: loss 1.6795, time 82.67ms, mfu 0.01%\n",
      "iter 1700: loss 1.4968, time 116.75ms, mfu 0.01%\n",
      "step 1750: train loss 2.2690, val loss 2.3191\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 1800: loss 1.6220, time 74.04ms, mfu 0.01%\n",
      "iter 1900: loss 1.5130, time 80.63ms, mfu 0.01%\n",
      "step 2000: train loss 2.2265, val loss 2.3076\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 2000: loss 1.6248, time 1186.04ms, mfu 0.01%\n",
      "iter 2100: loss 1.5785, time 148.66ms, mfu 0.01%\n",
      "iter 2200: loss 1.5182, time 93.03ms, mfu 0.01%\n",
      "step 2250: train loss 2.1829, val loss 2.2790\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 2300: loss 1.5495, time 114.73ms, mfu 0.01%\n",
      "iter 2400: loss 1.4522, time 86.46ms, mfu 0.01%\n",
      "step 2500: train loss 2.1824, val loss 2.2420\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 2500: loss 1.5208, time 1172.63ms, mfu 0.01%\n",
      "iter 2600: loss 1.4083, time 131.92ms, mfu 0.01%\n",
      "iter 2700: loss 1.4301, time 69.28ms, mfu 0.01%\n",
      "step 2750: train loss 2.1648, val loss 2.2270\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 2800: loss 1.6144, time 260.72ms, mfu 0.01%\n",
      "iter 2900: loss 1.4469, time 90.94ms, mfu 0.01%\n",
      "step 3000: train loss 2.1777, val loss 2.2483\n",
      "iter 3000: loss 1.4426, time 1344.82ms, mfu 0.01%\n",
      "iter 3100: loss 1.3581, time 76.81ms, mfu 0.01%\n",
      "iter 3200: loss 1.4255, time 102.36ms, mfu 0.01%\n",
      "step 3250: train loss 2.1127, val loss 2.2004\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 3300: loss 1.4586, time 84.41ms, mfu 0.01%\n",
      "iter 3400: loss 1.4510, time 135.68ms, mfu 0.01%\n",
      "step 3500: train loss 2.1049, val loss 2.2043\n",
      "iter 3500: loss 1.3027, time 1183.43ms, mfu 0.01%\n",
      "iter 3600: loss 1.3476, time 161.54ms, mfu 0.01%\n",
      "iter 3700: loss 1.3328, time 124.93ms, mfu 0.01%\n",
      "step 3750: train loss 2.0835, val loss 2.1780\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 3800: loss 1.3903, time 181.17ms, mfu 0.01%\n",
      "iter 3900: loss 1.2843, time 125.09ms, mfu 0.01%\n",
      "step 4000: train loss 2.0624, val loss 2.1949\n",
      "iter 4000: loss 1.4596, time 1038.57ms, mfu 0.01%\n",
      "iter 4100: loss 1.2876, time 85.23ms, mfu 0.01%\n",
      "iter 4200: loss 1.2242, time 122.11ms, mfu 0.01%\n",
      "step 4250: train loss 2.0558, val loss 2.1676\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 4300: loss 1.2961, time 100.41ms, mfu 0.01%\n",
      "iter 4400: loss 1.4465, time 104.77ms, mfu 0.01%\n",
      "step 4500: train loss 2.0594, val loss 2.1809\n",
      "iter 4500: loss 1.3962, time 1168.88ms, mfu 0.01%\n",
      "iter 4600: loss 1.4112, time 116.41ms, mfu 0.01%\n",
      "iter 4700: loss 1.3094, time 130.67ms, mfu 0.01%\n",
      "step 4750: train loss 2.0425, val loss 2.1711\n",
      "iter 4800: loss 1.3419, time 134.18ms, mfu 0.01%\n",
      "iter 4900: loss 1.2275, time 101.47ms, mfu 0.01%\n",
      "step 5000: train loss 2.0028, val loss 2.1425\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 5000: loss 1.3973, time 1264.26ms, mfu 0.01%\n",
      "iter 5100: loss 1.3242, time 74.56ms, mfu 0.01%\n",
      "iter 5200: loss 1.2853, time 115.07ms, mfu 0.01%\n",
      "step 5250: train loss 2.0129, val loss 2.1119\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 5300: loss 1.1578, time 112.04ms, mfu 0.01%\n",
      "iter 5400: loss 1.3328, time 93.74ms, mfu 0.01%\n",
      "step 5500: train loss 2.0403, val loss 2.1064\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 5500: loss 1.3769, time 1808.38ms, mfu 0.01%\n",
      "iter 5600: loss 1.3438, time 126.06ms, mfu 0.01%\n",
      "iter 5700: loss 1.2423, time 117.50ms, mfu 0.01%\n",
      "step 5750: train loss 1.9916, val loss 2.0954\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 5800: loss 1.2734, time 78.82ms, mfu 0.01%\n",
      "iter 5900: loss 1.2771, time 99.80ms, mfu 0.01%\n",
      "step 6000: train loss 1.9885, val loss 2.1155\n",
      "iter 6000: loss 1.3115, time 1034.68ms, mfu 0.01%\n",
      "iter 6100: loss 1.3552, time 102.69ms, mfu 0.01%\n",
      "iter 6200: loss 1.2492, time 69.53ms, mfu 0.01%\n",
      "step 6250: train loss 2.0057, val loss 2.0962\n",
      "iter 6300: loss 1.3307, time 74.39ms, mfu 0.01%\n",
      "iter 6400: loss 1.1881, time 99.03ms, mfu 0.01%\n",
      "step 6500: train loss 1.9970, val loss 2.1041\n",
      "iter 6500: loss 1.2784, time 1157.96ms, mfu 0.01%\n",
      "iter 6600: loss 1.3558, time 105.49ms, mfu 0.01%\n",
      "iter 6700: loss 1.2426, time 118.76ms, mfu 0.01%\n",
      "step 6750: train loss 1.9819, val loss 2.0939\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 6800: loss 1.2656, time 127.22ms, mfu 0.01%\n",
      "iter 6900: loss 1.2640, time 140.59ms, mfu 0.01%\n",
      "step 7000: train loss 1.9647, val loss 2.1007\n",
      "iter 7000: loss 1.3411, time 951.47ms, mfu 0.01%\n",
      "iter 7100: loss 1.2981, time 114.73ms, mfu 0.01%\n",
      "iter 7200: loss 1.1707, time 82.99ms, mfu 0.01%\n",
      "step 7250: train loss 1.9629, val loss 2.0929\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n",
      "iter 7300: loss 1.2945, time 117.24ms, mfu 0.01%\n",
      "iter 7400: loss 1.3071, time 71.53ms, mfu 0.01%\n",
      "step 7500: train loss 1.9525, val loss 2.0858\n",
      "saving checkpoint to out/proposed_margin_1.2_separated_embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7500: loss 1.2812, time 1342.60ms, mfu 0.01%\n",
      "iter 7600: loss 1.2227, time 134.69ms, mfu 0.01%\n",
      "iter 7700: loss 1.1895, time 143.32ms, mfu 0.01%\n",
      "step 7750: train loss 1.9688, val loss 2.0903\n",
      "iter 7800: loss 1.3171, time 131.38ms, mfu 0.01%\n",
      "iter 7900: loss 1.2419, time 109.45ms, mfu 0.01%\n",
      "step 8000: train loss 1.9520, val loss 2.0939\n",
      "iter 8000: loss 1.2794, time 987.69ms, mfu 0.01%\n",
      "done for margin=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.2_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_1.1_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 1.1\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8430, time 1939.08ms, mfu -100.00%\n",
      "iter 100: loss 2.3153, time 122.06ms, mfu 0.01%\n",
      "iter 200: loss 2.2459, time 74.05ms, mfu 0.01%\n",
      "step 250: train loss 2.9330, val loss 2.9570\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 300: loss 2.0706, time 101.59ms, mfu 0.01%\n",
      "iter 400: loss 2.0075, time 176.65ms, mfu 0.01%\n",
      "step 500: train loss 2.7455, val loss 2.7623\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 500: loss 2.0914, time 1506.15ms, mfu 0.01%\n",
      "iter 600: loss 1.9328, time 133.15ms, mfu 0.01%\n",
      "iter 700: loss 1.8580, time 137.21ms, mfu 0.01%\n",
      "step 750: train loss 2.6129, val loss 2.6345\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 800: loss 1.9348, time 103.27ms, mfu 0.01%\n",
      "iter 900: loss 1.7721, time 83.04ms, mfu 0.01%\n",
      "step 1000: train loss 2.4975, val loss 2.5600\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 1000: loss 1.7913, time 1439.73ms, mfu 0.01%\n",
      "iter 1100: loss 1.6097, time 93.34ms, mfu 0.01%\n",
      "iter 1200: loss 1.6500, time 107.83ms, mfu 0.01%\n",
      "step 1250: train loss 2.4484, val loss 2.4912\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 1300: loss 1.6557, time 132.47ms, mfu 0.01%\n",
      "iter 1400: loss 1.6395, time 65.56ms, mfu 0.01%\n",
      "step 1500: train loss 2.3540, val loss 2.4392\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 1500: loss 1.5417, time 1212.88ms, mfu 0.01%\n",
      "iter 1600: loss 1.6714, time 118.14ms, mfu 0.01%\n",
      "iter 1700: loss 1.4882, time 94.96ms, mfu 0.01%\n",
      "step 1750: train loss 2.3282, val loss 2.3770\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 1800: loss 1.5770, time 215.50ms, mfu 0.01%\n",
      "iter 1900: loss 1.5075, time 125.50ms, mfu 0.01%\n",
      "step 2000: train loss 2.2962, val loss 2.3734\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 2000: loss 1.6038, time 1527.58ms, mfu 0.01%\n",
      "iter 2100: loss 1.5386, time 136.59ms, mfu 0.01%\n",
      "iter 2200: loss 1.5112, time 71.61ms, mfu 0.01%\n",
      "step 2250: train loss 2.2402, val loss 2.3309\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 2300: loss 1.5113, time 79.88ms, mfu 0.01%\n",
      "iter 2400: loss 1.4339, time 95.98ms, mfu 0.01%\n",
      "step 2500: train loss 2.2430, val loss 2.3127\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 2500: loss 1.5000, time 1151.85ms, mfu 0.01%\n",
      "iter 2600: loss 1.4298, time 84.50ms, mfu 0.01%\n",
      "iter 2700: loss 1.4186, time 174.91ms, mfu 0.01%\n",
      "step 2750: train loss 2.2305, val loss 2.2953\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 2800: loss 1.5716, time 76.05ms, mfu 0.01%\n",
      "iter 2900: loss 1.4384, time 142.16ms, mfu 0.01%\n",
      "step 3000: train loss 2.2423, val loss 2.3126\n",
      "iter 3000: loss 1.4267, time 1170.06ms, mfu 0.01%\n",
      "iter 3100: loss 1.3470, time 78.31ms, mfu 0.01%\n",
      "iter 3200: loss 1.4186, time 72.34ms, mfu 0.01%\n",
      "step 3250: train loss 2.1884, val loss 2.2715\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 3300: loss 1.4907, time 73.13ms, mfu 0.01%\n",
      "iter 3400: loss 1.4198, time 84.91ms, mfu 0.01%\n",
      "step 3500: train loss 2.1686, val loss 2.2686\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 3500: loss 1.3304, time 1138.70ms, mfu 0.01%\n",
      "iter 3600: loss 1.3207, time 85.17ms, mfu 0.01%\n",
      "iter 3700: loss 1.3208, time 104.85ms, mfu 0.01%\n",
      "step 3750: train loss 2.1529, val loss 2.2497\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 3800: loss 1.3551, time 134.70ms, mfu 0.01%\n",
      "iter 3900: loss 1.2745, time 86.38ms, mfu 0.01%\n",
      "step 4000: train loss 2.1296, val loss 2.2581\n",
      "iter 4000: loss 1.4472, time 1357.28ms, mfu 0.01%\n",
      "iter 4100: loss 1.2600, time 80.68ms, mfu 0.01%\n",
      "iter 4200: loss 1.2353, time 78.39ms, mfu 0.01%\n",
      "step 4250: train loss 2.1256, val loss 2.2365\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 4300: loss 1.2599, time 117.46ms, mfu 0.01%\n",
      "iter 4400: loss 1.4365, time 135.25ms, mfu 0.01%\n",
      "step 4500: train loss 2.1267, val loss 2.2386\n",
      "iter 4500: loss 1.3510, time 1124.25ms, mfu 0.01%\n",
      "iter 4600: loss 1.4251, time 112.60ms, mfu 0.01%\n",
      "iter 4700: loss 1.2957, time 125.52ms, mfu 0.01%\n",
      "step 4750: train loss 2.1245, val loss 2.2455\n",
      "iter 4800: loss 1.3477, time 94.50ms, mfu 0.01%\n",
      "iter 4900: loss 1.2210, time 128.06ms, mfu 0.01%\n",
      "step 5000: train loss 2.0795, val loss 2.2087\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 5000: loss 1.3957, time 1298.23ms, mfu 0.01%\n",
      "iter 5100: loss 1.2988, time 192.30ms, mfu 0.01%\n",
      "iter 5200: loss 1.2810, time 92.07ms, mfu 0.01%\n",
      "step 5250: train loss 2.0941, val loss 2.1890\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 5300: loss 1.1413, time 94.20ms, mfu 0.01%\n",
      "iter 5400: loss 1.3202, time 103.93ms, mfu 0.01%\n",
      "step 5500: train loss 2.1137, val loss 2.1840\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 5500: loss 1.3817, time 1211.49ms, mfu 0.01%\n",
      "iter 5600: loss 1.3358, time 90.76ms, mfu 0.01%\n",
      "iter 5700: loss 1.2436, time 106.00ms, mfu 0.01%\n",
      "step 5750: train loss 2.0633, val loss 2.1623\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 5800: loss 1.2561, time 110.41ms, mfu 0.01%\n",
      "iter 5900: loss 1.2717, time 112.02ms, mfu 0.01%\n",
      "step 6000: train loss 2.0643, val loss 2.1837\n",
      "iter 6000: loss 1.2965, time 1326.40ms, mfu 0.01%\n",
      "iter 6100: loss 1.3359, time 102.42ms, mfu 0.01%\n",
      "iter 6200: loss 1.2418, time 151.84ms, mfu 0.01%\n",
      "step 6250: train loss 2.0758, val loss 2.1690\n",
      "iter 6300: loss 1.3040, time 110.29ms, mfu 0.01%\n",
      "iter 6400: loss 1.1631, time 126.46ms, mfu 0.01%\n",
      "step 6500: train loss 2.0716, val loss 2.1787\n",
      "iter 6500: loss 1.2524, time 1229.53ms, mfu 0.01%\n",
      "iter 6600: loss 1.3234, time 121.55ms, mfu 0.01%\n",
      "iter 6700: loss 1.2153, time 108.20ms, mfu 0.01%\n",
      "step 6750: train loss 2.0571, val loss 2.1680\n",
      "iter 6800: loss 1.2600, time 125.50ms, mfu 0.01%\n",
      "iter 6900: loss 1.2565, time 68.11ms, mfu 0.01%\n",
      "step 7000: train loss 2.0409, val loss 2.1725\n",
      "iter 7000: loss 1.3094, time 1206.80ms, mfu 0.01%\n",
      "iter 7100: loss 1.2740, time 109.29ms, mfu 0.01%\n",
      "iter 7200: loss 1.1825, time 69.88ms, mfu 0.01%\n",
      "step 7250: train loss 2.0447, val loss 2.1661\n",
      "iter 7300: loss 1.2766, time 114.84ms, mfu 0.01%\n",
      "iter 7400: loss 1.2999, time 79.15ms, mfu 0.01%\n",
      "step 7500: train loss 2.0316, val loss 2.1606\n",
      "saving checkpoint to out/proposed_margin_1.1_separated_embeddings\n",
      "iter 7500: loss 1.2626, time 1453.78ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7600: loss 1.2161, time 75.21ms, mfu 0.01%\n",
      "iter 7700: loss 1.1835, time 105.95ms, mfu 0.01%\n",
      "step 7750: train loss 2.0475, val loss 2.1692\n",
      "iter 7800: loss 1.2863, time 157.10ms, mfu 0.01%\n",
      "iter 7900: loss 1.2396, time 117.22ms, mfu 0.01%\n",
      "step 8000: train loss 2.0251, val loss 2.1646\n",
      "iter 8000: loss 1.2849, time 1354.99ms, mfu 0.01%\n",
      "done for margin=1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_1.1_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.9_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.9\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8215, time 1346.01ms, mfu -100.00%\n",
      "iter 100: loss 2.2796, time 110.36ms, mfu 0.01%\n",
      "iter 200: loss 2.2080, time 147.59ms, mfu 0.01%\n",
      "step 250: train loss 3.0784, val loss 3.1039\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 300: loss 2.0279, time 102.45ms, mfu 0.01%\n",
      "iter 400: loss 1.9819, time 89.31ms, mfu 0.01%\n",
      "step 500: train loss 2.8762, val loss 2.8902\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 500: loss 2.0302, time 1066.18ms, mfu 0.01%\n",
      "iter 600: loss 1.8892, time 81.70ms, mfu 0.01%\n",
      "iter 700: loss 1.7882, time 115.82ms, mfu 0.01%\n",
      "step 750: train loss 2.7524, val loss 2.7710\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 800: loss 1.8916, time 118.56ms, mfu 0.01%\n",
      "iter 900: loss 1.7369, time 78.54ms, mfu 0.01%\n",
      "step 1000: train loss 2.6541, val loss 2.7107\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 1000: loss 1.7628, time 1190.69ms, mfu 0.01%\n",
      "iter 1100: loss 1.5746, time 117.26ms, mfu 0.01%\n",
      "iter 1200: loss 1.6179, time 137.98ms, mfu 0.01%\n",
      "step 1250: train loss 2.5914, val loss 2.6276\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 1300: loss 1.5918, time 76.29ms, mfu 0.01%\n",
      "iter 1400: loss 1.6026, time 75.33ms, mfu 0.01%\n",
      "step 1500: train loss 2.5178, val loss 2.5833\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 1500: loss 1.5083, time 1789.11ms, mfu 0.01%\n",
      "iter 1600: loss 1.6480, time 87.87ms, mfu 0.01%\n",
      "iter 1700: loss 1.4682, time 169.35ms, mfu 0.01%\n",
      "step 1750: train loss 2.4835, val loss 2.5287\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 1800: loss 1.5444, time 133.45ms, mfu 0.01%\n",
      "iter 1900: loss 1.4710, time 141.01ms, mfu 0.01%\n",
      "step 2000: train loss 2.4531, val loss 2.5227\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 2000: loss 1.5812, time 1530.02ms, mfu 0.01%\n",
      "iter 2100: loss 1.4900, time 151.57ms, mfu 0.01%\n",
      "iter 2200: loss 1.4758, time 104.45ms, mfu 0.01%\n",
      "step 2250: train loss 2.4108, val loss 2.4879\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 2300: loss 1.4940, time 113.98ms, mfu 0.01%\n",
      "iter 2400: loss 1.4204, time 77.00ms, mfu 0.01%\n",
      "step 2500: train loss 2.4163, val loss 2.4699\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 2500: loss 1.4662, time 1536.94ms, mfu 0.01%\n",
      "iter 2600: loss 1.3765, time 99.00ms, mfu 0.01%\n",
      "iter 2700: loss 1.3798, time 75.50ms, mfu 0.01%\n",
      "step 2750: train loss 2.4022, val loss 2.4589\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 2800: loss 1.5470, time 103.48ms, mfu 0.01%\n",
      "iter 2900: loss 1.4095, time 100.95ms, mfu 0.01%\n",
      "step 3000: train loss 2.4085, val loss 2.4656\n",
      "iter 3000: loss 1.3991, time 1115.05ms, mfu 0.01%\n",
      "iter 3100: loss 1.3276, time 83.77ms, mfu 0.01%\n",
      "iter 3200: loss 1.3796, time 110.24ms, mfu 0.01%\n",
      "step 3250: train loss 2.3623, val loss 2.4295\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 3300: loss 1.4486, time 146.38ms, mfu 0.01%\n",
      "iter 3400: loss 1.3690, time 110.67ms, mfu 0.01%\n",
      "step 3500: train loss 2.3523, val loss 2.4342\n",
      "iter 3500: loss 1.2941, time 1038.50ms, mfu 0.01%\n",
      "iter 3600: loss 1.3070, time 149.09ms, mfu 0.01%\n",
      "iter 3700: loss 1.2803, time 73.26ms, mfu 0.01%\n",
      "step 3750: train loss 2.3343, val loss 2.4175\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 3800: loss 1.3647, time 110.91ms, mfu 0.01%\n",
      "iter 3900: loss 1.2495, time 73.98ms, mfu 0.01%\n",
      "step 4000: train loss 2.3196, val loss 2.4315\n",
      "iter 4000: loss 1.4370, time 1156.75ms, mfu 0.01%\n",
      "iter 4100: loss 1.2496, time 133.17ms, mfu 0.01%\n",
      "iter 4200: loss 1.2038, time 123.08ms, mfu 0.01%\n",
      "step 4250: train loss 2.3107, val loss 2.4077\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 4300: loss 1.2448, time 635.33ms, mfu 0.01%\n",
      "iter 4400: loss 1.3939, time 93.09ms, mfu 0.01%\n",
      "step 4500: train loss 2.3035, val loss 2.3997\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 4500: loss 1.3278, time 1350.31ms, mfu 0.01%\n",
      "iter 4600: loss 1.3729, time 151.18ms, mfu 0.01%\n",
      "iter 4700: loss 1.2922, time 118.09ms, mfu 0.01%\n",
      "step 4750: train loss 2.3080, val loss 2.4197\n",
      "iter 4800: loss 1.3184, time 67.04ms, mfu 0.01%\n",
      "iter 4900: loss 1.1781, time 115.93ms, mfu 0.01%\n",
      "step 5000: train loss 2.2663, val loss 2.3780\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 5000: loss 1.3832, time 1500.30ms, mfu 0.01%\n",
      "iter 5100: loss 1.2820, time 222.77ms, mfu 0.01%\n",
      "iter 5200: loss 1.2512, time 130.62ms, mfu 0.01%\n",
      "step 5250: train loss 2.2881, val loss 2.3681\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 5300: loss 1.1321, time 112.45ms, mfu 0.01%\n",
      "iter 5400: loss 1.3060, time 132.30ms, mfu 0.01%\n",
      "step 5500: train loss 2.3009, val loss 2.3563\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 5500: loss 1.3575, time 1214.66ms, mfu 0.01%\n",
      "iter 5600: loss 1.2972, time 120.27ms, mfu 0.01%\n",
      "iter 5700: loss 1.2153, time 171.56ms, mfu 0.01%\n",
      "step 5750: train loss 2.2626, val loss 2.3462\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 5800: loss 1.2523, time 103.65ms, mfu 0.01%\n",
      "iter 5900: loss 1.2191, time 127.60ms, mfu 0.01%\n",
      "step 6000: train loss 2.2581, val loss 2.3631\n",
      "iter 6000: loss 1.2703, time 1020.74ms, mfu 0.01%\n",
      "iter 6100: loss 1.3106, time 125.61ms, mfu 0.01%\n",
      "iter 6200: loss 1.2211, time 158.87ms, mfu 0.01%\n",
      "step 6250: train loss 2.2833, val loss 2.3548\n",
      "iter 6300: loss 1.2762, time 85.38ms, mfu 0.01%\n",
      "iter 6400: loss 1.1383, time 99.88ms, mfu 0.01%\n",
      "step 6500: train loss 2.2660, val loss 2.3586\n",
      "iter 6500: loss 1.2440, time 1186.32ms, mfu 0.01%\n",
      "iter 6600: loss 1.3035, time 104.95ms, mfu 0.01%\n",
      "iter 6700: loss 1.2281, time 109.86ms, mfu 0.01%\n",
      "step 6750: train loss 2.2611, val loss 2.3543\n",
      "iter 6800: loss 1.2224, time 87.55ms, mfu 0.01%\n",
      "iter 6900: loss 1.2134, time 128.63ms, mfu 0.01%\n",
      "step 7000: train loss 2.2413, val loss 2.3560\n",
      "iter 7000: loss 1.3074, time 1177.39ms, mfu 0.01%\n",
      "iter 7100: loss 1.2488, time 96.95ms, mfu 0.01%\n",
      "iter 7200: loss 1.1584, time 86.14ms, mfu 0.01%\n",
      "step 7250: train loss 2.2498, val loss 2.3557\n",
      "iter 7300: loss 1.2558, time 74.92ms, mfu 0.01%\n",
      "iter 7400: loss 1.2756, time 79.10ms, mfu 0.01%\n",
      "step 7500: train loss 2.2328, val loss 2.3434\n",
      "saving checkpoint to out/proposed_margin_0.9_separated_embeddings\n",
      "iter 7500: loss 1.2479, time 1259.14ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7600: loss 1.1910, time 110.57ms, mfu 0.01%\n",
      "iter 7700: loss 1.1727, time 76.80ms, mfu 0.01%\n",
      "step 7750: train loss 2.2459, val loss 2.3476\n",
      "iter 7800: loss 1.2683, time 136.53ms, mfu 0.01%\n",
      "iter 7900: loss 1.2165, time 111.30ms, mfu 0.01%\n",
      "step 8000: train loss 2.2241, val loss 2.3472\n",
      "iter 8000: loss 1.2511, time 1485.40ms, mfu 0.01%\n",
      "done for margin=0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_0.9_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.8_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.8\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.8071, time 1387.78ms, mfu -100.00%\n",
      "iter 100: loss 2.2583, time 67.87ms, mfu 0.02%\n",
      "iter 200: loss 2.1786, time 112.96ms, mfu 0.02%\n",
      "step 250: train loss 3.1564, val loss 3.1834\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 300: loss 1.9963, time 96.82ms, mfu 0.02%\n",
      "iter 400: loss 1.9612, time 117.64ms, mfu 0.02%\n",
      "step 500: train loss 2.9583, val loss 2.9732\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 500: loss 1.9995, time 1523.32ms, mfu 0.02%\n",
      "iter 600: loss 1.8679, time 80.76ms, mfu 0.02%\n",
      "iter 700: loss 1.7743, time 75.73ms, mfu 0.02%\n",
      "step 750: train loss 2.8381, val loss 2.8560\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 800: loss 1.8728, time 97.10ms, mfu 0.02%\n",
      "iter 900: loss 1.7122, time 107.50ms, mfu 0.02%\n",
      "step 1000: train loss 2.7371, val loss 2.7911\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 1000: loss 1.7456, time 1476.07ms, mfu 0.01%\n",
      "iter 1100: loss 1.5665, time 68.83ms, mfu 0.01%\n",
      "iter 1200: loss 1.5882, time 81.77ms, mfu 0.01%\n",
      "step 1250: train loss 2.6850, val loss 2.7182\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 1300: loss 1.5526, time 106.84ms, mfu 0.01%\n",
      "iter 1400: loss 1.5783, time 68.62ms, mfu 0.01%\n",
      "step 1500: train loss 2.6156, val loss 2.6731\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 1500: loss 1.5091, time 1099.56ms, mfu 0.01%\n",
      "iter 1600: loss 1.6385, time 102.69ms, mfu 0.01%\n",
      "iter 1700: loss 1.4396, time 101.35ms, mfu 0.01%\n",
      "step 1750: train loss 2.5796, val loss 2.6164\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 1800: loss 1.5545, time 279.55ms, mfu 0.01%\n",
      "iter 1900: loss 1.4499, time 105.27ms, mfu 0.01%\n",
      "step 2000: train loss 2.5701, val loss 2.6368\n",
      "iter 2000: loss 1.5801, time 1008.51ms, mfu 0.01%\n",
      "iter 2100: loss 1.4667, time 157.97ms, mfu 0.01%\n",
      "iter 2200: loss 1.4679, time 66.46ms, mfu 0.01%\n",
      "step 2250: train loss 2.5203, val loss 2.5909\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 2300: loss 1.4831, time 115.08ms, mfu 0.01%\n",
      "iter 2400: loss 1.4056, time 139.54ms, mfu 0.01%\n",
      "step 2500: train loss 2.5292, val loss 2.5748\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 2500: loss 1.4434, time 1378.82ms, mfu 0.01%\n",
      "iter 2600: loss 1.3738, time 90.36ms, mfu 0.01%\n",
      "iter 2700: loss 1.3476, time 105.48ms, mfu 0.01%\n",
      "step 2750: train loss 2.5099, val loss 2.5685\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 2800: loss 1.5269, time 143.27ms, mfu 0.01%\n",
      "iter 2900: loss 1.4043, time 110.43ms, mfu 0.01%\n",
      "step 3000: train loss 2.5153, val loss 2.5662\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 3000: loss 1.3971, time 1319.21ms, mfu 0.01%\n",
      "iter 3100: loss 1.2951, time 99.46ms, mfu 0.01%\n",
      "iter 3200: loss 1.3545, time 91.97ms, mfu 0.01%\n",
      "step 3250: train loss 2.4784, val loss 2.5440\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 3300: loss 1.4081, time 122.08ms, mfu 0.01%\n",
      "iter 3400: loss 1.3568, time 106.86ms, mfu 0.01%\n",
      "step 3500: train loss 2.4667, val loss 2.5496\n",
      "iter 3500: loss 1.2599, time 1418.09ms, mfu 0.01%\n",
      "iter 3600: loss 1.2902, time 121.68ms, mfu 0.01%\n",
      "iter 3700: loss 1.2912, time 118.49ms, mfu 0.01%\n",
      "step 3750: train loss 2.4538, val loss 2.5372\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 3800: loss 1.3344, time 99.87ms, mfu 0.01%\n",
      "iter 3900: loss 1.2324, time 110.71ms, mfu 0.01%\n",
      "step 4000: train loss 2.4277, val loss 2.5365\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 4000: loss 1.4257, time 1213.39ms, mfu 0.01%\n",
      "iter 4100: loss 1.2329, time 107.66ms, mfu 0.01%\n",
      "iter 4200: loss 1.1897, time 148.66ms, mfu 0.01%\n",
      "step 4250: train loss 2.4247, val loss 2.5135\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 4300: loss 1.2143, time 80.17ms, mfu 0.01%\n",
      "iter 4400: loss 1.3730, time 94.86ms, mfu 0.01%\n",
      "step 4500: train loss 2.4265, val loss 2.5173\n",
      "iter 4500: loss 1.3083, time 1337.34ms, mfu 0.01%\n",
      "iter 4600: loss 1.3894, time 98.69ms, mfu 0.01%\n",
      "iter 4700: loss 1.2681, time 108.38ms, mfu 0.01%\n",
      "step 4750: train loss 2.4223, val loss 2.5240\n",
      "iter 4800: loss 1.2901, time 80.86ms, mfu 0.01%\n",
      "iter 4900: loss 1.1773, time 81.32ms, mfu 0.01%\n",
      "step 5000: train loss 2.3892, val loss 2.4944\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 5000: loss 1.3851, time 1136.97ms, mfu 0.01%\n",
      "iter 5100: loss 1.2635, time 85.75ms, mfu 0.01%\n",
      "iter 5200: loss 1.2445, time 116.77ms, mfu 0.01%\n",
      "step 5250: train loss 2.4023, val loss 2.4802\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 5300: loss 1.1235, time 109.62ms, mfu 0.01%\n",
      "iter 5400: loss 1.3070, time 80.68ms, mfu 0.01%\n",
      "step 5500: train loss 2.4168, val loss 2.4706\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 5500: loss 1.3313, time 1332.66ms, mfu 0.01%\n",
      "iter 5600: loss 1.2734, time 140.63ms, mfu 0.01%\n",
      "iter 5700: loss 1.2063, time 117.02ms, mfu 0.01%\n",
      "step 5750: train loss 2.3751, val loss 2.4614\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 5800: loss 1.2292, time 172.89ms, mfu 0.01%\n",
      "iter 5900: loss 1.2180, time 78.98ms, mfu 0.01%\n",
      "step 6000: train loss 2.3757, val loss 2.4798\n",
      "iter 6000: loss 1.2609, time 1099.84ms, mfu 0.01%\n",
      "iter 6100: loss 1.3070, time 106.66ms, mfu 0.01%\n",
      "iter 6200: loss 1.1985, time 107.46ms, mfu 0.01%\n",
      "step 6250: train loss 2.3942, val loss 2.4640\n",
      "iter 6300: loss 1.2940, time 87.33ms, mfu 0.01%\n",
      "iter 6400: loss 1.1291, time 78.77ms, mfu 0.01%\n",
      "step 6500: train loss 2.3852, val loss 2.4748\n",
      "iter 6500: loss 1.2457, time 1203.50ms, mfu 0.01%\n",
      "iter 6600: loss 1.2932, time 98.53ms, mfu 0.01%\n",
      "iter 6700: loss 1.2096, time 101.06ms, mfu 0.01%\n",
      "step 6750: train loss 2.3807, val loss 2.4712\n",
      "iter 6800: loss 1.1967, time 90.32ms, mfu 0.01%\n",
      "iter 6900: loss 1.2174, time 138.74ms, mfu 0.01%\n",
      "step 7000: train loss 2.3633, val loss 2.4772\n",
      "iter 7000: loss 1.2936, time 1345.43ms, mfu 0.01%\n",
      "iter 7100: loss 1.2331, time 82.31ms, mfu 0.01%\n",
      "iter 7200: loss 1.1536, time 134.90ms, mfu 0.01%\n",
      "step 7250: train loss 2.3697, val loss 2.4739\n",
      "iter 7300: loss 1.2402, time 102.42ms, mfu 0.01%\n",
      "iter 7400: loss 1.2514, time 124.50ms, mfu 0.01%\n",
      "step 7500: train loss 2.3569, val loss 2.4616\n",
      "iter 7500: loss 1.2269, time 1243.54ms, mfu 0.01%\n",
      "iter 7600: loss 1.1822, time 97.10ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7700: loss 1.1568, time 127.94ms, mfu 0.01%\n",
      "step 7750: train loss 2.3618, val loss 2.4607\n",
      "saving checkpoint to out/proposed_margin_0.8_separated_embeddings\n",
      "iter 7800: loss 1.2467, time 112.55ms, mfu 0.01%\n",
      "iter 7900: loss 1.1789, time 89.82ms, mfu 0.01%\n",
      "step 8000: train loss 2.3474, val loss 2.4686\n",
      "iter 8000: loss 1.2376, time 1094.62ms, mfu 0.01%\n",
      "done for margin=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_0.8_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.7_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.7\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.7892, time 1637.81ms, mfu -100.00%\n",
      "iter 100: loss 2.2202, time 96.80ms, mfu 0.01%\n",
      "iter 200: loss 2.1534, time 120.20ms, mfu 0.01%\n",
      "step 250: train loss 3.2482, val loss 3.2717\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 300: loss 1.9606, time 113.60ms, mfu 0.01%\n",
      "iter 400: loss 1.9224, time 70.28ms, mfu 0.01%\n",
      "step 500: train loss 3.0539, val loss 3.0685\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 500: loss 1.9873, time 1225.23ms, mfu 0.01%\n",
      "iter 600: loss 1.8453, time 71.89ms, mfu 0.01%\n",
      "iter 700: loss 1.7548, time 89.80ms, mfu 0.01%\n",
      "step 750: train loss 2.9464, val loss 2.9671\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 800: loss 1.8194, time 105.03ms, mfu 0.01%\n",
      "iter 900: loss 1.6972, time 133.31ms, mfu 0.01%\n",
      "step 1000: train loss 2.8503, val loss 2.8964\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 1000: loss 1.7216, time 1359.17ms, mfu 0.01%\n",
      "iter 1100: loss 1.5325, time 120.05ms, mfu 0.01%\n",
      "iter 1200: loss 1.5779, time 129.14ms, mfu 0.01%\n",
      "step 1250: train loss 2.7986, val loss 2.8343\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 1300: loss 1.5363, time 112.93ms, mfu 0.01%\n",
      "iter 1400: loss 1.5611, time 104.90ms, mfu 0.01%\n",
      "step 1500: train loss 2.7284, val loss 2.7830\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 1500: loss 1.4991, time 1203.61ms, mfu 0.01%\n",
      "iter 1600: loss 1.6160, time 215.19ms, mfu 0.01%\n",
      "iter 1700: loss 1.4253, time 67.80ms, mfu 0.01%\n",
      "step 1750: train loss 2.7023, val loss 2.7328\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 1800: loss 1.5303, time 136.38ms, mfu 0.01%\n",
      "iter 1900: loss 1.4444, time 112.47ms, mfu 0.01%\n",
      "step 2000: train loss 2.6771, val loss 2.7320\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 2000: loss 1.5479, time 1440.28ms, mfu 0.01%\n",
      "iter 2100: loss 1.4638, time 120.06ms, mfu 0.01%\n",
      "iter 2200: loss 1.4334, time 67.50ms, mfu 0.01%\n",
      "step 2250: train loss 2.6302, val loss 2.6947\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 2300: loss 1.4496, time 66.52ms, mfu 0.01%\n",
      "iter 2400: loss 1.3611, time 108.84ms, mfu 0.01%\n",
      "step 2500: train loss 2.6394, val loss 2.6817\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 2500: loss 1.4139, time 1409.91ms, mfu 0.01%\n",
      "iter 2600: loss 1.3535, time 131.75ms, mfu 0.01%\n",
      "iter 2700: loss 1.3682, time 72.67ms, mfu 0.01%\n",
      "step 2750: train loss 2.6337, val loss 2.6834\n",
      "iter 2800: loss 1.5346, time 75.83ms, mfu 0.01%\n",
      "iter 2900: loss 1.3596, time 72.16ms, mfu 0.01%\n",
      "step 3000: train loss 2.6427, val loss 2.6974\n",
      "iter 3000: loss 1.3675, time 1100.64ms, mfu 0.01%\n",
      "iter 3100: loss 1.2756, time 110.90ms, mfu 0.01%\n",
      "iter 3200: loss 1.3519, time 197.85ms, mfu 0.01%\n",
      "step 3250: train loss 2.6053, val loss 2.6660\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 3300: loss 1.3931, time 96.83ms, mfu 0.01%\n",
      "iter 3400: loss 1.3466, time 128.95ms, mfu 0.01%\n",
      "step 3500: train loss 2.5907, val loss 2.6589\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 3500: loss 1.2444, time 1099.85ms, mfu 0.01%\n",
      "iter 3600: loss 1.2657, time 120.27ms, mfu 0.01%\n",
      "iter 3700: loss 1.2393, time 111.67ms, mfu 0.01%\n",
      "step 3750: train loss 2.5806, val loss 2.6462\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 3800: loss 1.3103, time 107.32ms, mfu 0.01%\n",
      "iter 3900: loss 1.2242, time 95.40ms, mfu 0.01%\n",
      "step 4000: train loss 2.5616, val loss 2.6563\n",
      "iter 4000: loss 1.3740, time 1001.66ms, mfu 0.01%\n",
      "iter 4100: loss 1.2028, time 108.34ms, mfu 0.01%\n",
      "iter 4200: loss 1.1697, time 89.86ms, mfu 0.01%\n",
      "step 4250: train loss 2.5567, val loss 2.6330\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 4300: loss 1.1939, time 139.76ms, mfu 0.01%\n",
      "iter 4400: loss 1.3602, time 94.28ms, mfu 0.01%\n",
      "step 4500: train loss 2.5571, val loss 2.6375\n",
      "iter 4500: loss 1.2830, time 1219.44ms, mfu 0.01%\n",
      "iter 4600: loss 1.3647, time 108.12ms, mfu 0.01%\n",
      "iter 4700: loss 1.2522, time 101.98ms, mfu 0.01%\n",
      "step 4750: train loss 2.5599, val loss 2.6488\n",
      "iter 4800: loss 1.2933, time 101.75ms, mfu 0.01%\n",
      "iter 4900: loss 1.1609, time 110.69ms, mfu 0.01%\n",
      "step 5000: train loss 2.5194, val loss 2.6142\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 5000: loss 1.3390, time 1010.59ms, mfu 0.01%\n",
      "iter 5100: loss 1.2616, time 70.70ms, mfu 0.01%\n",
      "iter 5200: loss 1.2175, time 77.84ms, mfu 0.01%\n",
      "step 5250: train loss 2.5360, val loss 2.6052\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 5300: loss 1.0992, time 90.57ms, mfu 0.01%\n",
      "iter 5400: loss 1.2814, time 118.85ms, mfu 0.01%\n",
      "step 5500: train loss 2.5406, val loss 2.5913\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 5500: loss 1.3174, time 1586.37ms, mfu 0.01%\n",
      "iter 5600: loss 1.2585, time 128.70ms, mfu 0.01%\n",
      "iter 5700: loss 1.1766, time 92.31ms, mfu 0.01%\n",
      "step 5750: train loss 2.5176, val loss 2.5915\n",
      "iter 5800: loss 1.2183, time 117.36ms, mfu 0.01%\n",
      "iter 5900: loss 1.2297, time 145.02ms, mfu 0.01%\n",
      "step 6000: train loss 2.5132, val loss 2.5984\n",
      "iter 6000: loss 1.2111, time 1028.16ms, mfu 0.01%\n",
      "iter 6100: loss 1.2604, time 97.66ms, mfu 0.01%\n",
      "iter 6200: loss 1.1968, time 186.71ms, mfu 0.01%\n",
      "step 6250: train loss 2.5285, val loss 2.5919\n",
      "iter 6300: loss 1.2469, time 77.96ms, mfu 0.01%\n",
      "iter 6400: loss 1.1121, time 101.32ms, mfu 0.01%\n",
      "step 6500: train loss 2.5126, val loss 2.5924\n",
      "iter 6500: loss 1.1882, time 1124.90ms, mfu 0.01%\n",
      "iter 6600: loss 1.2555, time 106.77ms, mfu 0.01%\n",
      "iter 6700: loss 1.1641, time 97.17ms, mfu 0.01%\n",
      "step 6750: train loss 2.5081, val loss 2.5829\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 6800: loss 1.1899, time 89.64ms, mfu 0.01%\n",
      "iter 6900: loss 1.1849, time 146.87ms, mfu 0.01%\n",
      "step 7000: train loss 2.4947, val loss 2.5858\n",
      "iter 7000: loss 1.2379, time 1404.89ms, mfu 0.01%\n",
      "iter 7100: loss 1.2022, time 107.89ms, mfu 0.01%\n",
      "iter 7200: loss 1.1326, time 89.72ms, mfu 0.01%\n",
      "step 7250: train loss 2.4975, val loss 2.5851\n",
      "iter 7300: loss 1.2380, time 121.02ms, mfu 0.01%\n",
      "iter 7400: loss 1.2209, time 106.25ms, mfu 0.01%\n",
      "step 7500: train loss 2.4832, val loss 2.5774\n",
      "saving checkpoint to out/proposed_margin_0.7_separated_embeddings\n",
      "iter 7500: loss 1.2227, time 1137.13ms, mfu 0.01%\n",
      "iter 7600: loss 1.1551, time 70.48ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7700: loss 1.1440, time 101.68ms, mfu 0.01%\n",
      "step 7750: train loss 2.4909, val loss 2.5805\n",
      "iter 7800: loss 1.2374, time 102.52ms, mfu 0.01%\n",
      "iter 7900: loss 1.1653, time 107.77ms, mfu 0.01%\n",
      "step 8000: train loss 2.4796, val loss 2.5830\n",
      "iter 8000: loss 1.2122, time 1340.92ms, mfu 0.01%\n",
      "done for margin=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_0.7_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.6_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.6\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.7676, time 1145.98ms, mfu -100.00%\n",
      "iter 100: loss 2.1697, time 126.85ms, mfu 0.01%\n",
      "iter 200: loss 2.1082, time 68.81ms, mfu 0.01%\n",
      "step 250: train loss 3.3578, val loss 3.3806\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 300: loss 1.9343, time 123.70ms, mfu 0.01%\n",
      "iter 400: loss 1.9185, time 256.05ms, mfu 0.01%\n",
      "step 500: train loss 3.1729, val loss 3.1882\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 500: loss 1.9378, time 1082.25ms, mfu 0.01%\n",
      "iter 600: loss 1.8171, time 114.27ms, mfu 0.01%\n",
      "iter 700: loss 1.7195, time 83.89ms, mfu 0.01%\n",
      "step 750: train loss 3.0669, val loss 3.0860\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 800: loss 1.7983, time 125.60ms, mfu 0.01%\n",
      "iter 900: loss 1.6648, time 78.06ms, mfu 0.01%\n",
      "step 1000: train loss 2.9872, val loss 3.0326\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 1000: loss 1.7165, time 1722.94ms, mfu 0.01%\n",
      "iter 1100: loss 1.4964, time 78.06ms, mfu 0.01%\n",
      "iter 1200: loss 1.5543, time 157.12ms, mfu 0.01%\n",
      "step 1250: train loss 2.9299, val loss 2.9620\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 1300: loss 1.5043, time 95.63ms, mfu 0.01%\n",
      "iter 1400: loss 1.5345, time 98.77ms, mfu 0.01%\n",
      "step 1500: train loss 2.8746, val loss 2.9269\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 1500: loss 1.4812, time 1469.79ms, mfu 0.01%\n",
      "iter 1600: loss 1.5914, time 113.64ms, mfu 0.01%\n",
      "iter 1700: loss 1.4052, time 119.81ms, mfu 0.01%\n",
      "step 1750: train loss 2.8516, val loss 2.8807\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 1800: loss 1.5151, time 130.85ms, mfu 0.01%\n",
      "iter 1900: loss 1.4090, time 72.98ms, mfu 0.01%\n",
      "step 2000: train loss 2.8318, val loss 2.8877\n",
      "iter 2000: loss 1.5409, time 1047.63ms, mfu 0.01%\n",
      "iter 2100: loss 1.4181, time 112.68ms, mfu 0.01%\n",
      "iter 2200: loss 1.4246, time 108.72ms, mfu 0.01%\n",
      "step 2250: train loss 2.7900, val loss 2.8493\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 2300: loss 1.4197, time 110.38ms, mfu 0.01%\n",
      "iter 2400: loss 1.3646, time 93.18ms, mfu 0.01%\n",
      "step 2500: train loss 2.7924, val loss 2.8317\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 2500: loss 1.4214, time 1249.05ms, mfu 0.01%\n",
      "iter 2600: loss 1.3180, time 81.07ms, mfu 0.01%\n",
      "iter 2700: loss 1.3302, time 100.83ms, mfu 0.01%\n",
      "step 2750: train loss 2.7832, val loss 2.8328\n",
      "iter 2800: loss 1.4755, time 86.00ms, mfu 0.01%\n",
      "iter 2900: loss 1.3323, time 80.91ms, mfu 0.01%\n",
      "step 3000: train loss 2.7940, val loss 2.8456\n",
      "iter 3000: loss 1.3327, time 1211.92ms, mfu 0.01%\n",
      "iter 3100: loss 1.2692, time 97.06ms, mfu 0.01%\n",
      "iter 3200: loss 1.2925, time 108.55ms, mfu 0.01%\n",
      "step 3250: train loss 2.7519, val loss 2.8081\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 3300: loss 1.3849, time 94.81ms, mfu 0.01%\n",
      "iter 3400: loss 1.3213, time 174.80ms, mfu 0.01%\n",
      "step 3500: train loss 2.7369, val loss 2.8091\n",
      "iter 3500: loss 1.2192, time 1182.76ms, mfu 0.01%\n",
      "iter 3600: loss 1.2437, time 75.76ms, mfu 0.01%\n",
      "iter 3700: loss 1.2336, time 119.02ms, mfu 0.01%\n",
      "step 3750: train loss 2.7180, val loss 2.7817\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 3800: loss 1.3103, time 73.03ms, mfu 0.01%\n",
      "iter 3900: loss 1.2125, time 174.43ms, mfu 0.01%\n",
      "step 4000: train loss 2.7035, val loss 2.7861\n",
      "iter 4000: loss 1.3648, time 1113.82ms, mfu 0.01%\n",
      "iter 4100: loss 1.1993, time 68.47ms, mfu 0.01%\n",
      "iter 4200: loss 1.1595, time 86.19ms, mfu 0.01%\n",
      "step 4250: train loss 2.7071, val loss 2.7856\n",
      "iter 4300: loss 1.1950, time 215.08ms, mfu 0.01%\n",
      "iter 4400: loss 1.3245, time 80.27ms, mfu 0.01%\n",
      "step 4500: train loss 2.6992, val loss 2.7734\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 4500: loss 1.2572, time 1573.82ms, mfu 0.01%\n",
      "iter 4600: loss 1.3222, time 90.63ms, mfu 0.01%\n",
      "iter 4700: loss 1.2147, time 73.42ms, mfu 0.01%\n",
      "step 4750: train loss 2.6998, val loss 2.7801\n",
      "iter 4800: loss 1.2594, time 89.36ms, mfu 0.01%\n",
      "iter 4900: loss 1.1392, time 123.56ms, mfu 0.01%\n",
      "step 5000: train loss 2.6563, val loss 2.7425\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 5000: loss 1.2903, time 1427.93ms, mfu 0.01%\n",
      "iter 5100: loss 1.2302, time 67.27ms, mfu 0.01%\n",
      "iter 5200: loss 1.2109, time 118.84ms, mfu 0.01%\n",
      "step 5250: train loss 2.6684, val loss 2.7348\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 5300: loss 1.0808, time 81.19ms, mfu 0.01%\n",
      "iter 5400: loss 1.2566, time 70.86ms, mfu 0.01%\n",
      "step 5500: train loss 2.6800, val loss 2.7275\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 5500: loss 1.2819, time 1196.86ms, mfu 0.01%\n",
      "iter 5600: loss 1.2418, time 114.00ms, mfu 0.01%\n",
      "iter 5700: loss 1.1608, time 80.59ms, mfu 0.01%\n",
      "step 5750: train loss 2.6508, val loss 2.7184\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 5800: loss 1.1833, time 118.23ms, mfu 0.01%\n",
      "iter 5900: loss 1.1954, time 92.79ms, mfu 0.01%\n",
      "step 6000: train loss 2.6437, val loss 2.7251\n",
      "iter 6000: loss 1.1863, time 1160.46ms, mfu 0.01%\n",
      "iter 6100: loss 1.2318, time 82.90ms, mfu 0.01%\n",
      "iter 6200: loss 1.1783, time 91.01ms, mfu 0.01%\n",
      "step 6250: train loss 2.6586, val loss 2.7188\n",
      "iter 6300: loss 1.2034, time 86.52ms, mfu 0.01%\n",
      "iter 6400: loss 1.1010, time 101.75ms, mfu 0.01%\n",
      "step 6500: train loss 2.6508, val loss 2.7266\n",
      "iter 6500: loss 1.1641, time 1202.33ms, mfu 0.01%\n",
      "iter 6600: loss 1.2199, time 120.90ms, mfu 0.01%\n",
      "iter 6700: loss 1.1034, time 138.75ms, mfu 0.01%\n",
      "step 6750: train loss 2.6497, val loss 2.7202\n",
      "iter 6800: loss 1.1725, time 112.80ms, mfu 0.01%\n",
      "iter 6900: loss 1.1694, time 110.51ms, mfu 0.01%\n",
      "step 7000: train loss 2.6294, val loss 2.7115\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 7000: loss 1.1991, time 1131.31ms, mfu 0.01%\n",
      "iter 7100: loss 1.1807, time 88.12ms, mfu 0.01%\n",
      "iter 7200: loss 1.1238, time 129.68ms, mfu 0.01%\n",
      "step 7250: train loss 2.6345, val loss 2.7151\n",
      "iter 7300: loss 1.2130, time 115.58ms, mfu 0.01%\n",
      "iter 7400: loss 1.2106, time 108.37ms, mfu 0.01%\n",
      "step 7500: train loss 2.6243, val loss 2.7101\n",
      "saving checkpoint to out/proposed_margin_0.6_separated_embeddings\n",
      "iter 7500: loss 1.2063, time 1689.58ms, mfu 0.01%\n",
      "iter 7600: loss 1.1310, time 105.86ms, mfu 0.01%\n",
      "iter 7700: loss 1.1279, time 86.76ms, mfu 0.01%\n",
      "step 7750: train loss 2.6309, val loss 2.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7800: loss 1.2304, time 86.33ms, mfu 0.01%\n",
      "iter 7900: loss 1.1561, time 133.07ms, mfu 0.01%\n",
      "step 8000: train loss 2.6183, val loss 2.7116\n",
      "iter 8000: loss 1.1914, time 1193.53ms, mfu 0.01%\n",
      "done for margin=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_0.6_separated_embeddings/ckpt.pt\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_0.5_separated_embeddings\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 0.5\n",
      "Overriding: separated_embeddings = True\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 147, with 811,264 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8737, val loss 4.8674\n",
      "iter 0: loss 4.7354, time 1607.05ms, mfu -100.00%\n",
      "iter 100: loss 2.1285, time 146.19ms, mfu 0.01%\n",
      "iter 200: loss 2.0841, time 146.47ms, mfu 0.01%\n",
      "step 250: train loss 3.4854, val loss 3.5014\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 300: loss 1.8937, time 96.74ms, mfu 0.01%\n",
      "iter 400: loss 1.8543, time 92.14ms, mfu 0.01%\n",
      "step 500: train loss 3.3004, val loss 3.3141\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 500: loss 1.8922, time 2014.60ms, mfu 0.01%\n",
      "iter 600: loss 1.7665, time 162.12ms, mfu 0.01%\n",
      "iter 700: loss 1.6843, time 86.21ms, mfu 0.01%\n",
      "step 750: train loss 3.2197, val loss 3.2330\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 800: loss 1.7609, time 107.56ms, mfu 0.01%\n",
      "iter 900: loss 1.6334, time 116.99ms, mfu 0.01%\n",
      "step 1000: train loss 3.1488, val loss 3.1873\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 1000: loss 1.6753, time 1351.38ms, mfu 0.01%\n",
      "iter 1100: loss 1.4720, time 84.87ms, mfu 0.01%\n",
      "iter 1200: loss 1.5399, time 119.21ms, mfu 0.01%\n",
      "step 1250: train loss 3.1079, val loss 3.1335\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 1300: loss 1.4945, time 171.45ms, mfu 0.01%\n",
      "iter 1400: loss 1.5092, time 98.16ms, mfu 0.01%\n",
      "step 1500: train loss 3.0523, val loss 3.0954\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 1500: loss 1.4250, time 1573.51ms, mfu 0.01%\n",
      "iter 1600: loss 1.5664, time 103.17ms, mfu 0.01%\n",
      "iter 1700: loss 1.3768, time 144.01ms, mfu 0.01%\n",
      "step 1750: train loss 3.0259, val loss 3.0542\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 1800: loss 1.4707, time 131.40ms, mfu 0.01%\n",
      "iter 1900: loss 1.3933, time 102.69ms, mfu 0.01%\n",
      "step 2000: train loss 3.0141, val loss 3.0640\n",
      "iter 2000: loss 1.4903, time 1266.70ms, mfu 0.01%\n",
      "iter 2100: loss 1.3989, time 115.53ms, mfu 0.01%\n",
      "iter 2200: loss 1.3814, time 256.94ms, mfu 0.01%\n",
      "step 2250: train loss 2.9752, val loss 3.0240\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 2300: loss 1.3977, time 117.99ms, mfu 0.01%\n",
      "iter 2400: loss 1.3467, time 118.10ms, mfu 0.01%\n",
      "step 2500: train loss 2.9791, val loss 3.0138\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 2500: loss 1.3727, time 1711.45ms, mfu 0.01%\n",
      "iter 2600: loss 1.3027, time 120.43ms, mfu 0.01%\n",
      "iter 2700: loss 1.3038, time 126.22ms, mfu 0.01%\n",
      "step 2750: train loss 2.9725, val loss 3.0151\n",
      "iter 2800: loss 1.4467, time 139.20ms, mfu 0.01%\n",
      "iter 2900: loss 1.3265, time 90.87ms, mfu 0.01%\n",
      "step 3000: train loss 2.9850, val loss 3.0236\n",
      "iter 3000: loss 1.3021, time 1528.88ms, mfu 0.01%\n",
      "iter 3100: loss 1.2247, time 111.25ms, mfu 0.01%\n",
      "iter 3200: loss 1.2827, time 110.52ms, mfu 0.01%\n",
      "step 3250: train loss 2.9521, val loss 2.9965\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 3300: loss 1.3439, time 125.74ms, mfu 0.01%\n",
      "iter 3400: loss 1.2879, time 87.92ms, mfu 0.01%\n",
      "step 3500: train loss 2.9438, val loss 3.0030\n",
      "iter 3500: loss 1.2202, time 1191.77ms, mfu 0.01%\n",
      "iter 3600: loss 1.2273, time 120.10ms, mfu 0.01%\n",
      "iter 3700: loss 1.2127, time 235.47ms, mfu 0.01%\n",
      "step 3750: train loss 2.9164, val loss 2.9751\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 3800: loss 1.2685, time 103.07ms, mfu 0.01%\n",
      "iter 3900: loss 1.1708, time 103.85ms, mfu 0.01%\n",
      "step 4000: train loss 2.9124, val loss 2.9898\n",
      "iter 4000: loss 1.3236, time 1455.12ms, mfu 0.01%\n",
      "iter 4100: loss 1.1520, time 130.12ms, mfu 0.01%\n",
      "iter 4200: loss 1.1104, time 132.10ms, mfu 0.01%\n",
      "step 4250: train loss 2.9113, val loss 2.9817\n",
      "iter 4300: loss 1.1650, time 142.42ms, mfu 0.01%\n",
      "iter 4400: loss 1.3002, time 88.13ms, mfu 0.01%\n",
      "step 4500: train loss 2.9053, val loss 2.9771\n",
      "iter 4500: loss 1.2503, time 1555.74ms, mfu 0.01%\n",
      "iter 4600: loss 1.3259, time 126.31ms, mfu 0.01%\n",
      "iter 4700: loss 1.2352, time 96.56ms, mfu 0.01%\n",
      "step 4750: train loss 2.9022, val loss 2.9743\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 4800: loss 1.2545, time 143.67ms, mfu 0.01%\n",
      "iter 4900: loss 1.1150, time 123.14ms, mfu 0.01%\n",
      "step 5000: train loss 2.8780, val loss 2.9572\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 5000: loss 1.2878, time 1913.13ms, mfu 0.01%\n",
      "iter 5100: loss 1.2263, time 112.11ms, mfu 0.01%\n",
      "iter 5200: loss 1.1718, time 96.72ms, mfu 0.01%\n",
      "step 5250: train loss 2.8847, val loss 2.9418\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 5300: loss 1.0725, time 263.23ms, mfu 0.01%\n",
      "iter 5400: loss 1.2346, time 114.00ms, mfu 0.01%\n",
      "step 5500: train loss 2.8895, val loss 2.9298\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 5500: loss 1.2580, time 1639.07ms, mfu 0.01%\n",
      "iter 5600: loss 1.2196, time 136.50ms, mfu 0.01%\n",
      "iter 5700: loss 1.1396, time 104.54ms, mfu 0.01%\n",
      "step 5750: train loss 2.8675, val loss 2.9314\n",
      "iter 5800: loss 1.1919, time 125.62ms, mfu 0.01%\n",
      "iter 5900: loss 1.1829, time 95.98ms, mfu 0.01%\n",
      "step 6000: train loss 2.8615, val loss 2.9343\n",
      "iter 6000: loss 1.1981, time 1415.00ms, mfu 0.01%\n",
      "iter 6100: loss 1.2071, time 125.93ms, mfu 0.01%\n",
      "iter 6200: loss 1.1231, time 124.43ms, mfu 0.01%\n",
      "step 6250: train loss 2.8671, val loss 2.9222\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 6300: loss 1.1770, time 150.72ms, mfu 0.01%\n",
      "iter 6400: loss 1.0935, time 134.41ms, mfu 0.01%\n",
      "step 6500: train loss 2.8663, val loss 2.9360\n",
      "iter 6500: loss 1.1662, time 1397.74ms, mfu 0.01%\n",
      "iter 6600: loss 1.2053, time 103.43ms, mfu 0.01%\n",
      "iter 6700: loss 1.1101, time 113.11ms, mfu 0.01%\n",
      "step 6750: train loss 2.8536, val loss 2.9232\n",
      "iter 6800: loss 1.1586, time 125.62ms, mfu 0.01%\n",
      "iter 6900: loss 1.1535, time 98.09ms, mfu 0.01%\n",
      "step 7000: train loss 2.8408, val loss 2.9212\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 7000: loss 1.1953, time 1441.65ms, mfu 0.01%\n",
      "iter 7100: loss 1.1416, time 116.20ms, mfu 0.01%\n",
      "iter 7200: loss 1.0878, time 114.98ms, mfu 0.01%\n",
      "step 7250: train loss 2.8505, val loss 2.9245\n",
      "iter 7300: loss 1.1935, time 124.47ms, mfu 0.01%\n",
      "iter 7400: loss 1.1818, time 126.71ms, mfu 0.01%\n",
      "step 7500: train loss 2.8370, val loss 2.9157\n",
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 7500: loss 1.1709, time 1485.58ms, mfu 0.01%\n",
      "iter 7600: loss 1.1429, time 91.13ms, mfu 0.01%\n",
      "iter 7700: loss 1.1036, time 108.60ms, mfu 0.01%\n",
      "step 7750: train loss 2.8392, val loss 2.9154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to out/proposed_margin_0.5_separated_embeddings\n",
      "iter 7800: loss 1.1913, time 277.35ms, mfu 0.01%\n",
      "iter 7900: loss 1.1317, time 245.38ms, mfu 0.01%\n",
      "step 8000: train loss 2.8286, val loss 2.9164\n",
      "iter 8000: loss 1.1740, time 1246.93ms, mfu 0.01%\n",
      "done for margin=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/2403027348.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed checkpoint saved to out/proposed_margin_0.5_separated_embeddings/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "for margin in [1.5, 1.4, 1.3, 1.2, 1.1, 0.9, 0.8, 0.7, 0.6, 0.5]:\n",
    "    model_name = f'proposed_margin_{margin}_separated_embeddings'\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/{model_name}\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "        \"separated_embeddings\": \"True\"\n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')\n",
    "    fix_ckpt(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29d8119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_4.0_wo_weight_tying\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 4.0\n",
      "Overriding: weight_tying = False\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8700, val loss 4.8689\n",
      "iter 0: loss 4.8684, time 1791.11ms, mfu -100.00%\n",
      "iter 100: loss 2.7774, time 53.92ms, mfu 0.02%\n",
      "iter 200: loss 2.5005, time 71.73ms, mfu 0.02%\n",
      "step 250: train loss 2.5186, val loss 2.5315\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 300: loss 2.4007, time 39.20ms, mfu 0.02%\n",
      "iter 400: loss 2.4186, time 81.61ms, mfu 0.02%\n",
      "step 500: train loss 2.2840, val loss 2.3315\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 500: loss 2.2199, time 872.46ms, mfu 0.02%\n",
      "iter 600: loss 2.1531, time 63.16ms, mfu 0.02%\n",
      "iter 700: loss 2.1129, time 120.24ms, mfu 0.02%\n",
      "step 750: train loss 2.1504, val loss 2.1779\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 800: loss 2.1826, time 43.72ms, mfu 0.02%\n",
      "iter 900: loss 1.9574, time 68.37ms, mfu 0.02%\n",
      "step 1000: train loss 2.0048, val loss 2.0743\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 1000: loss 2.0139, time 879.74ms, mfu 0.02%\n",
      "iter 1100: loss 1.9518, time 66.72ms, mfu 0.02%\n",
      "iter 1200: loss 1.8313, time 112.24ms, mfu 0.02%\n",
      "step 1250: train loss 1.8914, val loss 2.0423\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 1300: loss 1.8632, time 34.93ms, mfu 0.02%\n",
      "iter 1400: loss 1.7975, time 305.53ms, mfu 0.02%\n",
      "step 1500: train loss 1.8689, val loss 1.9641\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 1500: loss 1.7784, time 1411.02ms, mfu 0.02%\n",
      "iter 1600: loss 1.8319, time 58.01ms, mfu 0.02%\n",
      "iter 1700: loss 1.7392, time 48.22ms, mfu 0.02%\n",
      "step 1750: train loss 1.8167, val loss 1.9355\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 1800: loss 1.7370, time 63.09ms, mfu 0.02%\n",
      "iter 1900: loss 1.7491, time 209.76ms, mfu 0.02%\n",
      "step 2000: train loss 1.7606, val loss 1.9448\n",
      "iter 2000: loss 1.6255, time 1472.25ms, mfu 0.02%\n",
      "iter 2100: loss 1.6976, time 110.59ms, mfu 0.02%\n",
      "iter 2200: loss 1.7498, time 75.88ms, mfu 0.02%\n",
      "step 2250: train loss 1.7678, val loss 1.8996\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 2300: loss 1.7277, time 62.45ms, mfu 0.02%\n",
      "iter 2400: loss 1.6347, time 50.25ms, mfu 0.02%\n",
      "step 2500: train loss 1.7209, val loss 1.8388\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 2500: loss 1.6360, time 993.24ms, mfu 0.02%\n",
      "iter 2600: loss 1.6469, time 95.51ms, mfu 0.02%\n",
      "iter 2700: loss 1.5696, time 48.94ms, mfu 0.02%\n",
      "step 2750: train loss 1.7204, val loss 1.8578\n",
      "iter 2800: loss 1.7296, time 58.36ms, mfu 0.02%\n",
      "iter 2900: loss 1.6170, time 141.86ms, mfu 0.02%\n",
      "step 3000: train loss 1.6802, val loss 1.7925\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 3000: loss 1.7441, time 942.08ms, mfu 0.01%\n",
      "iter 3100: loss 1.7017, time 193.91ms, mfu 0.01%\n",
      "iter 3200: loss 1.6968, time 73.82ms, mfu 0.01%\n",
      "step 3250: train loss 1.6061, val loss 1.7888\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 3300: loss 1.6583, time 71.42ms, mfu 0.01%\n",
      "iter 3400: loss 1.6690, time 73.81ms, mfu 0.02%\n",
      "step 3500: train loss 1.6306, val loss 1.7791\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 3500: loss 1.7636, time 1316.28ms, mfu 0.01%\n",
      "iter 3600: loss 1.6294, time 61.45ms, mfu 0.01%\n",
      "iter 3700: loss 1.5990, time 81.15ms, mfu 0.01%\n",
      "step 3750: train loss 1.5978, val loss 1.7573\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 3800: loss 1.5793, time 66.68ms, mfu 0.02%\n",
      "iter 3900: loss 1.4208, time 211.16ms, mfu 0.01%\n",
      "step 4000: train loss 1.5805, val loss 1.7571\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 4000: loss 1.5334, time 1396.49ms, mfu 0.01%\n",
      "iter 4100: loss 1.6382, time 153.82ms, mfu 0.01%\n",
      "iter 4200: loss 1.4615, time 65.74ms, mfu 0.01%\n",
      "step 4250: train loss 1.5323, val loss 1.7059\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 4300: loss 1.5480, time 96.67ms, mfu 0.01%\n",
      "iter 4400: loss 1.5804, time 43.98ms, mfu 0.01%\n",
      "step 4500: train loss 1.5376, val loss 1.7234\n",
      "iter 4500: loss 1.4916, time 846.00ms, mfu 0.01%\n",
      "iter 4600: loss 1.5921, time 47.05ms, mfu 0.01%\n",
      "iter 4700: loss 1.4744, time 59.09ms, mfu 0.02%\n",
      "step 4750: train loss 1.5363, val loss 1.6972\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 4800: loss 1.4969, time 78.21ms, mfu 0.02%\n",
      "iter 4900: loss 1.5411, time 89.12ms, mfu 0.02%\n",
      "step 5000: train loss 1.5131, val loss 1.6834\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 5000: loss 1.4159, time 4308.65ms, mfu 0.01%\n",
      "iter 5100: loss 1.4153, time 90.22ms, mfu 0.01%\n",
      "iter 5200: loss 1.4340, time 91.77ms, mfu 0.01%\n",
      "step 5250: train loss 1.4913, val loss 1.6901\n",
      "iter 5300: loss 1.4940, time 70.74ms, mfu 0.01%\n",
      "iter 5400: loss 1.3870, time 67.11ms, mfu 0.02%\n",
      "step 5500: train loss 1.5057, val loss 1.6455\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 5500: loss 1.4172, time 2006.62ms, mfu 0.01%\n",
      "iter 5600: loss 1.4388, time 299.41ms, mfu 0.01%\n",
      "iter 5700: loss 1.4207, time 67.27ms, mfu 0.01%\n",
      "step 5750: train loss 1.4785, val loss 1.6479\n",
      "iter 5800: loss 1.5400, time 569.22ms, mfu 0.01%\n",
      "iter 5900: loss 1.4713, time 45.90ms, mfu 0.01%\n",
      "step 6000: train loss 1.5069, val loss 1.6244\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 6000: loss 1.4365, time 2810.52ms, mfu 0.01%\n",
      "iter 6100: loss 1.5590, time 79.37ms, mfu 0.01%\n",
      "iter 6200: loss 1.3098, time 134.67ms, mfu 0.01%\n",
      "step 6250: train loss 1.4685, val loss 1.6249\n",
      "iter 6300: loss 1.3452, time 99.31ms, mfu 0.01%\n",
      "iter 6400: loss 1.4676, time 104.12ms, mfu 0.01%\n",
      "step 6500: train loss 1.4400, val loss 1.6172\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 6500: loss 1.3431, time 2484.64ms, mfu 0.01%\n",
      "iter 6600: loss 1.3370, time 118.95ms, mfu 0.01%\n",
      "iter 6700: loss 1.3688, time 127.65ms, mfu 0.01%\n",
      "step 6750: train loss 1.4495, val loss 1.6297\n",
      "iter 6800: loss 1.4102, time 175.36ms, mfu 0.01%\n",
      "iter 6900: loss 1.5330, time 54.60ms, mfu 0.01%\n",
      "step 7000: train loss 1.4252, val loss 1.6145\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 7000: loss 1.4078, time 1684.17ms, mfu 0.01%\n",
      "iter 7100: loss 1.5209, time 74.01ms, mfu 0.01%\n",
      "iter 7200: loss 1.4107, time 85.77ms, mfu 0.01%\n",
      "step 7250: train loss 1.4447, val loss 1.6165\n",
      "iter 7300: loss 1.3455, time 49.38ms, mfu 0.01%\n",
      "iter 7400: loss 1.3169, time 89.75ms, mfu 0.01%\n",
      "step 7500: train loss 1.4446, val loss 1.6006\n",
      "saving checkpoint to out/proposed_margin_4.0_wo_weight_tying\n",
      "iter 7500: loss 1.4069, time 3699.40ms, mfu 0.01%\n",
      "iter 7600: loss 1.4164, time 247.88ms, mfu 0.01%\n",
      "iter 7700: loss 1.4643, time 74.53ms, mfu 0.01%\n",
      "step 7750: train loss 1.4306, val loss 1.6439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7800: loss 1.3660, time 55.94ms, mfu 0.01%\n",
      "iter 7900: loss 1.3607, time 126.62ms, mfu 0.01%\n",
      "step 8000: train loss 1.4442, val loss 1.6123\n",
      "iter 8000: loss 1.3152, time 2183.78ms, mfu 0.01%\n",
      "done for margin=4.0\n",
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 20\n",
      "log_interval = 100 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 12\n",
      "block_size = 64 # context of up to 64 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "dropout = 0.0\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 8000\n",
      "lr_decay_iters = 8000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out/proposed_margin_8.0_wo_weight_tying\n",
      "Overriding: model_type = proposed\n",
      "Overriding: margin = 8.0\n",
      "Overriding: weight_tying = False\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.82M\n",
      "num decayed parameter tensors: 19, with 827,904 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.8700, val loss 4.8689\n",
      "iter 0: loss 4.8684, time 2608.37ms, mfu -100.00%\n",
      "iter 100: loss 2.7887, time 129.08ms, mfu 0.01%\n",
      "iter 200: loss 2.5113, time 357.23ms, mfu 0.01%\n",
      "step 250: train loss 2.4956, val loss 2.5073\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 300: loss 2.4434, time 200.59ms, mfu 0.01%\n",
      "iter 400: loss 2.4336, time 88.34ms, mfu 0.01%\n",
      "step 500: train loss 2.2581, val loss 2.3063\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 500: loss 2.2398, time 1552.15ms, mfu 0.01%\n",
      "iter 600: loss 2.1699, time 96.20ms, mfu 0.01%\n",
      "iter 700: loss 2.1267, time 126.41ms, mfu 0.01%\n",
      "step 750: train loss 2.1204, val loss 2.1481\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 800: loss 2.2165, time 52.61ms, mfu 0.01%\n",
      "iter 900: loss 1.9740, time 114.41ms, mfu 0.01%\n",
      "step 1000: train loss 1.9835, val loss 2.0551\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 1000: loss 2.0740, time 1368.52ms, mfu 0.01%\n",
      "iter 1100: loss 1.9851, time 83.50ms, mfu 0.01%\n",
      "iter 1200: loss 1.8739, time 117.90ms, mfu 0.01%\n",
      "step 1250: train loss 1.8748, val loss 2.0425\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 1300: loss 1.8738, time 77.53ms, mfu 0.01%\n",
      "iter 1400: loss 1.8100, time 69.54ms, mfu 0.01%\n",
      "step 1500: train loss 1.8538, val loss 1.9612\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 1500: loss 1.7849, time 1648.49ms, mfu 0.01%\n",
      "iter 1600: loss 1.8687, time 50.00ms, mfu 0.01%\n",
      "iter 1700: loss 1.7462, time 132.93ms, mfu 0.01%\n",
      "step 1750: train loss 1.7909, val loss 1.9210\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 1800: loss 1.7516, time 87.74ms, mfu 0.01%\n",
      "iter 1900: loss 1.7986, time 66.51ms, mfu 0.01%\n",
      "step 2000: train loss 1.7346, val loss 1.9362\n",
      "iter 2000: loss 1.6227, time 1223.29ms, mfu 0.01%\n",
      "iter 2100: loss 1.7222, time 101.15ms, mfu 0.01%\n",
      "iter 2200: loss 1.7707, time 48.39ms, mfu 0.01%\n",
      "step 2250: train loss 1.7499, val loss 1.8925\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 2300: loss 1.7537, time 48.06ms, mfu 0.01%\n",
      "iter 2400: loss 1.7102, time 59.08ms, mfu 0.02%\n",
      "step 2500: train loss 1.7008, val loss 1.8262\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 2500: loss 1.6473, time 1785.50ms, mfu 0.01%\n",
      "iter 2600: loss 1.6646, time 92.30ms, mfu 0.01%\n",
      "iter 2700: loss 1.5655, time 134.92ms, mfu 0.01%\n",
      "step 2750: train loss 1.6923, val loss 1.8587\n",
      "iter 2800: loss 1.7373, time 63.96ms, mfu 0.01%\n",
      "iter 2900: loss 1.6468, time 88.17ms, mfu 0.01%\n",
      "step 3000: train loss 1.6607, val loss 1.8024\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 3000: loss 1.7816, time 1180.46ms, mfu 0.01%\n",
      "iter 3100: loss 1.7442, time 69.83ms, mfu 0.01%\n",
      "iter 3200: loss 1.7190, time 58.88ms, mfu 0.01%\n",
      "step 3250: train loss 1.5865, val loss 1.7910\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 3300: loss 1.6837, time 68.94ms, mfu 0.01%\n",
      "iter 3400: loss 1.6840, time 51.14ms, mfu 0.02%\n",
      "step 3500: train loss 1.6203, val loss 1.7919\n",
      "iter 3500: loss 1.8063, time 1819.05ms, mfu 0.01%\n",
      "iter 3600: loss 1.6283, time 152.35ms, mfu 0.01%\n",
      "iter 3700: loss 1.6250, time 50.23ms, mfu 0.02%\n",
      "step 3750: train loss 1.5788, val loss 1.7674\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 3800: loss 1.6190, time 281.79ms, mfu 0.01%\n",
      "iter 3900: loss 1.4352, time 50.19ms, mfu 0.02%\n",
      "step 4000: train loss 1.5549, val loss 1.7582\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 4000: loss 1.5792, time 2440.56ms, mfu 0.01%\n",
      "iter 4100: loss 1.6678, time 83.26ms, mfu 0.01%\n",
      "iter 4200: loss 1.4568, time 82.42ms, mfu 0.01%\n",
      "step 4250: train loss 1.5105, val loss 1.6939\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 4300: loss 1.5509, time 113.83ms, mfu 0.01%\n",
      "iter 4400: loss 1.5866, time 119.58ms, mfu 0.01%\n",
      "step 4500: train loss 1.5133, val loss 1.7207\n",
      "iter 4500: loss 1.5021, time 1007.94ms, mfu 0.01%\n",
      "iter 4600: loss 1.5996, time 101.20ms, mfu 0.01%\n",
      "iter 4700: loss 1.4728, time 101.34ms, mfu 0.01%\n",
      "step 4750: train loss 1.5102, val loss 1.6927\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 4800: loss 1.5098, time 89.69ms, mfu 0.01%\n",
      "iter 4900: loss 1.5792, time 96.54ms, mfu 0.01%\n",
      "step 5000: train loss 1.4830, val loss 1.6853\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 5000: loss 1.4278, time 1068.20ms, mfu 0.01%\n",
      "iter 5100: loss 1.4323, time 44.41ms, mfu 0.01%\n",
      "iter 5200: loss 1.4663, time 91.31ms, mfu 0.01%\n",
      "step 5250: train loss 1.4665, val loss 1.6773\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 5300: loss 1.5162, time 62.57ms, mfu 0.01%\n",
      "iter 5400: loss 1.3618, time 65.95ms, mfu 0.01%\n",
      "step 5500: train loss 1.4769, val loss 1.6419\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 5500: loss 1.4173, time 3684.15ms, mfu 0.01%\n",
      "iter 5600: loss 1.4438, time 57.63ms, mfu 0.01%\n",
      "iter 5700: loss 1.4493, time 832.64ms, mfu 0.01%\n",
      "step 5750: train loss 1.4522, val loss 1.6518\n",
      "iter 5800: loss 1.5615, time 295.78ms, mfu 0.01%\n",
      "iter 5900: loss 1.4783, time 137.95ms, mfu 0.01%\n",
      "step 6000: train loss 1.4752, val loss 1.6231\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 6000: loss 1.4569, time 1788.05ms, mfu 0.01%\n",
      "iter 6100: loss 1.5946, time 211.81ms, mfu 0.01%\n",
      "iter 6200: loss 1.3270, time 106.96ms, mfu 0.01%\n",
      "step 6250: train loss 1.4395, val loss 1.6187\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 6300: loss 1.3626, time 73.30ms, mfu 0.01%\n",
      "iter 6400: loss 1.4829, time 59.42ms, mfu 0.01%\n",
      "step 6500: train loss 1.4146, val loss 1.6106\n",
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 6500: loss 1.3669, time 1024.99ms, mfu 0.01%\n",
      "iter 6600: loss 1.3634, time 77.21ms, mfu 0.01%\n",
      "iter 6700: loss 1.3688, time 324.20ms, mfu 0.01%\n",
      "step 6750: train loss 1.4267, val loss 1.6265\n",
      "iter 6800: loss 1.4342, time 66.32ms, mfu 0.01%\n",
      "iter 6900: loss 1.5509, time 135.66ms, mfu 0.01%\n",
      "step 7000: train loss 1.3980, val loss 1.6169\n",
      "iter 7000: loss 1.4452, time 2219.74ms, mfu 0.01%\n",
      "iter 7100: loss 1.5340, time 317.88ms, mfu 0.01%\n",
      "iter 7200: loss 1.4274, time 372.67ms, mfu 0.01%\n",
      "step 7250: train loss 1.4170, val loss 1.6228\n",
      "iter 7300: loss 1.3626, time 150.14ms, mfu 0.01%\n",
      "iter 7400: loss 1.3294, time 951.49ms, mfu 0.01%\n",
      "step 7500: train loss 1.4112, val loss 1.5965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to out/proposed_margin_8.0_wo_weight_tying\n",
      "iter 7500: loss 1.4120, time 1164.92ms, mfu 0.01%\n",
      "iter 7600: loss 1.4322, time 128.99ms, mfu 0.01%\n",
      "iter 7700: loss 1.4753, time 163.03ms, mfu 0.01%\n",
      "step 7750: train loss 1.4090, val loss 1.6584\n",
      "iter 7800: loss 1.3692, time 762.33ms, mfu 0.01%\n",
      "iter 7900: loss 1.3797, time 209.39ms, mfu 0.01%\n",
      "step 8000: train loss 1.4265, val loss 1.6138\n",
      "iter 8000: loss 1.3417, time 2978.03ms, mfu 0.01%\n",
      "done for margin=8.0\n"
     ]
    }
   ],
   "source": [
    "for margin in [4.0, 8.0]:\n",
    "    args = {\n",
    "        \"out_dir\": f\"out/proposed_margin_{margin}_wo_weight_tying\",\n",
    "        \"model_type\": \"proposed\",\n",
    "        \"margin\": margin,\n",
    "    \"weight_tying\": \"False\"\n",
    "        \n",
    "    }\n",
    "    run_training(args)\n",
    "    print(f'done for margin={margin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448733f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# for margin in [0.75, 0.55, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]:\n",
    "#     model_name = f'proposed_margin_{margin}_separated_embeddings'\n",
    "#     args = {\n",
    "#         \"out_dir\": f\"out/{model_name}\",\n",
    "#         \"model_type\": \"proposed\",\n",
    "#         \"margin\": margin,\n",
    "#         \"separated_embeddings\": \"True\"\n",
    "#     }\n",
    "#     run_training(args)\n",
    "#     print(f'done for margin={margin}')\n",
    "#     fix_ckpt(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03315984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
